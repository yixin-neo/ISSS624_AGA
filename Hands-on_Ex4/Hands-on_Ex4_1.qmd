---
title: "Hands-on Exercise 4.1: Geopraphically Weighted Regression"
author: "NeoYX"
date: '4 Dec 2023'
date-modified: "`r Sys.Date()`"
editor: visual
execute: 
  freeze: auto
  warning: false
  #echo: false
  #message: false
format: 
  html:
    code-fold: false
    code-overflow: scroll
    code-summary: "Show the code"
    code-line-numbers: true
---

## **13.1 Overview**

**Geographically weighted regression (GWR)** is a spatial statistical technique that takes **non-stationary variables** into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).

In this hands-on exercise, we will learn how to build [hedonic pricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models by using GWR methods.

Hedonic Pricing is a model that identifies price factors according to the premise that price is determined by internal and external factors affecting it. For housing, internal could mean size, height, appearance, solar panels while external could mean crime rate, distance to school or downtown area.

The **dependent** variable is the **resale** prices of condominium in 2015. The **independent** variables are divided into either **structural** and **locational**.

## **13.2 The Data**

Two data sets will be used in this model building exercise, they are:

-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*)

-   Condo_resale_2015 in csv format (i.e. *Condo_resale_2015.csv*)

Using MP14 instead of MP19 because of condo_resale_2015.

## **13.3 Getting Started**

Before we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.

The R packages needed for this exercise are as follows:

-   R package for building OLS and performing diagnostics tests

    -   [**olsrr**](https://olsrr.rsquaredacademy.com/)
    -   It enhances the capabilities of the basic linear modeling functionality and provides a comprehensive set of regression diagnostics, model comparisons, and other statistics, including normality of residuals, homoscedasticity, and influential observations.

-   R package for calibrating geographical weighted family of models

    -   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/)

-   R package for multivariate data visualisation and analysis

    -   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)
    -   In this exercise, we will be using this package to visualise the correlation matrix

-   [ggpubr](https://rpkgs.datanovia.com/ggpubr/) for some easy-to-use functions for creating and customizing 'ggplot2'- based publication ready plots.

-   Spatial data handling

    -   **sf** for importing, integrating, processing and transforming geospatial data.

-   Attribute data handling

    -   **tidyverse**, especially **readr**, **ggplot2** and **dplyr**

-   Choropleth mapping

    -   **tmap** for creating thematic maps

-   Create publication ready summary tables in R

    -   [gtsummary](https://www.danieldsjoberg.com/gtsummary/) package and its [tbl_regression()](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) function

The code chunks below installs and launches these R packages into R environment.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, DT, knitr)
```

## **13.4 A short note about GWmodel**

[**GWmodel**](https://www.jstatsoft.org/article/view/v063i17) package provides a collection of **localised** spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.

## **13.5 Geospatial Data Wrangling**

### **13.5.1 Importing geospatial data**

The geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014's planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using `st_read()` of **sf** packages.

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")

st_crs(mpsz)
```

The report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called *mpsz* and it is a simple feature object. The geometry type is *multipolygon*. it is also important to note that mpsz simple feature object **does not** have EPSG information.

### **13.5.2 Updating CRS information**

The code chunk below updates the newly imported *mpsz* with the correct ESPG code (i.e. 3414)

```{r}
mpsz_svy21 <- mpsz %>%  
  st_transform(crs=3414)
```

The EPSG: is indicated as *3414* now.

Next, we will reveal the extent (rectangular boundary) of *mpsz_svy21* by using `st_bbox()` of sf package.

```{r}
st_bbox(mpsz_svy21)
```

## **13.6 Aspatial Data Wrangling**

### **13.6.1 Importing the aspatial data**

The *condo_resale_2015* is in csv file format. The codes chunk below uses `read_csv()` function of **readr** package to import *condo_resale_2015* into R as a tibble data frame called *condo_resale*.

```{r}
condo_resale <- read_csv('data/aspatial/Condo_resale_2015.csv')
```

Let us examine if the data file has been imported correctly.

The codes chunks below uses `glimpse()` to display the data structure.

```{r}
glimpse(condo_resale)
```

Check the first five longitude (X) and latitude (Y) columns.

```{r}
condo_resale %>% select(1,2) %>% head()
```

Next, `summary()` of base R is used to display the summary statistics of *cond_resale* tibble data frame.

Longitude and latitude are in decimal deg (\< 360deg) , WSG84 (or crs 4326).

Some postal codes are only 5 digits, the number 0 in front could have been truncated.

```{r}
summary(condo_resale)
```

### **13.6.2 Converting aspatial data frame into a sf object**

Currently, the *condo_resale* tibble data frame is aspatial. We will convert it to a **sf** object. The code chunk below converts condo_resale data frame into a simple feature **POINT** data frame by using `st_as_sf()` of **sf** packages.

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c('LONGITUDE',
                                       'LATITUDE'),
                            crs = 4326) %>% 
  st_transform(crs= 3414)
```

Notice that `st_transform()` of **sf** package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).

Next, `head()` is used to list the content of *condo_resale.sf* object.

```{r}
head(condo_resale.sf)
```

Notice that the output is in point feature data frame.

## **13.7 Exploratory Data Analysis (EDA)**

In the section, wewill learn how to use statistical graphics functions of **ggplot2** package to perform EDA.

### **13.7.1 EDA using statistical graphics**

We can plot the distribution of *SELLING_PRICE* by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.

```{r}
#| fig-width: 14
#| fig-asp: 0.68
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

```{r}
#| eval: false
#| echo: false
set.seed(1234)

median(condo_resale.sf$`SELLING_PRICE`)
# mean= 1751211 , median=1383222

nortest::ad.test(condo_resale.sf$`SELLING_PRICE`)
# selling_price  does not follow normal distribution
# we can reject the null hypo and conclude that the selling priec is not normally distributed.
ggstatsplot::gghistostats(data=condo_resale.sf,
             x = `SELLING_PRICE`,
             type='nonparametric',
             test.value =1383222,
             conf.level = 0.95,
             xlab = 'Selling price')
# we can reject the null hypothesis and conclude that the median selling price is not $1,383,222
```

The figure above reveals a **right skewed** distribution. This means that **more** condominium units were transacted at relative **lower** prices.

Statistically, the skewed distribution can be normalised by using **log** transformation. The code chunk below is used to derive a new variable called *LOG_SELLING_PRICE* by using a log transformation on the variable *SELLING_PRICE*. It is performed using `mutate()` of **dplyr** package.

```{r}
condo_resale.sf <- condo_resale.sf %>% 
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

Now, we can plot the *LOG_SELLING_PRICE* using the code chunk below.

```{r}
#| fig-width: 14
#| fig-asp: 0.68
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue") 
```

Check for normality of the LOG_SELLING_PRICE. Although it is still not normally-distributed, it is less skewed after the transformation.

```{r}
nortest::ad.test(condo_resale.sf$`LOG_SELLING_PRICE`)
```

### **13.7.2 Multiple Histogram Plots distribution of variables**

In this section, we will learn how to draw a small multiple histograms (also known as trellis plot) by using `ggarrange()` of [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/) package.

The code chunk below is used to create 12 histograms. Then, `ggarrange()` is used to organised these histogram into a 3 columns by 4 rows small multiple plot.

```{r}
#| fig-width: 14
#| fig-asp: 0.68
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

```{r}
#| eval: false
#| echo: false
# perform Shapiro-Wilk test on math scores by gender
shapiro_test <- by(condo_resale.sf$SELLING_PRICE, condo_resale.sf$FREEHOLD, shapiro.test)

# extract p-values
p_values <- sapply(shapiro_test, function(x) x$p.value)
# print results
print(p_values)

# we have enough statistical evidence to reject the null hypothesis and concluded that the sellingprice by freehold does not follow normal distribution.
```

```{r}
#| eval: false
#| echo: false
condo_resale.sf$FREEHOLD <- as.factor(condo_resale.sf$FREEHOLD)
ggstatsplot::ggbetweenstats(data=condo_resale.sf,
               x=FREEHOLD,
               y=SELLING_PRICE,
               type='np',
               messages=FALSE)
```

### **13.7.3 Drawing Statistical Point Map**

Lastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using **tmap** package.

```{r}
tmap_mode('plot')
#tmap_mode('view')
#tmap_options(check.and.fix = TRUE)
 
tm_shape(mpsz_svy21)+
  tm_polygons(alpha=0.5) +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

Notice that [`tm_dots()`](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_symbols) is used instead of `tm_bubbles()`.

If in tmap_mode('view') mode,

`set.zoom.limits` argument of `tm_view()` sets the minimum and maximum zoom level to 11 and 14 respectively.

## **13.8 Hedonic Pricing Modelling in R**

In this section, we will learn how to building hedonic pricing models for condominium resale units using [`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/lm) of R base.

By calibration, we meant estimating local parameters that vary spatially for each location in the study area, considering nearby observations with higher weights and more distant observations with lower weights. This allows the relationship between variables to vary spatially, capturing local variations in the relationships.

The significance of the variables can be assessed locally, telling us where (at which location) specific predictors have a more significant impact on our dependent variable.

Model Evaluation:: The calibration process in GWR involves fitting the model to the data at each location, and the quality of the fit can be assessed through various diagnostics (e.g., residuals, local R-squared values).

When calibrating a Geographically Weighted Regression model, the emphasis is on capturing spatial heterogeneity in the relationships between variables and understanding how these relationships change across the study area. This is different from traditional models that assume a globally constant relationship. (Hands-on3 and In-class3)

### **13.8.1 Simple Linear Regression Method**

First, we will build a simple linear regression model by using *SELLING_PRICE* as the dependent variable and *AREA_SQM* as the independent variable.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, 
                data = condo_resale.sf)
class(condo.slr)
```

```{r}
attributes(condo.slr)
```

```{r}
methods(class=class(condo.slr))
```

`lm()` returns an object of class "lm" or for multiple responses of class c("mlm", "lm").

The functions `summary()` and `anova()` can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by `lm`.

```{r}
summary(condo.slr)
```

The output report reveals that the SELLING_PRICE can be explained by using the formula:

\*y = -258121.1 + 14719x1\*

The R-squared of 0.4518 reveals that the independent variable is able to explain about 45% of the variation in the dependent variable, resale prices.

Since p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of *SELLING_PRICE*.

The **Coefficients:** section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.

To visualise the best fit curve on a scatterplot, we can incorporate `lm()` as a method function in ggplot's geometry as shown in the code chunk below.

```{r}
#| fig-width: 14
#| fig-asp: 0.68
plotly::ggplotly(ggplot(data= condo_resale.sf,
      aes(x = `AREA_SQM`,
          y= `SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm))
```

Figure above reveals that there are a **few statistical outliers** with relatively high selling prices.

### **13.8.2 Multiple Linear Regression Method**

#### 13.8.2.1 Visualising the relationships of the independent variables

Before building a multiple regression model, it is important to ensure that the indepdent variables used are **not** highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as **multicollinearity** in statistics.

**Correlation matrix** is commonly used to visualise the relationships between the independent variables. Beside the `pairs()` of R, there are many packages support the display of a correlation matrix. We will be using the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package.

The code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in *condo_resale* data.frame.

```{r}
colnames(condo_resale)
```

`cor(condo_resale[,5:23])` gives a matrix array of the correlation values between each pair of variables.

```{r}
#| fig-width: 14
#| fig-asp: 0.68
corrplot(cor(condo_resale[,5:23]),
         diag= FALSE,
         order= 'AOE',
         tl.pos= 'td',
         tl.cex = 0.8, # increase or decrease the size of variable names
         method = 'number',
         type = 'upper')
```

Matrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named "AOE", "FPC", "hclust", "alphabet". In the code chunk above, AOE order is used. It orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

**`tl.pos`:** This parameter specifies the position of the variable names (text labels) around the correlation plot. It can take one of the following values:

-   **`"n"`**: Names are displayed on the top of the plot.

-   **`"ne"`**: Names are displayed on the top-right corner of the plot.

-   **`"e"`**: Names are displayed on the right side of the plot.

-   **`"se"`**: Names are displayed on the bottom-right corner of the plot.

-   **`"s"`**: Names are displayed at the bottom of the plot.

-   **`"sw"`**: Names are displayed on the bottom-left corner of the plot.

-   **`"w"`**: Names are displayed on the left side of the plot.

-   **`"nw"`**: Names are displayed on the top-left corner of the plot.

-   **`"td"`**: Names are displayed outside of the plot.

From the scatterplot matrix, it is clear that ***Freehold*** is highly correlated to ***LEASE_99YEAR***. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, ***LEASE_99YEAR*** is **excluded** in the subsequent model building.

### **13.8.3 Building a hedonic pricing model using multiple linear regression method**

Previously, we have use `lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)` to fit only one independent variable.

The code chunk below using `lm()` to **calibrate** the *multiple* linear regression model, ***LEASE_99YEAR*** has been excluded.

```{r}
condo.mlr18 <- lm(formula = log(SELLING_PRICE) ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr18)
```

The 4 non-significant variables are 'PROX_HAWKER_MARKET', 'PROX_KINDERGARTEN', 'PROX_TOP_PRIMARY_SCH' and 'PROX_SUPERMARKET'.

### **13.8.4 Preparing Publication Quality Table: olsrr method**

With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those **4** variables above which are not statistically significant.

Now, we are ready to calibrate the revised model by using the code chunk below.

```{r}
condo.mlr14 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)

ols_regress(condo.mlr14)
```

**Confidence** **Interval Interpretation:** The entire confidence interval for age predictor is \[-30091.739, -19283.740\]. It means that out of 100 trials, 95/100 times the true coefficient for AGE lies within this range.

**Other tools in the olsrr package**

-   comprehensive regression output

-   residual diagnostics

-   measures of influence

-   heteroskedasticity tests

-   collinearity diagnostics

-   model fit assessment

-   variable contribution assessment

-   variable selection procedures

::: panel-tabset
## Variable selection methods

1.Best subset Regression

Select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largest R2 value or the smallest MSE, Mallow\'s Cp or AIC. [Plot](https://olsrr.rsquaredacademy.com/articles/variable_selection) is available.

```{r}
#| eval: false
ols_step_best_subset(condo.mlr18)
```

2.Stepwise Forward Regression

Build regression model from a set of candidate predictor variables by entering predictors based on p values, in a stepwise manner until there is no variable left to enter any more. The model should include all the candidate predictor variables. If details is set to `TRUE`, each step is displayed.

```{r}
ols_step_forward_p(condo.mlr18)
```

## Residual Diagnostics

1.Residual QQ plot: Detects violation of standard regression assumptions.

Assumptions:

-   The error has a normal distribution (normality assumption).

-   The errors have mean zero.

-   The errors have same but unknown variance (homoscedasticity assumption).

-   The error are independent of each other (independent errors assumption).

```{r}
ols_plot_resid_qq(condo.mlr14)
```

2.Residual Normality test

```{r}
ols_test_normality(condo.mlr14)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are **not** normally distributed.

3.Residual vs Fitted Values Plot

It is a scatter plot of residuals on the y axis and fitted values on the x axis to detect non-linearity, unequal error variances, and outliers.

Characteristics of a well behaved residual vs fitted plot:

-   The residuals spread randomly around the 0 line indicating that the relationship is linear.

-   The residuals form an approximate horizontal band around the 0 line indicating homogeneity of error variance.

-   No one residual is visibly away from the random pattern of the residuals indicating that there are no outliers

```{r}
ols_plot_resid_fit(condo.mlr14)
```

The spread of residual is wider for higher fitted values .

4.Residual Histogram

Histogram of residuals for detecting violation of normality assumption.

```{r}
ols_plot_resid_hist(condo.mlr14)
```

The residual appears to resemble a normal distribution, but chart 2 above shows that the residuals are **not** normal distributed.

[Link](https://olsrr.rsquaredacademy.com/articles/residual_diagnostics)

## Heteroskedasticity

F Test for heteroskedasticity under the assumption that the errors are independent and identically distributed (i.i.d.). We can perform the test using the fitted values of the model, the predictors in the model and a subset of the independent variables.

```{r}
ols_test_f(condo.mlr14)
```

## Collinearity Diagnostics

1.Check for signs of multicollinearity

```{r}
ols_vif_tol(condo.mlr14)
```

Since the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.

2.Observed vs Predicted plot

Plot of observed vs fitted values to assess the fit of the model. Ideally, all points should be close to a regressed diagonal line. Draw such a diagonal line within your graph and check out where the points lie. If your model had a high R Square, all the points would be close to this diagonal line. The lower the R Square, the weaker the Goodness of fit of your model, the more foggy or dispersed your points are from this diagonal line.

```{r}
ols_plot_obs_fit(condo.mlr14)
```

[Link](https://olsrr.rsquaredacademy.com/articles/regression_diagnostics)
:::

### **13.8.5 Preparing Publication Quality Table: gtsummary method**

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) package provides an elegant and flexible way to create publication-ready summary tables in R.

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
tbl_regression(condo.mlr14, intercept = TRUE)
```

With gtsummary package, model statistics can be included in the report by either appending them to the report table by using [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) or adding as a table source note by using [`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) as shown in the code chunk below.

```{r}
tbl_regression(condo.mlr14, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

#### 13.8.5.4 Testing for Spatial Autocorrelation

The hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert *condo_resale.sf* from sf data frame into a **SpatialPointsDataFrame**.

First, we will export the residual of the hedonic pricing model and save it as a data frame.

```{r}

#| eval: false
#| echo: false
#| fig-width: 14
#| fig-asp: 0.68
#| code-fold: True
```

## Summaries
