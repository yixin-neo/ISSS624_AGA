---
title: "Hands-on Exercise 2.2: Global Measures of Spatial Autocorrelation"
author: "NeoYX"
date: '22 Nov 2023'
date-modified: "`r Sys.Date()`"
editor: visual
execute: 
  freeze: auto
  warning: false
  #echo: false
  #message: false
format: 
  html:
    code-fold: false
    code-overflow: scroll
    code-summary: "Show the code"
    code-line-numbers: true
---

## **9.1 Overview**

In this hands-on exercise, we will learn to

-   compute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of **spdep** package,

    -   plot Moran scatterplot,

    -   compute and plot spatial correlogram using appropriate function of **spdep** package.

-   compute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions **spdep** package;

-   compute Getis-Ord\'s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of **spdep** package; and

-   to visualise the analysis output by using **tmap** package.

## **9.2 Getting Started**

### 9.2.1 The analytical question

-   In spatial policy, local government/planners aims to ensure equal distribution of development in the province.

-   we should apply appropriate spatial statistical methods to discover if development are even distributed geographically in the province

-   if answer is NO, we ask \"is there sign of clustering?\" (GLOBAL spatial autocorrelation)

-   if YES, \"Where are the clusters\" (LOCAL spatial autocorrelation)

In this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.
(https://en.wikipedia.org/wiki/Hunan)

### 

9.2.2 The Study Area and Data

Two data sets will be used in this hands-on exercise:

1.  Geospatial data: Hunan province administrative boundary layer at county level in ESRI shapefile format

2.  Aspatial data: *Hunan_2012.csv* containing local development indicators

### 9.2.3 Setting the Analytical Tools

Packages we need:

-   sf is use for importing and handling geospatial data in R,

-   tidyverse is mainly use for wrangling attribute data in R,

-   spdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and

-   tmap will be used to prepare cartographic quality chropleth map.

```{r}
pacman::p_load(sf, spdep, tmap, tidyverse, knitr)
```

## **9.3 Getting the Data Into R Environment**

The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.

### **9.3.1 Import shapefile into r environment**

The code chunk below uses [*st_read()*](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import Hunan shapefile into R. The imported shapefile will be **simple features** Object of **sf**.

`hunan` is in WSG84 geographical system.

```{r}
hunan <- st_read(dsn='data/geospatial',
                 layer='Hunan')
#st_crs(hunan)
head(hunan,3)
```

### **9.3.2 Import csv file into r environment**

Next, we will import *Hunan_2012.csv* into R by using *read_csv()* of **readr** package. The output is R data frame class.

```{r}
hunan2012 <- read_csv('data/aspatial/Hunan_2012.csv')
head(hunan2012,3)
```

### **9.3.3 Performing relational join**

The code chunk below will be used to update the attribute table of *hunan*\'s SpatialPolygonsDataFrame (geospatial) with the attribute fields of *hunan2012* dataframe (aspatial) . This is performed by using *left_join()* of **dplyr** package. Since the join columns are not specified, identical columns names ('County') form both dataset will be used for the join.

Column 7 and 15 are the 'County' and 'GDPPC' columns respectively.

```{r}
hunan <- left_join(hunan, hunan2012) %>% 
  select(1:4, 7,15)
head(hunan,3)
```

### **9.3.4 Visualising Regional Development Indicator**

Now, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using *qtm()* of **tmap** package.

-   tm_fill() \'s **n** refer to the number of equal intervals

```{r}
#| fig-width: 14
#| fig-asp: 0.68
equal <- tm_shape(hunan)+
  tm_fill('GDPPC',
          n=5,
          style='equal') +
  tm_borders(alpha=0.5) +
  tm_layout(main.title = 'Equal interval classification',
            main.title.size=1.5,
            legend.height = 0.25,
            legend.width = 0.25)

quantile <- tm_shape(hunan)+
  tm_fill('GDPPC',
          n=5,
          style='quantile') +
  tm_borders(alpha=0.5) +
  tm_layout(main.title = 'Equal quantile classification',
            main.title.size=1.5,
            legend.height = 0.25,
            legend.width = 0.25)

tmap_arrange(equal, quantile, asp =1, ncol=2)
```

## **9.4 Global Spatial Autocorrelation**

In this section, we will

-   compute global spatial autocorrelation statistics

-   perform spatial complete randomness test for global spatial autocorrelation (test for significance).

### **9.4.1 Computing Contiguity Spatial Weights**

Before we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.

In the code chunk below, [*poly2nb()*](https://r-spatial.github.io/spdep/reference/poly2nb.html) of **spdep** package is used to compute contiguity weight matrices for the study area. This function will

-   build a neighbours list based on regions with contiguous boundaries.

-   if 'queen' argument is TRUE: spatial units are considered neighbours if they share a common point. A list of first order neighbours using the Queen criteria will be returned.

-   if 'queen' argument is FALSE: spatial unit are considered neighbours if they share a least two common points.

More specifically, the code chunk below is used to compute Queen contiguity weight matrix.

```{r}
wm_q <- poly2nb(hunan,
                queen=TRUE)
summary(wm_q)
```

Sneakpeak at the neighbours' list of the first three polygons.

```{r}
wm_q[1:3]
```

The summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.

### **9.4.2 Row-standardised weights matrix**

Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=\"W\"). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors\' values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we\'ll stick with the style=\"W\" option for simplicity\'s sake but note that other more robust options are available, notably style=\"B\".

```{r}
rswm_q <- nb2listw(wm_q,
                   style='W',
                   zero.policy=TRUE)
rswm_q
```

Sneak peak at the neighbour weights of the first three polygons

```{r}
rswm_q$weights[1:3]
```

The input of *nb2listw()* must be an object of class **nb**. The syntax of the function has two major arguments, namely style and zero.poly.

-   *style* can take values \"W\", \"B\", \"C\", \"U\", \"minmax\" and \"S\". B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).

-   If *zero policy* is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %\*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.

```{r}
attributes(rswm_q)
methods(class=class(rswm_q))
```

### **9.4.3 Global Spatial Autocorrelation: Moran\'s I**

In this section, we will learn how to perform Moran\'s I statistics testing by using [*moran.test()*](https://r-spatial.github.io/spdep/reference/moran.test.html) of **spdep**.

### **9.4.4 Moran\'s I test**

**Global** spatial association assesses the overall spatial pattern of a variable across the entire study area. It provides a single value or metric that summarizes the extent to which similar values
cluster together or are dispersed across the entire geographic space.

**The Moran's I assumes data follows a normal distribution and are randomised.**

![](images/Global%20Moran%20stats%20test.png){width="357"}

The code chunk below performs Moran\'s I statistical testing using [*moran.test()*](https://r-spatial.github.io/spdep/reference/moran.test.html) of **spdep**. It takes in the main arguments:

-   variable

-   `listw` , our spatial weights matrix that defines the neighbourhood and relationship between them.

```{r}
moran.test(hunan$GDPPC,
           listw = rswm_q,
           zero.policy=TRUE,
           na.action=na.omit)
```

> The null hypothesis: Observed spatial patterns of values is equally likely as any random spatial
> pattern.
>
> Since the p-value is less than 0.05 and Moran I statistic is greater than 1, we can reject the null hypothesis and conclude that similar values tend to cluster together in our area of study.

#### 9.4.4.1 Computing Monte Carlo Moran\'s I

In the event we are unsure whether the data follows a normal distribution and are randomised, we can use the Monte Carlo Simulation to simulate Moran's I [n]{.underline} times under the assumption of no spatial pattern (shuffle/permutate the variable across all spatial units). This creates a baseline to compare with the observed Moran's I value from dataset.

The code chunk below performs permutation test for Moran\'s I statistic by using [*moran.mc()*](https://r-spatial.github.io/spdep/reference/moran.mc.html) of **spdep**. A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm = moran.mc(hunan$GDPPC,
                 listw=rswm_q,
                 nsim=999,
                 zero.policy=TRUE,
                 na.action=na.omit)
bperm
```

| Since the p-value is less than 0.05 and Moran I statistic is greater than 1, we can reject the null hypothesis and conclude that similar values tend to cluster together in our area of study.

#### 9.4.4.2 Visualising Monte Carlo Moran\'s I

It is always a good practice for us the examine the simulated Moran\'s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.

In the code chunk below [*hist()*](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/hist) and [*abline()*](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/abline) of R Graphics are used.

Get the mean of simulated moran's I values. The 'res' column contains the simulated moran's i values.

```{r}
mean(bperm$res[1:999]) 
```

Get the variance

```{r}
var(bperm$res[1:999])
```

Summary statistics

```{r}
summary(bperm$res[1:999])
```

Plotting

::: panel-tabset
## ggplot2

We will use [ggplot2](https://ggplot2.tidyverse.org/) to create the histogram instead of base r.

[**ggthemes**](https://cran.r-project.org/web/packages/ggthemes/) provides [\'ggplot2\' themes](https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/) that replicate the look of plots by Edward Tufte, Stephen Few, [Fivethirtyeight](https://fivethirtyeight.com/), [The Economist](https://www.economist.com/graphic-detail), \'Stata\', \'Excel\', and [The Wall Street Journal](https://www.pinterest.com/wsjgraphics/wsj-graphics/), among others.

```{r}
#| fig-width: 14
#| fig-asp: 0.68
#| code-fold: true
library(ggplot2)
library(ggthemes)

bperm_df <- as.data.frame(bperm$res)
colnames(bperm_df) <- c('res')

q <- quantile(bperm_df$res, probs = c(0.25, 0.5, 0.75))
mean <- mean(bperm_df$res)

ggplot(data=bperm_df,
       aes(x=res)) +
  geom_histogram(bins=20,
                 #boundary=100,
                 color='grey25',
                 fill='grey90',size=0.8) +
  geom_vline(xintercept = q[2], linetype='dotted', size = 0.8, color='blue') +
  geom_vline(xintercept = q[3], linetype='dotted', size = 0.8) +
  geom_vline(xintercept = mean, linetype='dotted', size = 0.8, color='red') +
  annotate('text' , x= -0.055, y=180, label='50th \npercentile', size = 5, color='blue') +
  annotate('text' , x= 0.06, y=180, label='75th \npercentile', size = 5) +
  annotate('text' , x= 0.005, y=180, label='mean', size = 5, color='red') +
  labs(y= 'Frequency', x="Moran's I values") +
  theme_economist() +
  theme(axis.title.y=element_text(angle = 0,
                                  vjust=0.9)) +
  ggtitle("Histogram of Simulated Moran's I")

```

## Base Graph

```{r}
hist(bperm$res,
     freq=TRUE,
     breaks=20,
     xlab="Simulated Moran's I",
     main = paste("Histogram of Simulated Moran I"))
abline(v=0,
       col='red')
```
:::

```{r}
#| eval: false
#| echo: false
#| code-fold: True
#| fig-width: 14
#| fig-asp: 0.68
```

## Summaries

### 
