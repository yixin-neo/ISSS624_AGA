---
title: "In-class Exercise 2"
author: "NeoYX"
date: '25 Nov 2023'
date-modified: "`r Sys.Date()`"
editor: visual
execute: 
  freeze: auto
  warning: false
  #echo: false
  #message: false
format: 
  html:
    #fontsize: 20px
    code-fold: false
    code-overflow: scroll
    code-summary: "Show the code"
    code-line-numbers: true
---

# Getting started

Installing and loading the required R packages.

`dplyr`: reshape data, joins, pivot for instance..

`tidyr`: transform data

`knitr`: generate html table

Today, `sfdep` library will replace the `spdep` library because it is more recent (\<2 years ago). Allows to [mutate]{.underline} using [spatial function]{.underline}s. `spdep` does not allow for mutate for instance.

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse, knitr, plotly,Kendall, DT)
```

## The data

-   Hunan, geospatial dataset in shapefile format

-   Hunan_2012, an attribute dataset in csv format

### Part 1: Spatial weights - sfdep methods

#### Import geospatial and aspatial data

In the code chunk below , import geospatial data using st_read() from `sf` library. The output has projection of WSG84 and 88 observations. Class of 'sf' and 'tibble df', and in tibble df contains a geometry list. In 'sf', each row/observation represents a geographical region/area/unit.

```{r}
 hunan<- st_read(dsn="data/geospatial", layer = "Hunan")
 class(hunan)
```

In the code chunk below , import aspatial data

```{r}
hunan_2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

Combine both files together using left_join hunan (geo) with hunan_2012 (aspatial) to retain the `sf` class. If the unique identifier is not specified, R will use identical columns, in this case 'County' columns in both objects.

```{r}
hunan_GDPPC <- left_join(hunan, hunan_2012,
                   by = c('County' = 'County')) %>% 
  select(1:4, 7,15)

hunan
```

Plot a choropleth map

```{r}
#| fig-width: 14
#| fig-asp: 0.68
tmap_mode("plot")
tm_shape(hunan_GDPPC) +
  tm_fill("GDPPC", 
          style = "quantile", 
          palette = "Blues",
          title = "GDPPC") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Distribution of GDP per capita by district, Hunan Province",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

#### 1.1 Identifying contiguity neighbours using Queen's method

We can see the neighbours list in the first column.

```{r}
nb_queen <- hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry),
         .before = 1)

kable(head(nb_queen,3))
```

**Identifying higher order neighbors**

To identify higher order contiguity nb, we can use `st_nb_lag_cumul()` should be used as shown in the code chunk below.

```{r}
nb2_queen <-  hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry),
         nb2 = st_nb_lag_cumul(nb, 2),
         .before = 1)

kable(head(nb2_queen,3))
```

#### 1.2 Deriving Contiguity Spatial Weights

Deriving Contiguity spatial weights using `sfdep` (wrapper of spdep: allows for mutate using spatial functions) instead of `spdep` library.

In the code chunk below [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) is used to derive a contiguity neighbour list by using Queen's method.

Use two functions from `spdep` library, use less objects.

Note: '*nb*' and '*wt*' fields contain lists.

```{r}
wm_q <- hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry), # calculate by queen default
         wt = st_weights(nb,           # calculate row-stand spatial weight matrix
                         style='W'),
         .before =1)   # put these two columns at front of wm_q sf df

class(wm_q)
```

#### 1.3 Distance based-weights

There are three popularly used distance-based spatial weights, they are:

-   fixed distance weights,

-   adaptive distance weights, and

-   inverse distance weights (IDW).

##### 1.3.1 Deriving fixed distance weights

To determine the upper limit for distance band:

```{r}
geo <- sf::st_geometry(hunan_GDPPC)
nb <- st_knn(geo, k=1, longlat = TRUE)

dists <- unlist(st_nb_dists(geo, nb))
```

> -   [`st_nb_dists()`](https://sfdep.josiahparry.com/reference/st_nb_dists.html) of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation's neighbors list.
>
> -   [`unlist()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unlist) of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.

Derieve summary statistics of nearest nb distances

```{r}
summary(dists)
```

The maximum nearest neighbour distance is 65.8 km, thus we will use threshold value of 66km to ensure each spatial unit as least one neighbour.

Compute the fixed distance weights by using the code chunk below.

```{r}
wm_fd <- hunan_GDPPC %>%
  mutate(nb = st_dist_band(geometry,
                           upper = 66),
               wt = st_weights(nb,
                               style = 'W'),
               .before = 1)
```

-   [`st_dists_band()`](https://sfdep.josiahparry.com/reference/st_dist_band.html) of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).

-   [`st_weights()`](https://sfdep.josiahparry.com/reference/st_weights.html) is then used to calculate polygon spatial weights of the nb list. Note that:

    -   the default `style` argument is set to "W" for row standardized weights, and

    -   the default `allow_zero` is set to TRUE, assigns zero as lagged value to zone without neighbors.

##### 

##### 1.3.2 Deriving adaptive distance weights

```{r}
wm_ad <- hunan_GDPPC %>% 
  mutate(nb = st_knn(geometry,
                     k=8),
         wt = st_weights(nb),  # refers to `nb` generated above light
               .before = 1)
```

-   [`st_knn()`](https://sfdep.josiahparry.com/reference/st_knn.html) of sfdep is used to identify neighbors based on k (i.e. k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).

-   [`st_weights()`](https://sfdep.josiahparry.com/reference/st_weights.html) is then used to calculate polygon spatial weights of the nb list. Note that:

    -   the default `style` argument is set to "W" for row standardized weights, and

    -   the default `allow_zero` is set to TRUE, assigns zero as lagged value to zone without neighbors.

##### 1.3.3 Calculate inverse distance weights

```{r}
wm_idw <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wts = st_inverse_distance(nb, geometry,
                                   scale = 1,
                                   alpha = 1),
         .before = 1)
```

-   [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) of sfdep is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).

-   [`st_inverse_distance()`](https://sfdep.josiahparry.com/reference/st_inverse_distance.html) is then used to calculate inverse distance weights of neighbours on the nb list.

### Part 2 : Performing Global and Local autocorrelation of spatial association using sfdep methods

After deriving the spatial weights matrix `wm_q` (class: 'sf' and 'df' ) ('nb' and 'wt') using the Queen's method, we can calculate

#### 2.1 Computing Global Moran' I (No simulation)

Using the sfdep package, it can be calculated using the code chunk below

```{r}
global_moran_test(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

-   The default for `alternative` argument is "two.sided". Other supported arguments are "greater" or "less". randomization, and

-   By default the `randomization` argument is **TRUE**. If FALSE, under the assumption of normality.

#### 2.2 Computing Global Moran' I (Simulation)

Monte carlo simulation should be used to perform the statistical test. For **sfdep**, it is supported by [`globel_moran_perm()`](https://sfdep.josiahparry.com/reference/global_moran_perm.html). Do not assume normality.

It is always a good practice to use `set.seed()` before performing simulation. This is to ensure that the computation is reproducible.

```{r}
set.seed(1234)
```

```{r}
global_moran_perm(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt,
                  nsim = 99)
```

Since p-value is smaller than 0.05 , we can reject the null hypothesis that the spatial patterns spatial independent. Because the Moran's I statistics is **greater than 0**. We can infer the spatial distribution shows sign of **clustering**.

#### **2.3 Local Moran using sfdep.**

```{r}
lisa <- wm_q %>% 
  mutate(local_moran = local_moran(
    GDPPC, nb,wt, nsim=99),   # straightaway use simulation instead of typical p-value
    .before=1) %>%
  unnest(local_moran)  # due to local_moran() function it produces output in a group object. to see it, need to unnest()
```

The output of local_moran() is a sf df containing the columns

```{r}
colnames(lisa)
```

-   ii: local moran statistic

-   eii: expectation of local moran statistic; for localmoran_permthe permutation sample means

-   var_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations

-   z_ii: p-value

-   skewness: For `localmoran_perm`, the output of e1071::skewness() for the permutation samples underlying the standard deviates

-   kurtosis: For `localmoran_perm`, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.

Print the `lisa` sf df.

The quadrants (HH, LH, HL, LL) is automatically calculated for us.

Usually we use mean for the Moran-Scatterplot. However, if the data GDPPC is **highly skewed**, use **median** instead of mean.

```{r}
kable(head(lisa,3))
```

#### 2.4 Visualising local Moran's I

Use tmap core functions to build a choropleth, using the local moran's I (ii) field

```{r}
tmap_mode("plot")

map1 <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)

map1
```

#### 2.5 Visualising the p-value of local Moran's I

```{r}
tmap_mode("plot")

map2<-tm_shape(lisa) +
  tm_fill("p_ii_sim",
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues",
          title = "local Moran's I p-values") + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

map2
```

Putting both maps side by side

```{r}
#| fig-width: 14
#| fig-asp: 0.68
tmap_arrange(map1, map2, asp=1, ncol = 2)
```

#### **2.6 Visualising LISA map**

LISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran's I of geographical areas and their respective p-values.

In lisa sf data.frame, we can find three fields contain the LISA categories. They are ***mean*****, *median* and *pysal*.** In general, classification in ***mean*** will be used as shown in the code chunk below.

```{r}
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5)
```

```{r}
lisa_sig <- lisa  %>%
  filter(p_ii < 0.05)  # lpot only sig region at 95% Conf.lvl
tmap_mode("plot")

tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

```{r}
#| fig-width: 14
#| fig-asp: 0.68
lisa_sig <- lisa  %>%
  filter(p_ii < 0.05)
tmap_mode("plot")
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

### Part 3: Hot Spot and Cold Spot Area Analysis (HCSA)

HCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.

#### 

#### **3.1 Computing Local Gi\* statistics**

Using the inverse distance spatial weights matrix `wm_idw` derived in section 1.3.3, we can use the `local_gstar_perm()` of sfdep library to compute Gi\* values of each region stored in HCSA, class: 'sf', tbl_df', 'tbl', 'df'.

It contains not just the Gi\* values based on simulated data but also the p-values.

```{r}
HCSA <- wm_idw %>% 
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim=99),   # these 3 var are found in wm_idw
    .before=1) %>% 
  unnest(local_Gi)

HCSA
```

#### 3.2 Visualising Local Gi\* (sig and not-sig)

```{r}
tmap_mode("plot")

map1 <- tm_shape(HCSA) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))+
  tm_layout(main.title = "Gi* of GDPPC",
            main.title.size = 0.8)

map1
```

#### 3.3 **Visualising p-value of HCSA (Gi\*)**

```{r}
tmap_mode("plot")

map2<- tm_shape(HCSA) +
  tm_fill("p_sim",
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues",
          title = "local Gi* sim p-values") + 
  tm_borders(alpha = 0.5)+
  tm_layout(main.title = "p-value of Gi*",
            main.title.size = 0.8)

map2
```

Putting both maps side by side

```{r}
#| fig-width: 14
#| fig-asp: 0.68
tmap_arrange(map1, map2, ncol = 2)
```

#### 

#### 3.4 Visualising hot spot and cold spot areas

To plot only the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below

```{r}
#| fig-width: 14
#| fig-asp: 0.68
HCSA_sig <- HCSA  %>%
  filter(p_sim < 0.05)
tmap_mode("plot")
tm_shape(HCSA) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(HCSA_sig) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4)
```

### Part 4: Emerging hotspots analysis

In this section, we will load in additional set of data. Requires time series data to perform emerging hot spots analysis. The Hunan_GDPPC.csv file contains 10 years worth of GDPPC for each region.

The GDPPC has to be in NUMERICAL field. The time field year can be in numerical /pct or date format.

```{r}
GDPPC <- read_csv('data/aspatial/Hunan_GDPPC.csv')
```

#### 4.1. Creating a Time series cube

Using spacetime() of sfdep. Creating OLAP cube on the fly.

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = 'County',  #location column
                      .time_col = 'Year')    # time column

class(GDPPC_st)
str(GDPPC_st)
```

Note: GDPPC_st looks identical to GDPPC but they are not the same class type. To verify, use the code below

```{r}
is_spacetime_cube(GDPPC_st)
```

#### 4.2 Computing Gi\*

##### 4.2.1 Derive the spatial weights (need to be distance matrix)

The code chunk below will identify nb and derive a [*space-time*]{.underline} inverse weights distance weights. The earlier derived were spatial weights matrix, this time round involves time dimension too.

Note that this dataset now has neighbors and weights for **each time-slice**.

```{r}
GDPPC_nb <- GDPPC_st %>% 
  activate('geometry') %>% # space-time cube has 'geometry' and 'attributes' component, we have to activate the geometry component before able to derive neighbours
  mutate(nb= include_self(st_contiguity(geometry)),  # need to inlude self to get Gi*. otherwise its Gi.
         wt = st_inverse_distance(nb, geometry,
                                  scale =1,
                                  alpha =1),
         .before =1) %>% 
  set_nbs('nb') %>% 
  set_wts('wt')

class(GDPPC_nb)
```

::: callout-note
-   `activate()` of dplyr package is used to activate the geometry context

-   `mutate()` of dplyr package is used to create two new columns *nb* and *wt*.

-   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using `set_nbs()` and `set_wts()`

    -   row order is very important so do not rearrange the observations after using `set_nbs()` or `set_wts()`.
:::

Explaining why need to activate geometry, show table later

```{r}
head(GDPPC_nb)
```

::: callout-note
GDPPC_st : space-time cube

GDPPC_nb: space-time weights matrixs
:::

##### 4.2.2 Computing GI\*

We can use the time-space weights matrix `GDPPC_nb` to manually calculate the local Gi\* for each location.

-   group by *Year* and using `local_gstar_perm()` of sfdep package.

-   use `unnest()` to unnest *gi_star* column of the newly created *gi_stars* data.frame. THe output of this function is grouped and thus we cannot see it unless unnest().

    `gi_stars` df has 1496 rows instead of 88 because we have ten years worth of data

```{r}
gi_stars <- GDPPC_nb %>% 
  group_by(Year) %>%   # look at a time slice, for all locations
  mutate(gi_star = local_gstar_perm(
  GDPPC, nb, wt)) %>% 
  tidyr::unnest(gi_star)

gi_stars
str(gi_stars)
```

Print the `gi_stars` df

```{r}
#kable(head(gi_stars,5))
gi_stars %>% 
  arrange(County) %>% 
  head(5) %>% 
  kable()
  
```

#### 4.3 Mann-Kendall test

With these yearly Gi\* measures for each location, we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county. Test location by location.

```{r}
cbg <- gi_stars %>% 
  ungroup() %>%   # we have grouped it earlier by Year
  filter(County == 'Changsha') %>% 
  select(County, Year, gi_star)

cbg
```

Plotting using ggplot2() functions

```{r}
ggplot(data = cbg,
       aes(x= Year,
           y = gi_star)) +
  geom_line() +
  theme_light()
```

Use `ggploty()` from `plotly` library to make the chart interactive.

```{r}
ggplotly(ggplot(data = cbg,
       aes(x= Year,
           y = gi_star)) +
  geom_line() +
  theme_light())
```

Performing the Mann-Kendall test.

The 'sl' column is the p_value. 'tau' column is the trend

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

There is a slight upward but insignificant trend (\> 0.05)

##### 4.3.1 Perform Mann-Kendall for all locations

To replicate this code for each location (to check for statistical sig for trend in all location), use the `group_by` function. The number of rows is back to 88.

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

Print the output in an interactive table

```{r}
datatable(ehsa)
```

##### 

##### 4.3.2 Sort by top 5 emerging and significant hotspots

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)

kable(emerging)
```

#### 4.4 Emerging hot spot analysis

##### 4.4.1 **Calculating ehsa using spacetime cube**

We will perform EHSA analysis by using [`emerging_hotspot_analysis()`](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html) of sfdep package. It requires

1.  a spacetime object, x (i.e. GDPPC_st)
2.  the quoted name of the variable of interest, .var (i.e. GDPPC)
3.  k argument is used to specify the number of time lags which is set to 1 by default.
4.  nsim map numbers of simulation to be performed.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st, # timeseries data stored in this spacetime cube
  .var='GDPPC',  # spatial element found inside this column
  k=1,   # timelag
  nsim=99
)
```

Print the output

```{r}
datatable(ehsa)
```

##### 4.4.2 **Visualising the distribution of EHSA classes**

plot barchart to check distribution of EHSA classes.

```{r}
ggplotly(ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar())
```

Sporadic cold spots has the highest numbers of county

##### 4.4.3 Visualising EHSA on map

Because ehsa df does not have geometry data, we can join both hunan geospatial df with ehsa to make a sf object.

```{r}
hunan_ehsa <-left_join(hunan, ehsa,
            by=c('County' = 'location'))

class(hunan_ehsa)
```

Use tmap functions to plot choropleth map We can use the p-value inside to plot only the significant emerging hot/cold spots, by filtering rows with p_values \< 0.05

Take note: no pattern (yellow) doesnt mean not-sig (grey)

```{r}
#| fig-width: 14
#| fig-asp: 0.68
ehsa_sig <-  hunan_ehsa %>% 
  filter(p_value < 0.05)

tmap_mode('plot')
#base
tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
  #sif
  tm_shape(ehsa_sig) +
  tm_fill('classification') +
  tm_borders(alpha = 0.4)
```

```{r}
#| eval: false
#| echo: false
#| code-fold: True
#| fig-width: 14
#| fig-asp: 0.68

```
