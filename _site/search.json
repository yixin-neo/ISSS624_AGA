[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I will learn how to import and wrangle geospatial data using appropriate R packages:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#data-acquisition",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#data-acquisition",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.2 Data Acquisition",
    "text": "1.2 Data Acquisition\nIn this hands-on exercise, data is acquired from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#getting-started",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#getting-started",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\nThe code chunk below install and load sf and tidyverse packages into R environment:\n\npacman::p_load(sf, tidyverse)\n\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nThe sp package provides classes and methods for spatial data types in 2005. The sf package was released in 2016 to give standardise support for vector data in R. It is also coherent with tidyverse, that consists of the following (not exhaustive):\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#importing-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.4 Importing Geospatial data",
    "text": "1.4 Importing Geospatial data\nThe data that we will be importing takes the following forms:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n1.4.1 Importing polygon feature data in shapefile format\nst_read() is a func from sf package, used to read files in shapefile format.\ndsn- data source name (aka data path)\nlayer - shapefile name. No extensions like .shp, .dbf, .prj and .shx are needed.\n\nmpsz <- st_read(dsn=\"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ngeospatial objects are multipolygon features\ntotal of 323 multipolygon features and 15 fields in mpsz simple feature data frame.\nmpsz is in svy21 projected coordinates systems\nx extend and y extend of the data are given\n\n\nlibrary(knitr)\nkable(head(mpsz, n = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOBJECTID\nSUBZONE_NO\nSUBZONE_N\nSUBZONE_C\nCA_IND\nPLN_AREA_N\nPLN_AREA_C\nREGION_N\nREGION_C\nINC_CRC\nFMEL_UPD_D\nX_ADDR\nY_ADDR\nSHAPE_Leng\nSHAPE_Area\ngeometry\n\n\n\n\n1\n1\nMARINA SOUTH\nMSSZ01\nY\nMARINA SOUTH\nMS\nCENTRAL REGION\nCR\n5ED7EB253F99252E\n2014-12-05\n31595.84\n29220.19\n5267.381\n1630379.3\nMULTIPOLYGON (((31495.56 30…\n\n\n2\n1\nPEARL’S HILL\nOTSZ01\nY\nOUTRAM\nOT\nCENTRAL REGION\nCR\n8C7149B9EB32EEFC\n2014-12-05\n28679.06\n29782.05\n3506.107\n559816.2\nMULTIPOLYGON (((29092.28 30…\n\n\n3\n3\nBOAT QUAY\nSRSZ03\nY\nSINGAPORE RIVER\nSR\nCENTRAL REGION\nCR\nC35FEFF02B13E0E5\n2014-12-05\n29654.96\n29974.66\n1740.926\n160807.5\nMULTIPOLYGON (((29932.33 29…\n\n\n\n\n\n\n\n1.4.2 Importing polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn='data/geospatial', layer='CyclingPathGazette')\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2248 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\nkable(head(cyclingpath, n = 3))\n\n\n\n\nPLANNING_A\nPLANNING_1\ngeometry\n\n\n\n\nNA\nNA\nMULTILINESTRING ((16001.13 …\n\n\nNA\nNA\nMULTILINESTRING ((16012.86 …\n\n\nNA\nNA\nMULTILINESTRING ((16021.49 …\n\n\n\n\n\n\n\n1.4.3 Importing GIS data in kml format\nThe pre-schools-location-kml is in kml format.\n\npreschool = st_read('data/geospatial/pre-schools-location-kml.kml')\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nNote that preschool is in WSG84 coordinates system (3D).\n\nkable(head(preschool, n = 3))\n\n\n\n\n\n\n\n\n\nName\nDescription\ngeometry\n\n\n\n\nkml_1\n\nPOINT Z (103.7009 1.338325 0)\n\n\nkml_2\n\nPOINT Z (103.8987 1.39044 0)\n\n\nkml_3\n\nPOINT Z (103.8068 1.438017 0)\n\n\n\n\n\n\n\n1.5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nclass(mpsz)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\n1.5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(head(mpsz))\n\nRows: 6\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\"\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\"\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\"\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\"\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05,…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913\n$ SHAPE_Area <dbl> 1630379.3, 559816.2, 160807.5, 595428.9, 387429.4, 1030378.8\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (((29092.28 30…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n1.5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz,3)\n\nSimple feature collection with 3 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28160.23 ymin: 28369.47 xmax: 32362.39 ymax: 30247.18\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO    SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N PLN_AREA_C\n1        1          1 MARINA SOUTH    MSSZ01      Y    MARINA SOUTH         MS\n2        2          1 PEARL'S HILL    OTSZ01      Y          OUTRAM         OT\n3        3          3    BOAT QUAY    SRSZ03      Y SINGAPORE RIVER         SR\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84 29220.19\n2 CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06 29782.05\n3 CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96 29974.66\n  SHAPE_Leng SHAPE_Area                       geometry\n1   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3   1740.926   160807.5 MULTIPOLYGON (((29932.33 29..."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. One of the ways is to use the plot() of R Graphic.\n\nplot(mpsz)\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry (multi-polygon) by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"REGION_N\"])\n\n\n\n\n\n\n\n\n\n\nNote: plot() is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#working-with-projection",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#working-with-projection",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.7 Working with Projection",
    "text": "1.7 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n1.7.1 Assigning EPSG code to a simple feature data frame\nCommon issues:\n\ncoordinate system of the source data was missing (such as due to missing .proj for ESRI shapefile)\nwrongly assigned during the importing process\n\nUsing the st_crs() to check in detail of the mpszreveals that although it claims to be in svy21 (singapore proj sys), reading until end of print shows that it is wrongly in EPSG9001 (singapore uses epsg3414)\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 <- st_transform(mpsz, 3414)\n\nRecheck\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n1.7.2 Transforming the projection of preschool from wgs84 to svy21 (EPSG3414).\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system (3D) to projected coordinate system (2D). This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system (3D).\n\nst_geometry(preschool)\n\nGeometry set for 1925 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\n\nst_crs(preschool)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nNote that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 <- st_transform(preschool, crs=3414)\n\nRecheck\n\n\nGeometry set for 1925 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nNotice that it is in svy21 projected coordinate system now. Furthermore, if we refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.8 Importing and Converting An Aspatial Data",
    "text": "1.8 Importing and Converting An Aspatial Data\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- (long) and y-coordinates (lat) of the data points.\nIn this section, we will learn how to\n\nimport an aspatial data into R environment and save it as a tibble data frame\nconvert it into a simple feature data frame.\n\nThe listings.csv data downloaded from AirBnb will be used.\n\n1.8.1 Importing the aspatial data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings <- read_csv('data/aspatial/listings.csv')\nclass(listings)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings)\n\n[[1]]\n# A tibble: 4,161 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    <dbl> <chr>       <dbl> <chr>     <chr>               <chr>            <dbl>\n 1  50646 Pleasant…  227796 Sujatha   Central Region      Bukit Timah       1.33\n 2  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 3  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 4  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Cozy Blu… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330089 Cozy Blu… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 330095 10 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n# ℹ 4,151 more rows\n# ℹ 11 more variables: longitude <dbl>, room_type <chr>, price <dbl>,\n#   minimum_nights <dbl>, number_of_reviews <dbl>, last_review <date>,\n#   reviews_per_month <dbl>, calculated_host_listings_count <dbl>,\n#   availability_365 <dbl>, number_of_reviews_ltm <dbl>, license <chr>\n\n\nOther ways of displaying tabular data in R:\n\nkablegtDT (interactive table)\n\n\n\nkable(head(listings))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_name\nneighbourhood_group\nneighbourhood\nlatitude\nlongitude\nroom_type\nprice\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\ncalculated_host_listings_count\navailability_365\nnumber_of_reviews_ltm\nlicense\n\n\n\n\n50646\nPleasant Room along Bukit Timah\n227796\nSujatha\nCentral Region\nBukit Timah\n1.33432\n103.7852\nPrivate room\n80\n92\n18\n2014-12-26\n0.18\n1\n365\n0\nNA\n\n\n71609\nEnsuite Room (Room 1 & 2) near EXPO\n367042\nBelinda\nEast Region\nTampines\n1.34537\n103.9589\nPrivate room\n145\n92\n20\n2020-01-17\n0.15\n6\n340\n0\nNA\n\n\n71896\nB&B Room 1 near Airport & EXPO\n367042\nBelinda\nEast Region\nTampines\n1.34754\n103.9596\nPrivate room\n85\n92\n24\n2019-10-13\n0.18\n6\n265\n0\nNA\n\n\n71903\nRoom 2-near Airport & EXPO\n367042\nBelinda\nEast Region\nTampines\n1.34531\n103.9610\nPrivate room\n85\n92\n47\n2020-01-09\n0.34\n6\n365\n0\nNA\n\n\n275344\n15 mins to Outram MRT Single Room\n1439258\nKay\nCentral Region\nBukit Merah\n1.28836\n103.8114\nPrivate room\n49\n60\n14\n2022-07-09\n0.11\n44\n296\n1\nS0399\n\n\n289234\nBooking for 3 bedrooms\n367042\nBelinda\nEast Region\nTampines\n1.34490\n103.9598\nPrivate room\n184\n92\n12\n2019-01-01\n0.10\n6\n285\n0\nNA\n\n\n\n\n\n\n\n\nlibrary(gt)\nhead(listings) %>% gt() %>% tab_header(title = \"AirBnB listings\")\n\n\n\n\n\n  \n    \n      AirBnB listings\n    \n    \n    \n      id\n      name\n      host_id\n      host_name\n      neighbourhood_group\n      neighbourhood\n      latitude\n      longitude\n      room_type\n      price\n      minimum_nights\n      number_of_reviews\n      last_review\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n      number_of_reviews_ltm\n      license\n    \n  \n  \n    50646\nPleasant Room along Bukit Timah\n227796\nSujatha\nCentral Region\nBukit Timah\n1.33432\n103.7852\nPrivate room\n80\n92\n18\n2014-12-26\n0.18\n1\n365\n0\nNA\n    71609\nEnsuite Room (Room 1 & 2) near EXPO\n367042\nBelinda\nEast Region\nTampines\n1.34537\n103.9589\nPrivate room\n145\n92\n20\n2020-01-17\n0.15\n6\n340\n0\nNA\n    71896\nB&B  Room 1 near Airport & EXPO\n367042\nBelinda\nEast Region\nTampines\n1.34754\n103.9596\nPrivate room\n85\n92\n24\n2019-10-13\n0.18\n6\n265\n0\nNA\n    71903\nRoom 2-near Airport & EXPO\n367042\nBelinda\nEast Region\nTampines\n1.34531\n103.9610\nPrivate room\n85\n92\n47\n2020-01-09\n0.34\n6\n365\n0\nNA\n    275344\n15 mins to Outram MRT Single Room\n1439258\nKay\nCentral Region\nBukit Merah\n1.28836\n103.8114\nPrivate room\n49\n60\n14\n2022-07-09\n0.11\n44\n296\n1\nS0399\n    289234\nBooking for 3 bedrooms\n367042\nBelinda\nEast Region\nTampines\n1.34490\n103.9598\nPrivate room\n184\n92\n12\n2019-01-01\n0.10\n6\n285\n0\nNA\n  \n  \n  \n\n\n\n\n\n\n\nlibrary(DT)\ndatatable(head(listings), class = 'cell-border stripe', options = list(pageLength = 3))\n\n\n\n\nTwo useful fields we need are latitude and longitude and they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n1.8.2 Creating a simple feature data frame from an aspatial data frame\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages.\nEPSG 4326 is associated with WGS84.\n\nlistings_sf <- st_as_sf(listings,\n                        coords = c('longitude','latitude'),\n                        crs=4326) %>%\n  st_transform(crs=3414)\n\nclass(listings_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n\nLet us examine the content of our newly created sf dataframe\n\nglimpse(listings_sf)\n\nRows: 4,161\nColumns: 17\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275344, 289…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 145, 85, 85, 49, 184, 79, 49, 55, 5…\n$ minimum_nights                 <dbl> 92, 92, 92, 92, 60, 92, 92, 60, 60, 60,…\n$ number_of_reviews              <dbl> 18, 20, 24, 47, 14, 12, 133, 17, 12, 3,…\n$ last_review                    <date> 2014-12-26, 2020-01-17, 2019-10-13, 20…\n$ reviews_per_month              <dbl> 0.18, 0.15, 0.18, 0.34, 0.11, 0.10, 1.0…\n$ calculated_host_listings_count <dbl> 1, 6, 6, 6, 44, 6, 7, 44, 44, 44, 6, 7,…\n$ availability_365               <dbl> 365, 340, 265, 365, 296, 285, 365, 181,…\n$ number_of_reviews_ltm          <dbl> 0, 0, 0, 0, 1, 0, 0, 3, 2, 0, 1, 0, 0, …\n$ license                        <chr> NA, NA, NA, NA, \"S0399\", NA, NA, \"S0399…\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\nA new column geometry has been added at the back of the df. Additionally, lat long columns were both dropped from the df."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.9 Geoprocessing with sf package",
    "text": "1.9 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, we will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n1.9.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nA buffer is a zone around a spatial object, recall that cyclingpath is a multiline-string sf object.\n\nst_geometry(cyclingpath)\n\nGeometry set for 2248 features \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cyclingpath .\n\nbuffer_cycling <- st_buffer(cyclingpath,\n                            dist = 5,\n                            nQuadSegs = 30)\n\nTake a peak at this df before calculating area.\n\nhead(buffer_cycling)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 15867.37 ymin: 36795.67 xmax: 16026.95 ymax: 36953.73\nProjected CRS: SVY21\n  PLANNING_A PLANNING_1                       geometry\n1       <NA>       <NA> POLYGON ((16004.15 36799.78...\n2       <NA>       <NA> POLYGON ((16013.15 36849.86...\n3       <NA>       <NA> POLYGON ((16016.91 36892.98...\n4       <NA>       <NA> POLYGON ((16017.59 36864, 1...\n5       <NA>       <NA> POLYGON ((16022.36 36900.57...\n6       <NA>       <NA> POLYGON ((15903.87 36941.12...\n\n\nNow, we will calculate the area of the buffers as shown in the code chunk below.\nWe are also adding a derived column to buffer_cycling too.\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\nkable(head(buffer_cycling))\n\n\n\n\n\n\n\n\n\n\nPLANNING_A\nPLANNING_1\ngeometry\nAREA\n\n\n\n\nNA\nNA\nPOLYGON ((16004.15 36799.78…\n186.2934 [m^2]\n\n\nNA\nNA\nPOLYGON ((16013.15 36849.86…\n293.4840 [m^2]\n\n\nNA\nNA\nPOLYGON ((16016.91 36892.98…\n284.8275 [m^2]\n\n\nNA\nNA\nPOLYGON ((16017.59 36864, 1…\n144.8915 [m^2]\n\n\nNA\nNA\nPOLYGON ((16022.36 36900.57…\n281.2016 [m^2]\n\n\nNA\nNA\nPOLYGON ((15903.87 36941.12…\n398.7081 [m^2]\n\n\n\n\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1556978 [m^2]\n\n\nGood Job!\nMission Accomplished!\n\n\n1.9.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nBefore that, lets double confirm both data are using same projection system.\n\nidentical(st_crs(mpsz3414), st_crs(preschool3414))\n\n[1] TRUE\n\n\nThe solution:\nThe code chunk below performs two operations at one go.\n\nidentify pre-schools located inside each Planning Subzone by using st_intersects().\nlength() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nmpsz3414$`PreSch Count` <- lengths(st_intersects(mpsz3414,preschool3414))\n\n\n\n\n\n\n\nst_intersects(): count points in polygon\nst_intersection(): find polygon areas overlap\n\n\n\nNow check summary stats of PreSch Count column in each subzone.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    3.00    5.96    9.00   58.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package can be used.\n\nkable(top_n(mpsz3414, 3, `PreSch Count`) %>%\n        arrange(desc(`PreSch Count`)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOBJECTID\nSUBZONE_NO\nSUBZONE_N\nSUBZONE_C\nCA_IND\nPLN_AREA_N\nPLN_AREA_C\nREGION_N\nREGION_C\nINC_CRC\nFMEL_UPD_D\nX_ADDR\nY_ADDR\nSHAPE_Leng\nSHAPE_Area\nPreSch Count\ngeometry\n\n\n\n\n189\n2\nTAMPINES EAST\nTMSZ02\nN\nTAMPINES\nTM\nEAST REGION\nER\n21658EAAF84F4D8D\n2014-12-05\n41122.55\n37392.39\n10180.624\n4339824\n58\nMULTIPOLYGON (((42196.76 38…\n\n\n290\n3\nWOODLANDS EAST\nWDSZ03\nN\nWOODLANDS\nWD\nNORTH REGION\nNR\nC90769E43EE6B0F2\n2014-12-05\n24506.64\n46991.63\n6603.608\n2553464\n47\nMULTIPOLYGON (((24786.75 46…\n\n\n199\n4\nBEDOK NORTH\nBDSZ04\nN\nBEDOK\nBD\nEAST REGION\nER\nA2254301F85C1EDF\n2014-12-05\n39429.21\n34737.62\n8414.962\n3203663\n31\nMULTIPOLYGON (((40284.24 35…\n\n\n\n\n\nTo calculate the density (# schools/subzone area) of preschool by planning subzone :\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(AREA = st_area(mpsz3414),\n         DENSITY = `PreSch Count` /AREA * 1000000) %>% \n  arrange(desc(DENSITY))\n\nkable(head(mpsz3414))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOBJECTID\nSUBZONE_NO\nSUBZONE_N\nSUBZONE_C\nCA_IND\nPLN_AREA_N\nPLN_AREA_C\nREGION_N\nREGION_C\nINC_CRC\nFMEL_UPD_D\nX_ADDR\nY_ADDR\nSHAPE_Leng\nSHAPE_Area\nPreSch Count\nAREA\nDENSITY\ngeometry\n\n\n\n\n27\n8\nCECIL\nDTSZ08\nY\nDOWNTOWN CORE\nDT\nCENTRAL REGION\nCR\n65AA82AF6F4D925D\n2014-12-05\n29730.20\n29011.33\n2116.0947\n196619.86\n7\n196619.86 [m^2]\n35.60169 [1/m^2]\nMULTIPOLYGON (((29808.18 28…\n\n\n278\n3\nMANDAI ESTATE\nMDSZ03\nN\nMANDAI\nMD\nNORTH REGION\nNR\nF6266F7368DBB9AB\n2014-12-05\n27082.70\n45367.46\n1633.7084\n143137.94\n5\n143137.94 [m^2]\n34.93134 [1/m^2]\nMULTIPOLYGON (((27119.56 45…\n\n\n37\n4\nPHILLIP\nDTSZ04\nY\nDOWNTOWN CORE\nDT\nCENTRAL REGION\nCR\n615D4EDDEF809F8E\n2014-12-05\n29706.72\n29744.91\n871.5549\n39437.94\n1\n39437.94 [m^2]\n25.35630 [1/m^2]\nMULTIPOLYGON (((29814.11 29…\n\n\n291\n3\nSEMBAWANG CENTRAL\nSBSZ03\nN\nSEMBAWANG\nSB\nNORTH REGION\nNR\n772A64AB9A93FC3A\n2014-12-05\n26268.73\n47558.08\n3955.1176\n962437.40\n23\n962437.40 [m^2]\n23.89766 [1/m^2]\nMULTIPOLYGON (((26311.14 46…\n\n\n253\n3\nSERANGOON NORTH\nSGSZ03\nN\nSERANGOON\nSG\nNORTH-EAST REGION\nNER\nC685042EC58E5C55\n2014-12-05\n32458.80\n39597.64\n3610.7324\n684704.30\n15\n684704.30 [m^2]\n21.90727 [1/m^2]\nMULTIPOLYGON (((32860.5 397…\n\n\n272\n3\nSENGKANG TOWN CENTRE\nSESZ03\nN\nSENGKANG\nSE\nNORTH-EAST REGION\nNER\n5A2D0E9E6B285069\n2014-12-05\n35163.81\n41501.14\n5216.4005\n1455507.86\n30\n1455507.86 [m^2]\n20.61136 [1/m^2]\nMULTIPOLYGON (((35615.75 40…\n\n\n\n\n\nThe table above shows the top 6 highest density subzones ."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#explorotary-data-analysis-eda",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#explorotary-data-analysis-eda",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "1.10 Explorotary Data Analysis (EDA)",
    "text": "1.10 Explorotary Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, wewill learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$DENSITY)\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nStaticInteractive (plotly)\n\n\n\nq <- quantile(as.numeric(mpsz3414$DENSITY), probs = c(0.25, 0.5, 0.75))\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(DENSITY)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(xintercept = q[2]+1, linetype='dashed', size = 0.5, color='blue') +\n  geom_vline(xintercept = q[3]+1, linetype='dashed', size = 0.5) +\n  annotate('text' , x= 4, y=75, label='50th \\npercentile', size = 2) +\n  annotate('text' , x= 9, y=75, label='75th \\npercentile', size = 2) +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\nlibrary(plotly)\nq <- quantile(as.numeric(mpsz3414$DENSITY), probs = c(0.25, 0.5, 0.75))\n\np <- ggplot(data=mpsz3414, \n       aes(x= as.numeric(DENSITY)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(xintercept = q[2]+1, linetype='dashed', size = 0.5, color='blue') +\n  geom_vline(xintercept = q[3]+1, linetype='dashed', size = 0.5) +\n  annotate('text' , x= 4, y=75, label='50th \\npercentile', size = 2) +\n  annotate('text' , x= 9, y=75, label='75th \\npercentile', size = 2) +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\nggplotly(p)\n\n\n\n\n\n\n\n\nDIY: Using ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(DENSITY)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_1.html#theories",
    "href": "Hands-on_Ex1/Hands-on_Ex1_1.html#theories",
    "title": "Hands-on Exercise 1.1: Geospatial Data Wrangling with R",
    "section": "Theories",
    "text": "Theories\n\nKML and shapefiles\nA KMZ file is a zipped (or compressed) KML file, and a SHZ is a zipped/compressed Shapefile.\nA shapefile is an Esri vector data storage format for storing the location, shape, and attributes of geographic features. It is stored as a set of related files and contains one feature class.\nThe shapefile format can spatially describe vector features: points, lines, and polygons, representing, for example, water wells, rivers, and lakes. Each item usually has attributes that describe it, such as name or temperature.\nKML and Shapefiles could contain the exact same data, however KML (Keyhole Markup Language) is much more suited to displaying time based track information, whereas shapefiles are more suited to displaying Geometries, like boundaries, areas, roads, etc.\nShapefiles are composed of 3 mandatory files\n·       . shp (geometry), <- multipolygon, polylines or points. Can only be one type in each file , can combined in layers\n·       . dbf (attributes) <- table\n·       . shx (index)  <- binds first two together\n\n\nGCS [Geographic Coordinate System] (3D) and PCS [Projected Coordinate System ] (2D)\nGCS: uses lat, long, elevation to locate positions on Earth. Units are in degree and metres. Earth is represented as a sphere. Eg. WGS84 (world Gedetic system 1984)\nPCS: Units are usually metres to locate position on a Flat surface. Involves projecting 3D Earth into a 2D plane. It distorts the true shapes, areas, distances, or directions to some extent, depending on the projection method chosen.\nPreserve:\n·       Conformal projections minimize distortion in shape\n·       Equidistant projections minimize distortion in distance\n·       Equal-area projection minimize distortion in area\n·       Azimuthal or True-direction projections minimize distortion in direction.\nGoogle maps uses Mercator projection system. It is chosen as it preserves direction and angles. It is useful for navigation (google map) . It is originally created for sea navigation in older days. The cons are that this projection does not preserve area and shape.\nSingapore uses SVY21 or the EPSG:3414 projected coordinate system.\n\n\nGeospatial data handling functions\n\nst_read & read sf: read simple features from file or database, or retrieve layer names and their geometry type(s)\nst write &write_sf: write simple features object to file or database\nst_as_sf: convert a sf object from a non-geospatial tabular data frame\nst as_text: convert to Well Known Text(WKT)\nst as_binary: convert to Well Known Binary(WKB)\nst_as_sfc: convert geometries to sfc (e.g., from WKT, WKB) as(x, “Spatial”): convert to Spatial*\nst transform(x, crs, …): convert coordinates of x to a different coordinate reference system\n\nThe code chunk below allows us to unsf the mpsz and work tbl_df or data.frame.\n\nmpsz_tbl_df <- as_tibble(mpsz)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_2.html",
    "href": "Hands-on_Ex1/Hands-on_Ex1_2.html",
    "title": "Hands-on Exercise 1.2: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_2.html#getting-started",
    "href": "Hands-on_Ex1/Hands-on_Ex1_2.html#getting-started",
    "title": "Hands-on Exercise 1.2: Choropleth Mapping with R",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\nIn this hands-on exercise, we learn how to plot functional and truthful choropleth maps by using r packages called tmap package.\nBeside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nLets us first load all the required libraries.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_2.html#importing-data-into-r",
    "href": "Hands-on_Ex1/Hands-on_Ex1_2.html#importing-data-into-r",
    "title": "Hands-on Exercise 1.2: Choropleth Mapping with R",
    "section": "2.3 Importing Data into R",
    "text": "2.3 Importing Data into R\n\n2.3.1 The Data\nThe Two datasets will be used to create the choropleth map are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore, the specific link can be found here. Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n2.3.2 Importing Geospatial Data into R\n\nmpsz <- st_read(dsn='data/geospatial',\n                layer = 'MP14_SUBZONE_WEB_PL')\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nCheck the projection system of mpsz. It is not in svy21 or epsg3414. We will need to convert it later.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nTake a look at first few records of data\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nLets convert to EPSG3414 now.\n\nmpsz3414 <- st_transform(mpsz, 3414)\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nhead(mpsz3414,3)\n\nSimple feature collection with 3 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28160.23 ymin: 28369.47 xmax: 32362.39 ymax: 30247.18\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO    SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N PLN_AREA_C\n1        1          1 MARINA SOUTH    MSSZ01      Y    MARINA SOUTH         MS\n2        2          1 PEARL'S HILL    OTSZ01      Y          OUTRAM         OT\n3        3          3    BOAT QUAY    SRSZ03      Y SINGAPORE RIVER         SR\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84 29220.19\n2 CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06 29782.05\n3 CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96 29974.66\n  SHAPE_Leng SHAPE_Area                       geometry\n1   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n\n\n\n\n2.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2000to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\nhead(popdata)\n\n# A tibble: 6 × 7\n  PA         SZ                     AG     Sex   TOD                   Pop  Time\n  <chr>      <chr>                  <chr>  <chr> <chr>               <dbl> <dbl>\n1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 1- and 2-Room …     0  2011\n2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 3-Room Flats       10  2011\n3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 4-Room Flats       30  2011\n4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 5-Room and Exe…    50  2011\n5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HUDC Flats (exclud…     0  2011\n6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males Landed Properties       0  2011\n\n\nSummary stats using skimr package.\n\nlibrary(skimr)\nskim(popdata)\n\n\nData summary\n\n\nName\npopdata\n\n\nNumber of rows\n984656\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nPA\n0\n1\n4\n23\n0\n55\n0\n\n\nSZ\n0\n1\n4\n29\n0\n335\n0\n\n\nAG\n0\n1\n6\n11\n0\n19\n0\n\n\nSex\n0\n1\n5\n7\n0\n2\n0\n\n\nTOD\n0\n1\n6\n39\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPop\n0\n1\n39.86\n132.25\n0\n0\n0\n10\n2860\n▇▁▁▁▁\n\n\nTime\n0\n1\n2015.51\n2.88\n2011\n2013\n2016\n2018\n2020\n▇▇▇▇▇"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_2.html#data-preparation",
    "href": "Hands-on_Ex1/Hands-on_Ex1_2.html#data-preparation",
    "title": "Hands-on Exercise 1.2: Choropleth Mapping with R",
    "section": "2.3.4 Data preparation",
    "text": "2.3.4 Data preparation\nPrepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n2.3.4.1 Data Wrangling\n\npivot_wider() (rows to column headers) of tidyr package, and\nmutate() (create new cal col), filter() (subset rows), group_by() and select() (select cols) of dplyr package\n\nThe complete code chunk:\n\npopdata2020 <- popdata %>% \n  filter(Time==2020) %>% \n  group_by(PA,SZ,AG) %>%   #<< for calculating POP column below\n  summarise(`POP` = sum(Pop)) %>% \n  ungroup() %>%\n  pivot_wider(names_from = AG,\n              values_from = POP) %>% \n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %>% \n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+rowSums(.[15])) %>%\n  mutate(AGED = rowSums(.[16:21])) %>%\n  mutate(TOTAL = rowSums(.[3:21])) %>% \n  mutate(DEPENDENCY = (YOUNG+AGED)/`ECONOMY ACTIVE`) %>% \n  select(PA, SZ,YOUNG,'ECONOMY ACTIVE', AGED, TOTAL, DEPENDENCY)\n\nhead(popdata2020)\n\n# A tibble: 6 × 7\n  PA         SZ                    YOUNG `ECONOMY ACTIVE`  AGED TOTAL DEPENDENCY\n  <chr>      <chr>                 <dbl>            <dbl> <dbl> <dbl>      <dbl>\n1 Ang Mo Kio Ang Mo Kio Town Cent…  1290             2760   760  4810      0.743\n2 Ang Mo Kio Cheng San              5640            16460  6050 28150      0.710\n3 Ang Mo Kio Chong Boon             5100            15000  6470 26570      0.771\n4 Ang Mo Kio Kebun Bahru            4620            13010  5120 22750      0.749\n5 Ang Mo Kio Sembawang Hills        1880             3630  1310  6820      0.879\n6 Ang Mo Kio Shangri-La             3330             9050  3610 15990      0.767\n\n\nTo understand the first 6 lines of code in the code chunk above, print the output:\n\n\n# A tibble: 6 × 21\n  PA       SZ    `0_to_4` `10_to_14` `15_to_19` `20_to_24` `25_to_29` `30_to_34`\n  <chr>    <chr>    <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n1 Ang Mo … Ang …      170        280        340        270        260        310\n2 Ang Mo … Chen…     1060       1040       1160       1330       1720       2020\n3 Ang Mo … Chon…      850       1020       1070       1310       1610       1890\n4 Ang Mo … Kebu…      680        960       1010       1170       1410       1420\n5 Ang Mo … Semb…      210        400        450        500        500        340\n6 Ang Mo … Shan…      560        640        700        860        970       1030\n# ℹ 13 more variables: `35_to_39` <dbl>, `40_to_44` <dbl>, `45_to_49` <dbl>,\n#   `50_to_54` <dbl>, `55_to_59` <dbl>, `5_to_9` <dbl>, `60_to_64` <dbl>,\n#   `65_to_69` <dbl>, `70_to_74` <dbl>, `75_to_79` <dbl>, `80_to_84` <dbl>,\n#   `85_to_89` <dbl>, `90_and_over` <dbl>\n\n\n\n2.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nSince mpsz’s SUBZONE_N and PLN_AREA_N are in uppercase, we have to convert PA and SZ fields in popdata2020 into all uppercase using:\n\nmutate.at()<- mutate multiple columns\nfirst argument of mutate_at(): .var <- list of columns generated by vars()\nsecond argument: .funs <- a function fun, a quosure style lambda. The function used in toupper() <- to upper case\n\nThere are many ways to achieve the final output\n\nProf’s bookAlternative\n\n\n\npopdata2020 <- popdata2020 %>% \n  mutate_at(.vars= vars(PA, SZ),\n            .funs = funs(toupper)) %>% \n  filter(`ECONOMY ACTIVE` >0)\n\npopdata2020\n\n# A tibble: 234 × 7\n   PA         SZ                   YOUNG `ECONOMY ACTIVE`  AGED TOTAL DEPENDENCY\n   <chr>      <chr>                <dbl>            <dbl> <dbl> <dbl>      <dbl>\n 1 ANG MO KIO ANG MO KIO TOWN CEN…  1290             2760   760  4810      0.743\n 2 ANG MO KIO CHENG SAN             5640            16460  6050 28150      0.710\n 3 ANG MO KIO CHONG BOON            5100            15000  6470 26570      0.771\n 4 ANG MO KIO KEBUN BAHRU           4620            13010  5120 22750      0.749\n 5 ANG MO KIO SEMBAWANG HILLS       1880             3630  1310  6820      0.879\n 6 ANG MO KIO SHANGRI-LA            3330             9050  3610 15990      0.767\n 7 ANG MO KIO TAGORE                1940             4480  1530  7950      0.775\n 8 ANG MO KIO TOWNSVILLE            4190            11950  5100 21240      0.777\n 9 ANG MO KIO YIO CHU KANG EAST     1110             2410   750  4270      0.772\n10 ANG MO KIO YIO CHU KANG WEST     5690            13750  4680 24120      0.754\n# ℹ 224 more rows\n\n\nThe reason for filtering is because some subzones are not inhibited by residents as seen below.\n\n\n\npopdata2020 %>%\n  mutate(PA = toupper(PA),\n         SZ = toupper(SZ)) %>% \n  filter(`ECONOMY ACTIVE` >0)\n\n\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <-left_join(mpsz3414, popdata2020,\n                         by = c('SUBZONE_N' = 'SZ'))\nmpsz_pop2020\n\nSimple feature collection with 323 features and 21 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area              PA YOUNG ECONOMY ACTIVE AGED\n1  29220.19   5267.381  1630379.3            <NA>    NA             NA   NA\n2  29782.05   3506.107   559816.2          OUTRAM   930           3130 2120\n3  29974.66   1740.926   160807.5 SINGAPORE RIVER     0             40   10\n4  29933.77   3313.625   595428.9     BUKIT MERAH  2600           7450 3320\n5  30005.70   2825.594   387429.4     BUKIT MERAH  2760           6160 1740\n6  29991.38   4428.913  1030378.8     BUKIT MERAH  2800           7340 3420\n7  30230.86   3275.312   551732.0     BUKIT MERAH  2750           8080 3610\n8  30222.86   2208.619   290184.7 SINGAPORE RIVER     0             50   10\n9  29893.78   6571.323  1084792.3      QUEENSTOWN  1120           2770  610\n10 30104.18   3454.239   631644.3      QUEENSTOWN    30            160   60\n   TOTAL DEPENDENCY                       geometry\n1     NA         NA MULTIPOLYGON (((31495.56 30...\n2   6180  0.9744409 MULTIPOLYGON (((29092.28 30...\n3     50  0.2500000 MULTIPOLYGON (((29932.33 29...\n4  13370  0.7946309 MULTIPOLYGON (((27131.28 30...\n5  10660  0.7305195 MULTIPOLYGON (((26451.03 30...\n6  13560  0.8474114 MULTIPOLYGON (((25899.7 297...\n7  14440  0.7871287 MULTIPOLYGON (((27746.95 30...\n8     60  0.2000000 MULTIPOLYGON (((29351.26 29...\n9   4500  0.6245487 MULTIPOLYGON (((20996.49 30...\n10   250  0.5625000 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\n\n\nImportant\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nOriginal # rows in mpsz3414 = 323\nOriginal # of rows in popdata2022 = 234\nFinal # of rows in mpsz_pop2020 = 323\nWrite the data as rds, can preserve the format of data.\n\n# write_rds(mpsz_pop2020, 'data/rds/mpszpop2020.rds')\n# mpsz_pop2020 <- readRDS('data/rds/mpszpop2020.rds')\n\nLets take a look at the df mpsz_pop2020"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_2.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex1/Hands-on_Ex1_2.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 1.2: Choropleth Mapping with R",
    "section": "2.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "2.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n2.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode('plot')\n#tmap_mode('view')\n#ttm()\n#last_map()\n#tmap_options(check.and.fix = TRUE)\n\nqtm(mpsz_pop2020,\n    fill='DEPENDENCY')\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n2.4.2 Creating a Choropleth map by using tmap’s elements\ntmap’ drawing elements (unlike qtm() ) is able to give us finer control over our chloropleth map.\ntm_shape() <- define input data and specify the shape object / spatial data object\ntm_fill() <- fills polygons (no border)\n\nstyle: method to process the color scale when col (data variable) is a numeric variable. To process numeric and categorical use “cat”\npalette : palettes names or vectors of colors. default is taken from tm_layout’s aes.paletttes argument. To reverse the colour scheme , add a “-” prefix.\n\ntm_borders() <- draws the borders of polygons (alpha is 0-1, transparency)\ntm_polygons() (= tm_fill() + tm_borders() )<- fills the polygon and draws the polygon borders\n*qtm = shape + polygon or * qtm = shape + fill + border\ntm_layout() <- specify map layout\n\naes.palette <- ‘seq’ (sequential palettes), ‘div’ (diverging palettes) , ‘cat’ (categorical palettes)\n\ntm_compass() <- create map compass\ntm_scale_bar() <- creates scale bar\ntm_grid() <- creates grid lines (alpha is 0-1, transparency of grid lines)\ntm_credits() <- create a text for credits\n\nHigh quality graphmpsz_pop2020 table\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Reds\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nlibrary(knitr)\nkable(head(mpsz_pop2020,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOBJECTID\nSUBZONE_NO\nSUBZONE_N\nSUBZONE_C\nCA_IND\nPLN_AREA_N\nPLN_AREA_C\nREGION_N\nREGION_C\nINC_CRC\nFMEL_UPD_D\nX_ADDR\nY_ADDR\nSHAPE_Leng\nSHAPE_Area\nPA\nYOUNG\nECONOMY ACTIVE\nAGED\nTOTAL\nDEPENDENCY\ngeometry\n\n\n\n\n1\n1\nMARINA SOUTH\nMSSZ01\nY\nMARINA SOUTH\nMS\nCENTRAL REGION\nCR\n5ED7EB253F99252E\n2014-12-05\n31595.84\n29220.19\n5267.381\n1630379.3\nNA\nNA\nNA\nNA\nNA\nNA\nMULTIPOLYGON (((31495.56 30…\n\n\n2\n1\nPEARL’S HILL\nOTSZ01\nY\nOUTRAM\nOT\nCENTRAL REGION\nCR\n8C7149B9EB32EEFC\n2014-12-05\n28679.06\n29782.05\n3506.107\n559816.2\nOUTRAM\n930\n3130\n2120\n6180\n0.9744409\nMULTIPOLYGON (((29092.28 30…\n\n\n3\n3\nBOAT QUAY\nSRSZ03\nY\nSINGAPORE RIVER\nSR\nCENTRAL REGION\nCR\nC35FEFF02B13E0E5\n2014-12-05\n29654.96\n29974.66\n1740.926\n160807.5\nSINGAPORE RIVER\n0\n40\n10\n50\n0.2500000\nMULTIPOLYGON (((29932.33 29…\n\n\n\n\n\n\n\n\n\n2.4.2.1 Drawing a base map\nBasic building blocks are tm_fill() and tm_polygons().\ntm_polygons() = fill + borders\n\ntm_borders()tm_fill()\n\n\nBase Map\n\ntm_shape(mpsz_pop2020) +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\nTo show the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY')\n\n\n\n\n\n\n\n\n\n2.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\nNo variableWith variable\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n2.4.2.3 Drawing a choropleth map using tm_fill() + *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nApart from alpha (transparency number 0 - 1), other arguments are\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n2.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe codes below uses quantile classification with 5 classes.\n\nfill + borderpolygons\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing equal classification method\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nDistribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",n = 5,\n          style = \"jenks\" )\n\n\n\n\n\n\n\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\nUsing tmap_arrange:\n\nkmeans<- tm_shape(mpsz_pop2020)+\n    tm_fill(\"DEPENDENCY\", n = 5, style = 'kmeans') +\n  tm_borders(alpha = 0.5)\n\nsd <- tm_shape(mpsz_pop2020)+\n    tm_fill(\"DEPENDENCY\", n = 5, style = 'sd') +\n  tm_borders(alpha = 0.5)\n\nfisher <- tm_shape(mpsz_pop2020)+\n    tm_fill(\"DEPENDENCY\", n = 5, style = 'fisher') +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(kmeans, sd, fisher, asp = 4, nrow=3)\n\n\n\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\nAssigning multiple values to at least one of the aesthetics arguments.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c('DEPENDENCY','DEPENDENCY'),\n          style= c('jenks','jenks'),\n          n = c(3,10),\n          palette= list('Greens', 'Greens')) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c('right','bottom'),\n            legend.width = 0.5,\n            legend.height = 0.4,\n            legend.text.size = 0.35 )\n\n\n\n\n\n\n2.4.3.2 Plotting choropleth maps with custome break\nThe breakpoints can be set explicitly by means of the breaks argument to the tm_fill().\n\nbreaks include min and max\nto have n categories, n+1 elements to be specified in breaks option\nvalues must be in increasing order\n\nGood practise: descriptive statistics on variable before setting break points\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nWith reference to summary stats and boxplot above,\n\nbreak points are 0.6, 0.7, 0.8, 0.9\nmin = 0 and max = 1.0\nbreak vector is thus c(0, 0.6, 0.7, 0.8, 0.9, 1.0)\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", breaks = c(0, 0.6, 0.7, 0.8, 0.9, 1.0)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n2.4.4 Colour scheme\n\n2.4.4.1 Using ColourBrewer palette (predefined)\n(YIOrRd, YIORrBr, YIGnBu, YIGn, Reds, RdPu, Purples, PuRd, PuBuGn, PuBu, OrRd, Oranges, Greys, Greens, GnBu, BuPu, BuGn, Blues)\n\n\n\n\n\nAssign the preferred colour to palette argument of tm_fill()\n\nnormalreverse\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nReverse the colour scheme by adding “-”\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2.4.5 Map Layouts\n\ncustomise title, scale bar, compass, margins, aspect ratios\nother than colour palette and data classification (breaks) which is done in tm_fill()\n\n\n2.4.5.1 Map Legend\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n2.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style()\nPredefined styles: ‘white’, ‘gray’, ‘natural’, ‘bw’, ‘classic’, ‘cobalt’, albatross’, ‘beaver’, ‘col_blind’, ‘watercolor’\nBelow is an example of classic style\n\ngraynaturalbwclassiccobaltbeaver\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)+\n  tmap_style('gray')\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)+\n  tmap_style('natural')\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)+\n  tmap_style('bw')\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)+\n  tmap_style('classic')\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-RdPu\") +\n  tm_borders(alpha = 0.5)+\n  tmap_style('cobalt')\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Purples\") +\n  tm_borders(alpha = 0.5)+\n  tmap_style('beaver')\n\n\n\n\n\n\n\n\n\n2.4.5.3 Cartographic Furniture\nCan include other map furniture like compass, scale bar, and grid lines\ntmap_style has to be used at the last, think cannot mix with tm\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Reds\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.0,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style('natural')\n\n\n\n\nTo reset the default style, refer below\n\ntmap_style(\"white\")\n\n\n\n\n2.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nSmall multiple choropleth maps are created by\n\n2.4.6.1.1 Defining ncols in tm_fill() : c(‘YOUNG’, ‘AGED’)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n            legend.height = 0.35, \n            legend.width = 0.35) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n2.4.6.1.2 Assigning multiple values to at least one of the aesthetic arguments\n\nshape + polygon method (Prof’s)\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n            legend.height = 0.35, \n            legend.width = 0.35) +\n  tmap_style('white')\n\n\n\n\n\n\n\n\nshape + fill + borders method (NYX’s)\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c('YOUNG', 'AGED','TOTAL'),\n          style=c('equal', 'quantile', 'equal'),\n          palette= list('Blues', 'Greens', 'Reds')) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c('right','bottom'),\n            legend.height = 0.35, \n            legend.width = 0.35)\n\n\n\n\n\n\n\n2.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\nthres.poly: number that specifies the threshold at which polygons are taken into account. The number itself corresponds to the proportion of the area sizes of the polygons to the total polygon size. By default, all polygons are drawn.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nCreating multiple stand-alone maps with tmap_arrange() asp : aspect ratio\nnrow : number of rows (ncols)\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\") +\n  tm_layout(legend.position = c('right','bottom'),\n          legend.height = 0.5, \n          legend.width = 0.35)\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\") +\n  tm_layout(legend.position = c('right','bottom'),\n        legend.height = 0.5, \n        legend.width = 0.35)\n\ntmap_arrange(youngmap, agedmap, asp=3, ncol=1, nrow=2)\n\n\n\n\n\n\n\n2.4.7 Mappping Spatial Object Meeting a Selection Criterion\nUse selection funtion to map spatial objects meeting the selection criterion. The comma , is used to indicate that we want to select all columns for the rows that meet the condition.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.3) +\n  tm_layout(main.title = \"Mapping spatial obj with conditions \\n(Central Region)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.outside = TRUE,\n            legend.height = 0.3, \n            legend.width = 0.3,\n            legend.title.size= 0.8,\n            legend.text.size= 0.6,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE,\n            bg.color = \"mintcream\") + #call colors()\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nWhat to do if legend overlaps with map?\n\n\nreduce legend.text.size or legend.height and legend.width\nadjust inner.margin\n\n\nAdjust background colours with its corresponding text colours.\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons(\"DEPENDENCY\") +\n    tm_layout(main.title = \"Mapping spatial obj with conditions \\n(Central Region)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.outside = TRUE,\n            legend.height = 0.3, \n            legend.width = 0.3,\n            legend.title.size= 0.8,\n            legend.text.size= 0.6,\n            inner.margins = c(0.01, 0.01, 0, .15), # ensures legend does not overlap with chart c(bottom,top,left,right)\n            frame = FALSE,\n            legend.position = c(\"right\", \"top\"),\n            bg.color = \"black\",\n            main.title.color = 'white',\n            legend.title.color = 'white',\n            legend.text.color= 'white')\n\n\n\n\n\n\n\n\n\n2.4.8. Tmap summary\nFrom chap 2, using tmap package to plot\n1. tm_shape+ tm_polygon\n2. tm_shape+ tm_fill + tm_borders _ tm_layout\n3. qtm\nChap 2: To plot small multiple chloroplath maps via qtm or tmap elements:\n1. Add multiple values to tm_polygon or tm_fill\n2. Tm_facets\n3. Tm_arrange\nChap 3: To plot small multiple layered chloroplath maps via plot() method:\n 4. To arrange the maps via plot(),\npar(mfrow=c(2,2)) <- 2x2 layout\nFrom chap 3: to add layers to a plot\n1. just keep repeating plot(weight matrix ,coords,add=TRUE)\nhttps://rstudio-pubs-static.s3.amazonaws.com/730482_d7889d9c65c8422f843b3d4e0196633c.html"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex1_2.html#reference",
    "href": "Hands-on_Ex1/Hands-on_Ex1_2.html#reference",
    "title": "Hands-on Exercise 1.2: Choropleth Mapping with R",
    "section": "2.5 Reference",
    "text": "2.5 Reference\n\n2.5.1 All about tmap package\n\ntmp arguments and defaults : Introduction to Geospatial Visualization with the tmap package\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n2.5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n2.5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions\n\n\n# there are 111 arguments, run the code below to see the defaults\ntmap_options()"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute spatial weights using R. By the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#the-study-area-and-data",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#the-study-area-and-data",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.2 The Study Area and Data",
    "text": "8.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file (aspatial) contains selected Hunan’s local development indicators in 2012.\n\n\n8.2.1 Getting Started\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#getting-the-data-into-r-environment",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.3 Getting the Data Into R Environment",
    "text": "8.3 Getting the Data Into R Environment\n8.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan <- st_read(dsn='data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex2\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\nclass(hunan)\n\n[1] \"sf\"         \"data.frame\"\n\n\nhunan is in WSG84 coordinate system.\n\nhead(hunan,3) %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nShape_Leng\nShape_Area\nCounty\ngeometry\n\n\n\n\nChangde\n21098\nAnxiang\nCounty\n1.869074\n0.1005619\nAnxiang\nPOLYGON ((112.0625 29.75523…\n\n\nChangde\n21100\nHanshou\nCounty\n2.360691\n0.1997875\nHanshou\nPOLYGON ((112.2288 29.11684…\n\n\nChangde\n21101\nJinshi\nCounty City\n1.425620\n0.0530241\nJinshi\nPOLYGON ((111.8927 29.6013,…\n\n\n\n\n\n\n8.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 <- read_csv('data/aspatial/Hunan_2012.csv')\nhead(hunan2012,3) %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nCity\navg_wage\ndeposite\nFAI\nGov_Rev\nGov_Exp\nGDP\nGDPPC\nGIO\nLoan\nNIPCR\nBed\nEmp\nEmpR\nEmpRT\nPri_Stu\nSec_Stu\nHousehold\nHousehold_R\nNOIP\nPop_R\nRSCG\nPop_T\nAgri\nService\nDisp_Inc\nRORP\nROREmp\n\n\n\n\nAnhua\nYiyang\n30544\n10967.0\n6831.7\n456.72\n2703.0\n13225.0\n14567\n9276.9\n3954.9\n3528.3\n2718\n494.31\n441.4\n338.0\n54.175\n32.830\n290.4\n234.5\n101\n670.3\n5760.6\n910.8\n4942.253\n5414.5\n12373\n0.7359464\n0.8929619\n\n\nAnren\nChenzhou\n28058\n4598.9\n6386.1\n220.57\n1454.7\n4941.2\n12761\n4189.2\n2555.3\n3271.8\n970\n290.82\n255.4\n99.4\n33.171\n17.505\n104.6\n121.9\n34\n243.2\n2386.4\n388.7\n2357.764\n3814.1\n16072\n0.6256753\n0.8782065\n\n\nAnxiang\nChangde\n31935\n5517.2\n3541.0\n243.64\n1779.5\n12482.0\n23667\n5108.9\n2806.9\n7693.7\n1931\n336.39\n270.5\n205.9\n19.584\n17.819\n148.1\n135.4\n53\n346.0\n3957.9\n528.3\n4524.410\n14100.0\n16610\n0.6549309\n0.8041262\n\n\n\n\n\n\n\n8.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\nAs the join columns are not specified, this function will assume that columns with the same names, e.g., ‘county’ in both dfs will be the join columns.\nThe select() will retain the columns indicated in the resulting joined df.\n\nhunan <- left_join(hunan, hunan2012) %>% \n  select(1:4,7,15)\nclass(hunan)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNote the geospatial characteristics of hunan is retained.\n\nhead(hunan,3) %>% kable\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nCounty\nGDPPC\ngeometry\n\n\n\n\nChangde\n21098\nAnxiang\nCounty\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\nChangde\n21100\nHanshou\nCounty\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\nChangde\n21101\nJinshi\nCounty City\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#visualising-regional-development-indicator",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.4 Visualising Regional Development Indicator",
    "text": "8.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap <- tm_shape(hunan)+\n  tm_polygons() +\n  tm_text('NAME_3',\n          size = 0.5)\n\ngdppc <- tm_shape(hunan)+\n  tm_polygons('GDPPC') +\n    tm_layout(legend.height = 0.20, \n            legend.width = 0.25)\n  \n# or gdppc <- qtm(hunan, \"GDPPC\")\n\ntmap_arrange(basemap, gdppc,\n           asp=1,\n           ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.5 Computing Contiguity Spatial Weights",
    "text": "8.5 Computing Contiguity Spatial Weights\nIn this section, we will use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. Use this if we know that for a variable, sharing common boundary increases spatial interaction.\nBy default, ‘queen’ argument is set to TRUE: two polygons sharing one shared boundary point will meet contiguity condition. Returns a list of first order neighbours using the Queen criteria.\nIf ‘queen’ argument is set to FALSE: requires more than one shared boundary point. (but may not mean a shared boundary line)\n\n8.5.1 Computing (QUEEN) contiguity based neighbours\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nwm_q (weights matrix queen) class: ‘nb’\n\nclass(wm_q)\n\n[1] \"nb\"\n\n\nA quick peak at wm_q\n\nhead(wm_q,3)\n\n[[1]]\n[1]  2  3  4 57 85\n\n[[2]]\n[1]  1 57 58 78 85\n\n[[3]]\n[1]  1  4  5 85\n\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nPolygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nx1 <- wm_q[[1]]\nhunan$County[c(x1)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nTo display the complete weight matrix, use str()\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n#wm_q[1:10]\n#methods(class = class(wm_q))\n\n\n\n8.5.2 Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n8.5.3 Visualising contiguity weights (find centroid coords first)\nA connectivity graph takes a point and displays a line to each neighbouring point.\n\nneed a point in polygon via polygon centroids (its lat & long) <- calculate using the sf package first\n\nTo obtain coordinates in a separate data frame\n\ninput vector is the geometry column of us.bound (in hunan), an sf object <- a polygon\nUsing hunan$geometry[[1]]: The input vector of the FIRST POLYGON looks like POLYGON ((112.0625 29.75523, 112.069 29.74544, 112.0707 29.7415, 112.0716 29.73667, …. , 112.0625 29.75523).\nTo find the centroid (CG) of one polygon, use the st_centroid() function, which is a formula shorthand for a small anonymous function (lambda function). It takes an argument represented by .x. st_centroid(.x)[[1]] extracts the X-coordinate (longitude) of the centroid while st_centroid(.x)[[2]] extracts the Y-coords.\n\nX and YX onlyY only\n\n\n\n#hunan$geometry[[1]]\nst_centroid(hunan$geometry[[1]])\n\n\n\n\nst_centroid(hunan$geometry[[1]])[[1]]\n\n[1] 112.1531\n\n\n\n\n\nst_centroid(hunan$geometry[[1]])[[2]]\n\n[1] 29.44362\n\n\n\n\n\n\nmap_dbl(...) is a function from the purrr package that applies a function to each element of a list or vector and returns the results as a double vector. In this case, the function being applied is the expression following ~. Example of how this function is used in a simple way:\n\n# Example list of vectors\nlist_of_vectors <- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n\n# Applying a function to calculate the sum of each vector\nmap_dbl(list_of_vectors, ~ sum(.x))\n\n a  b  c \n 6 15 24 \n\n\nAfter finding centroid, to access longitude values\n\ndouble bracket notation [[ ]] and 1\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlongitude\n\n [1] 112.1531 112.0372 111.8917 111.7031 111.6138 111.0341 113.7065 112.3460\n [9] 112.8169 113.3534 113.8942 112.4006 112.5542 113.6636 112.9206 113.1883\n[17] 113.4521 112.4209 113.0152 112.6350 112.7087 112.9095 111.9522 110.2606\n[25] 110.0921 109.7985 109.5765 109.7211 109.7339 109.1537 110.6442 110.5916\n[33] 109.5984 111.4783 112.1745 111.2315 110.3149 111.3248 110.5859 110.9593\n[41] 111.8296 110.1926 110.7334 110.9123 111.4599 112.5268 112.3406 109.5602\n[49] 109.5071 109.9954 109.4273 109.7587 109.5044 109.9899 109.9664 111.3785\n[57] 112.4350 112.5558 111.7379 112.1831 111.9743 111.7009 112.2196 112.6472\n[65] 113.5102 113.1172 113.7089 112.7963 110.9276 113.6420 113.4577 113.8404\n[73] 113.4758 113.1428 110.3017 113.1957 111.7410 112.1831 111.3390 111.8208\n[81] 110.0753 112.3965 112.7683 113.1679 111.4495 112.7956 111.5896 111.2393\n\n\n\nTo access the latitude value\n\ndouble bracket notation [[ ]] and 2\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nWith both longitude and latitude, use cbind() to combine both in the same object,\n\ncoords <- cbind(longitude, latitude)\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n8.5.3.1 Plotting Queen contiguity based neighbours map\nwm_ q is weight matrix using queen method (88 rows of list of neighbours)\ncoords is an array of x,y coordinates of centroids for each of the 88 counties\n\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen's contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\nUse the plot() function\n\nThe shape of the markers: The plot markers are by default small, empty circles. These are also known as plot characters - denoted by pch. Pch values 0 to 25 are valid and give several different symbols on the graph. Pch 0 is for a square, 1 is for a circle, 3 is for a triangle, 4 is for a cross and so on.\nSize of the plot markers: The cex parameter can be set to 0.5 if we want the markers to be 50% smaller and 1.5 if wewant them to be 50% larger.\nColor of the plot markers: These colors can be selected from a list provided by R under the colors() function.\n\n\n\n\n8.5.3.2 Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border = 'lightgrey', main='Rooks contiguity')\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col =\"blue\")\n\n\n\n\n\n\n8.5.3.3 Plotting both Queen and Rook contiguity based neighbours maps side-by-side\npar(mfrow = c(1, 2)) arranges subsequent plots in a grid with one row and two columns\n\npar(mfrow=c(1,2))\n\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen's contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\", main=\"Queen Contiguity\")\nplot(hunan$geometry, border=\"lightgrey\",main=\"Rook's contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"blue\", main=\"Rook Contiguity\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#computing-distance-based-neighbours",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.6 Computing distance based neighbours",
    "text": "8.6 Computing distance based neighbours\n\nIn this section, we will derive distance-based weight matrices by using dnearneigh() of spdep package.\ndnearneigh(x, d1, d2, row.names = NULL, longlat = NULL, bounds=c(“GE”, “LE”),\n use_kd_tree=TRUE, symtest=FALSE, use_s2=packageVersion(“s2”) > “1.0.7”, k=200,\n dwithin=TRUE)\nidentifies neighbours using distance band with lower d1= and upper d2= bounds controlled by the bounds= argument\nIf unprojected coordinates are used (WSG84 geographic) and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\n8.6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep. Class: ‘knn’. Output: NN of poly1 = poly3, NN of poly2 = poly 78 etc…\n\nstr(knearneigh(coords,\n           k=1))\n\nList of 5\n $ nn       : int [1:88, 1] 3 78 1 5 4 69 67 46 84 70 ...\n $ np       : int 88\n $ k        : num 1\n $ dimension: int 2\n $ x        : num [1:88, 1:2] 112 112 112 112 112 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"longitude\" \"latitude\"\n - attr(*, \"class\")= chr \"knn\"\n - attr(*, \"call\")= language knearneigh(x = coords, k = 1)\n\n#knearneigh(coords,k=1)['nn']  #<< shows matrix\n\nConvert the matrix knn object returned by knearneigh() into a neighbours list with a list of integer vectors containing neighbour region number ids by using knn2nb(). Class: nb\n\nknn2nb(knearneigh(coords))[1:5]\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 78\n\n[[3]]\n[1] 1\n\n[[4]]\n[1] 5\n\n[[5]]\n[1] 4\n\n\nReturn a list of the length (the distance to one’s nearest neighbour) of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km if in WSG84. Class: ‘nbdist’\n\nnbdists(knn2nb(knearneigh(coords, k=1)),\n        coords,\n        longlat = TRUE) [1:5]\n\n[[1]]\n[1] 25.53398\n\n[[2]]\n[1] 43.03114\n\n[[3]]\n[1] 25.53398\n\n[[4]]\n[1] 29.2848\n\n[[5]]\n[1] 29.2848\n\n\nRemove the list structure of the returned object by using unlist(). Class of k1dists: numeric.\nOutput: 25.53398 43.03114 25.53398 29.28480 29.28480 45.98097 58.52704 28.95985 34.45062 37.99885 44.49442 33.48816 35.98123\n\nk1 <- knn2nb(knearneigh(coords, k=1))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79. So using this as the upper threshold (62km) will help to ensure that all units (polygons) will have at least one neighbour.\n\n\n8.6.2 Computing fixed distance weight matrix\n(Earlier, we had used poly2nb(hunan, queen=TRUE/FALSE) to define neighbours using Queen or Rook method, resulting in wm_q and wm_r.)\n(We also have k1 of ‘nb’ class where each polygon has 1 nb. Additionally, we used summary(unlist(nbdists())) to calculate the distance between furthest neighbours.)\nNow, to define neighbours using a distance threshold, we use the dnearneigh() ,\n\nlonglat argument: TRUE if point coordinates are geographical longitude-latitude decimal degrees (WSG84)\nsome polygons have more than 1 nb, but all polygons have at least 1 nb here due to distance threshold.\n\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat=TRUE )\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nAverage number of links is calculated by dividing the total number of links by the number of regions. On average, each region has approximately 3.68 neighboring regions. Class of wm_d62: ‘nb’\n\nclass(wm_d62)\n\n[1] \"nb\"\n\n\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\nThe column headers “1” means the count of counties that have 1 neighbor within the specified distance of 62km, “2” means the count of counties that have 2 neighbors, and so on.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nTO find the number of connected components (aka see if there are any spatial units / regions without a neighbour)\n\nn_comp <- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\nIn this connected component, there are 88 elements (spatial units)\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n8.6.2.1 Plotting fixed distance weight matrix\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nwm_d62 is the fixed distance weight matrix,\ncoords refers to long, lat coordinates for CG of each polygon\nk1 is the list of integer ID of the polygon which is the nearest neighbour to me. The topmost layer of the plot colours the nearest neighbour edge to red colour.\n\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08) \n\n\n\n\nThe red lines shows the links of 1st nearest neighbours and the black lines show the lines of neighbours within the cut-off distance of 62km.\nTo plot red and black side by side,\n\npar(mfrow = c(1, 2))\nplot(hunan$geometry, border=\"lightgrey\", main = 'Neighbours within 62 km')\nplot(wm_d62, coords, add=TRUE)\nplot(hunan$geometry, border=\"lightgrey\", main = 'Nearest Neighbour')\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08) \n\n\n\n\n\n\n\n8.6.3 Computing adaptive distance weight matrix\nUse this method if the dataset is highly skewed to fix the # of nbs.\nEarlier, we used k1 <- knn2nb(knearneigh(coords, k=1)) and wm_d62 <- dnearneigh(coords, 0, 62, longlat=TRUE ) to plot maps of nearest nb and nbs within 62km range.\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below. class of knn6: ‘nb’\n\nknn6 <- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str(). Note that each spatial units has exactly 6 neighbours.\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n8.6.3.1 Plotting distance based neighbours\n\nplot(st_geometry(hunan), border = 'lightgray')\nplot(knn6, coords, pch=18, cex=0.6, add= TRUE, col='red')"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#weights-based-on-inverse-distance-method",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#weights-based-on-inverse-distance-method",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.7 Weights based on Inverse Distance Method",
    "text": "8.7 Weights based on Inverse Distance Method\nIn this section, we will learn how to derive a spatial weight matrix based on Inversed Distance method. This will assign greater weightage to closer polygons, use this if we know that closer proximity will result in more spatial interaction.\nEarlier, we applied nbdists() nbdists(knn2nb(knearneigh(coords, k=1)), coords, longlat = TRUE) to compute the distance of one’s nearest neighbour.\nFirst, we will compute the distances between areas (defined using Queen contiguity method) by using nbdists() of spdep. Class of dist: nbdist\n\ndist <- nbdists(wm_q, coords, longlat=TRUE)\nhead(dist,3)\n\n[[1]]\n[1] 65.12941 25.53398 54.91802 35.61352 87.32760\n\n[[2]]\n[1] 65.12941 56.67944 51.92312 43.03114 58.16151\n\n[[3]]\n[1] 25.53398 35.43536 27.05778 71.64530\n\n\nNow compute the inverse distances for all polygons to their neighbours. Class of ids: matrix of list. ids is also glist (general weights for each nb).\n\nids <- lapply(dist, function(x) 1/(x))\nhead(ids,3)\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n\n\n8.7.1 Row-standardised weights matrix\nnb2listw(neighbours, glist=NULL, style=\"W\", zero.policy=NULL)\nArguments\n\n\n\n\n\n\n\nneighbours\nan object of class nb\n\n\n\n\nglist\nlist of general weights corresponding to neighbours\n\n\nstyle\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”\n\n\nzero.policy\ndefault NULL, use global option value; if FALSE stop with error for any empty neighbour sets, if TRUE permit the weights list to be formed with zero-length weights vectors\n\n\n\nDetails\nStarting from a binary neighbours list, in which regions are either listed as neighbours or are absent (thus not in the set of neighbours for some definition), the function adds a weights list with values given by the coding scheme style chosen.\nWeights to each neighboring polygon\n\neach neighboring polygon will be assigned equal weight (style=“W”) <- row standardised. This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values\ndrawback of this method is that polygon along the edge will base their lagged values on fewer polygons, thus over estimating the true nature of spatial autocorrelation in the data. Next time can consider other more robust options are available, notably style=“B” <- basic binary coding\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error. Class of rsmq_q = ‘listw’ and ‘nb’\n\n\nstyle = ‘W’style = ‘B’\n\n\n\n# wm_q is a matrix containing nb indexes\nrswm_q <- nb2listw(wm_q, style = 'W', zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\nnb2listw(wm_q, style = 'B', zero.policy = TRUE)$weights[1]\n\n[[1]]\n[1] 1 1 1 1 1\n\n\n\n\n\n\nclass(rswm_q)\n\n[1] \"listw\" \"nb\"   \n\n\nTo see the weight of the first polygon’s 5 neighbours type:\n\nrswm_q$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nThe 5 neighbours are\n\nrswm_q$neighbours[1]\n\n[[1]]\n[1]  2  3  4 57 85\n\n\nExplanation of above: Each neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same queen’s method, we can also derive a (row standardised ?) inverse-distance weight matrix by using the code chunk below. Recall ids contains inverse-distances to neighbours, also our glist (general weights of neighbours) , originally from queens wm_q which is a neighbour structure. We will now use nb2listw() and wm_q and ids to create a spatial weights matrix. Class of rswm_ids is ‘listw’ and ‘nb’. It contains 3 lists, ‘style’ of class character ,‘neighbours’ of class nb, ‘weights’ of class nb\n\nrswm_ids <- nb2listw(wm_q, glist=ids, style ='W', zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 40.39879 361.0957\n\n\nTo see the weights of the 1st polygon; it is actually the similar as ids, but not identical.\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.1367760 0.3488740 0.1622080 0.2501337 0.1020083\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.05894 0.13342 0.17603 0.19643 0.23961 1.00000 \n\n\nFinally, a comparison of the weights for polygon 1 across three spatial weights methods:\n\na <- unlist(rswm_q$weights[1])\nb <- unlist(nb2listw(wm_q, style = 'B', zero.policy = TRUE)$weights[1])\nc <- unlist(rswm_ids$weights[1])\n\nweights_comparison <- as.data.frame(cbind(a,b,c))\ncolnames(weights_comparison) <- c(\"row standardised\", \"binary\", 'row standardised inverse distance')\nweights_comparison\n\n  row standardised binary row standardised inverse distance\n1              0.2      1                         0.1367760\n2              0.2      1                         0.3488740\n3              0.2      1                         0.1622080\n4              0.2      1                         0.2501337\n5              0.2      1                         0.1020083"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#application-of-spatial-weight-matrix",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "8.8 Application of Spatial Weight Matrix",
    "text": "8.8 Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n8.8.1 Spatial lag with row-standardized weights\n\ncompute the average neighbour GDPCC values for each polygon\ncommonly called spatially lagged values\ndoes not include itself\nRecalled in the previous section, we retrieved the GDPPC of these five countries (neighbours of poly1 using queen method) by using the code chunk below.\n\n\nx1 <- wm_q[[1]]\nhunan$GDPPC[c(x1)]\n\n[1] 20981 34592 24473 21311 22879\n\n\n\nNow we compute the row-standardised lag variable using lag.listw() and rsqm_q (neighbour structure).\nIn the code below, use spatial weight matrix (equal weightage of 0.2 for poly1 etc.. ) by 0.2 * 20981 + 0.2 * 34592 + 0.2 * 24473 + 0.2 * 21311 + 0.2 * 22879 = 24847.20 for poly1 “Anxiang”\n22724.80 is the average GDPPC for poly2 based on equal weightage (0.2) of all its five neighbours . 24143.25 for poly3 based on equal weightage (0.25) based on four neighbours.\n\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\nFirst, create lag.list that contains two individual lists, namely NAME_3 column in hunan and the lag variable. Next, transform both lists into a dataframe. Rename the column headers. Perform a left join with hunan and lag.res will automatically use NAME_3 column as the join column.\n\nlag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nstr(lag.list)\n\nList of 2\n $ : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n $ : num [1:88] 24847 22725 24143 27738 27270 ...\n\nlag.res <- as.data.frame(lag.list)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.res)\n\nhead(as_tibble(hunan),3)\n\n# A tibble: 3 × 8\n  NAME_2   ID_3 NAME_3  ENGTYPE_3   County  GDPPC `lag GDPPC`\n  <chr>   <int> <chr>   <chr>       <chr>   <dbl>       <dbl>\n1 Changde 21098 Anxiang County      Anxiang 23667      24847.\n2 Changde 21100 Hanshou County      Hanshou 20981      22725.\n3 Changde 21101 Jinshi  County City Jinshi  34592      24143.\n# ℹ 1 more variable: geometry <POLYGON [°]>\n\n# head(hunan,3) %>%  kable()\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, 'GDPPC') +\n    tm_layout(main.title='No lag variable',\n              legend.height = 0.2,\n              legend.width=0.2)\nlag_gdppc <- qtm(hunan, 'lag GDPPC') +\n  tm_layout(main.title='With lag variable (Row-stand)',\n            legend.height = 0.2,\n            legend.width=0.4)\n\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe chart on the left plots the original GDPPC values of respective spatial units. The chart on the right plots the average GDPPC values of all spatial units that shares a boundary (Queen) for a particular spatial unit. Does not include diagonal (own’s) GDPPC value.\n\n\nCompute lag variable using rswm_ids (inverse-distance method)\n\nlag.list.2 <- list(hunan$NAME_3, lag.listw(rswm_ids, hunan$GDPPC))\nstr(lag.list.2)\n\nList of 2\n $ : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n $ : num [1:88] 26572 22568 24356 28108 28265 ...\n\nlag.res.2 <- as.data.frame(lag.list.2)\ncolnames(lag.res.2) <- c(\"NAME_3\", \"lag GDPPC ID\")\nhunan <- left_join(hunan,lag.res.2)\n\nhead(as_tibble(hunan),3)\n\n# A tibble: 3 × 9\n  NAME_2   ID_3 NAME_3  ENGTYPE_3   County  GDPPC `lag GDPPC` `lag GDPPC ID`\n  <chr>   <int> <chr>   <chr>       <chr>   <dbl>       <dbl>          <dbl>\n1 Changde 21098 Anxiang County      Anxiang 23667      24847.         26572.\n2 Changde 21100 Hanshou County      Hanshou 20981      22725.         22568.\n3 Changde 21101 Jinshi  County City Jinshi  34592      24143.         24356.\n# ℹ 1 more variable: geometry <POLYGON [°]>\n\nlag_gdppc_id <- qtm(hunan, 'lag GDPPC ID') +\n  tm_layout(main.title='With lag variable (Inv_dist)',\n            legend.height = 0.2,\n            legend.width=0.4)\n\ntmap_arrange(lag_gdppc, lag_gdppc_id, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLag variable using Queen’s contiguity (Row standardised weight matrix)\nThe lag variable represents the spatial AVERAGE of GDPPC for neighbouring areas based on Queen contiguity.\nLag variable using Inverse-Distance Weight matrix\nThe lag variable is calculated as a WEIGHTED AVERAGE of GDPPC. Further neighbours are penalised (assign lesser weights) when we use inverse-distance.\n\n\n\n\n8.8.2 Spatial lag as a sum of neighboring values\nPart 1: Obtain binary weights matrix called 'b_weights' (glist):\nwm_q (aka neighbour list) is a 'nb' class containing neighbour IDs for each polygon. E.g. Neighbours list of the first three polygons:\n\n\n[[1]]\n[1]  2  3  4 57 85\n\n[[2]]\n[1]  1 57 58 78 85\n\n[[3]]\n[1]  1  4  5 85\n\n\nTo create a binary list, we will now apply lambda function of 0*neighbour ID + 1 ; so that if there is a neighbour, the value is 1. Class of b_weights is a matrix with lists of 1 .\nSimilar to ids earlier, b_weights is the glist (general weights corresponding to neighbours). We need the neighbour structure wm_q and glist to convert to spatial weights matrix using nb2listw().\n\nb_weights <- lapply(wm_q, function(x) 0*x+1)\nclass(b_weights)\n\n[1] \"list\"\n\nb_weights[1:3]\n\n[[1]]\n[1] 1 1 1 1 1\n\n[[2]]\n[1] 1 1 1 1 1\n\n[[3]]\n[1] 1 1 1 1\n\n\nPart 2: Create the spatial weights matrix using nb2listw() , wm_q (neighbour structure) and b_weights (glist)\n\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nSneakpeak at b_weights2\n\nb_weights2$weights[1:3]\n\n[[1]]\n[1] 1 1 1 1 1\n\n[[2]]\n[1] 1 1 1 1 1\n\n[[3]]\n[1] 1 1 1 1\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC. Since the weights are all ‘1’s, we will be summing all our neighbours’ GDPPC.\nlag_sum contains two lists, namely NAME_3 and lag_sum GDPPC. Combine both lists into a df. Rename the df column headers.\n\nlag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nstr(lag_sum)\n\nList of 2\n $ : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n $ : num [1:88] 124236 113624 96573 110950 109081 ...\n\nlag.res <- as.data.frame(lag_sum)\ncolnames(lag.res) <- c('NAME_3', 'lag_sum GDPPC')\n\nNext, append lag.res to hunan sf dataframe\n\nhunan <- left_join(hunan, lag.res)\n\nPlot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, 'GDPPC') +\n    tm_layout(main.title='No lag variable',\n              legend.height = 0.2,\n              legend.width=0.2)\n\nlag_sum_gdppc <- qtm(hunan, 'lag_sum GDPPC') +\n  tm_layout(main.title = 'spatial lag as lag sum of nb values',\n            legend.height = 0.2,\n            legend.width=0.2)\n\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lag variable created by summing up neighbours’ GDPPC look more alike the lag variable created using inverse-distance weights matrix.\n\n\n\n\n8.8.3 Spatial window average\n\nmust use row-standardized weights\nsimilar to 8.8.1 but only difference is that it includes the diagonal element;\nTo include diagonal in R, we need to go back to the neighbors structure wm_q and add the diagonal element before assigning weights.\n\n\n# weight matrix queen self\nwm_qs <- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nwm_qs[1:3]\n\n[[1]]\n[1]  1  2  3  4 57 85\n\n[[2]]\n[1]  1  2 57 58 78 85\n\n[[3]]\n[1]  1  3  4  5 85\n\n\nWe can see that polygon 1 has its diagonal (1) inside and polygon 2 has its diagonal (2) inside.We can see that polygon 1 has its diagonal (1) inside and polygon 2 has its diagonal (2) inside.\nAs expected, the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909.\nNow, proceed to create the spatial weights matrix that includes self:\nDefault style is ‘W’ - row standardised.\n\nwm_qs <- nb2listw(wm_qs, style ='W', zero.policy = TRUE)\nwm_qs$weights[1:3]\n\n[[1]]\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n[[2]]\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n[[3]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nNow, create lag variable using spatial weights matrix wm_qs and hunan$GDPCC variable:\n\nlag_w_ave_gdppc <- lag.listw(wm_qs,\n                             hunan$GDPPC)\nlag_w_ave_gdppc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nAppend lag_w_ave_gdppc to the hunan dataframe by using the series of steps below:\n\nlag.list.wm_qs <- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nstr(lag.list.wm_qs)\n\nList of 2\n $ : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n $ : num [1:88] 24651 22434 26233 27085 26927 ...\n\nlag_wm_qs.res <- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) <- c('NAME_3', 'lag_window_avg GDPPC')\n\nhunan <- left_join(hunan, lag_wm_qs.res)\n\nhunan %>% \n  select('County', 'lag GDPPC', 'lag_window_avg GDPPC') %>% \n  head() %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\n\n\n#as_tibble(hunan)  #<< will isolate the geometry part\n\nUse dtm() to plot ‘lag GDPPC’ and ‘lag_window_avg GDPPC’ side-by-side\n\nW_avg_gdppc <- qtm(hunan, 'lag_window_avg GDPPC') +\n  tm_layout(main.title='With lag_sum variable',\n            legend.height = 0.2,\n            legend.width=0.4)\n\ntmap_arrange(lag_gdppc, W_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth charts use row-standardised spatial weights matrix.\nDifference between charts is that left does not include self’s GDPPC but the right includes self’s GDPPC.\n\n\n\n\n8.8.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights. (similar to 8.8.2 but including self)\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs <- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary general weights to the neighbour structure that includes the diagonal element.\n\nb_weights <- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1:3]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n[[2]]\n[1] 1 1 1 1 1 1\n\n[[3]]\n[1] 1 1 1 1 1\n\n\nEach spatial unit has one more element, that is itself.\nAgain, we use nb2listw() and wm_qs (nb structure) and b_weights (glist) to create spatial weights matrix b_weights\n\nb_weights2 <- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nCompute the window sum lag variable using lag.listw(), b_weights2 (spatial weights matrix) and hunan$GDPCC (variable). The below also appends the window sum lag variable to hunan df.\n\nw_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nstr(w_sum_gdppc)\n\nList of 2\n $ : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n $ : num [1:88] 147903 134605 131165 135423 134635 ...\n\nw_sum_gdppc.res <- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) <- c(\"NAME_3\", \"w_sum GDPPC\")\n\nhunan <- left_join(hunan, w_sum_gdppc.res)\nhunan %>%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %>%\n  head() %>% \n  kable()\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nW_sum_gdppc <- qtm(hunan, 'w_sum GDPPC') +\n  tm_layout(main.title='With lag_win_sum variable',\n            legend.height = 0.2,\n            legend.width=0.4)\n\ntmap_arrange(lag_sum_gdppc, W_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth charts do not use row-standardised spatial matrix, instead they use binary ‘1’ because of the need to sum up the GDPPC values. Difference is that left does not include self but right includes self.\n\n\n\n#| eval: false\n#| echo: false\n#| fig-width: 14\n#| fig-asp: 0.68\n#| code-fold: True"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_1.html#summaries",
    "href": "Hands-on_Ex2/Hands-on_Ex2_1.html#summaries",
    "title": "Hands-on Exercise 2.1: Spatial Weights and Applications",
    "section": "Summaries",
    "text": "Summaries\nSummary of deriving spatial weights matrix\n\nSummary of deriving lag variables"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we will learn to\n\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#getting-started",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#getting-started",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "9.2 Getting Started",
    "text": "9.2 Getting Started\n\n9.2.1 The analytical question\n\nIn spatial policy, local government/planners aims to ensure equal distribution of development in the province.\nwe should apply appropriate spatial statistical methods to discover if development are even distributed geographically in the province\nif answer is NO, we ask “is there sign of clustering?” (GLOBAL spatial autocorrelation)\nif YES, “Where are the clusters” (LOCAL spatial autocorrelation)\n\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China. (https://en.wikipedia.org/wiki/Hunan)\n\n\n\n\n\n9.2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise:\n\nGeospatial data: Hunan province administrative boundary layer at county level in ESRI shapefile format\nAspatial data: Hunan_2012.csv containing local development indicators\n\n\n\n9.2.3 Setting the Analytical Tools\nPackages we need:\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#getting-the-data-into-r-environment",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "9.3 Getting the Data Into R Environment",
    "text": "9.3 Getting the Data Into R Environment\nThe geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n9.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\nhunan is in WSG84 geographical system.\n\nhunan <- st_read(dsn='data/geospatial',\n                 layer='Hunan')\n\nReading layer `Hunan' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex2\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n#st_crs(hunan)\nhead(hunan,3)\n\nSimple feature collection with 3 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 111.7027 ymin: 28.61762 xmax: 112.3013 ymax: 29.77344\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3 Shape_Leng Shape_Area  County\n1 Changde 21098 Anxiang      County   1.869074 0.10056190 Anxiang\n2 Changde 21100 Hanshou      County   2.360691 0.19978745 Hanshou\n3 Changde 21101  Jinshi County City   1.425620 0.05302413  Jinshi\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n\n\n\n\n9.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 <- read_csv('data/aspatial/Hunan_2012.csv')\nhead(hunan2012,3)\n\n# A tibble: 3 × 29\n  County  City  avg_wage deposite   FAI Gov_Rev Gov_Exp    GDP GDPPC   GIO  Loan\n  <chr>   <chr>    <dbl>    <dbl> <dbl>   <dbl>   <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 Anhua   Yiya…    30544   10967  6832.    457.   2703  13225  14567 9277. 3955.\n2 Anren   Chen…    28058    4599. 6386.    221.   1455.  4941. 12761 4189. 2555.\n3 Anxiang Chan…    31935    5517. 3541     244.   1780. 12482  23667 5109. 2807.\n# ℹ 18 more variables: NIPCR <dbl>, Bed <dbl>, Emp <dbl>, EmpR <dbl>,\n#   EmpRT <dbl>, Pri_Stu <dbl>, Sec_Stu <dbl>, Household <dbl>,\n#   Household_R <dbl>, NOIP <dbl>, Pop_R <dbl>, RSCG <dbl>, Pop_T <dbl>,\n#   Agri <dbl>, Service <dbl>, Disp_Inc <dbl>, RORP <dbl>, ROREmp <dbl>\n\n\n\n\n9.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame (geospatial) with the attribute fields of hunan2012 dataframe (aspatial) . This is performed by using left_join() of dplyr package. Since the join columns are not specified, identical columns names (‘County’) form both dataset will be used for the join.\nColumn 7 and 15 are the ‘County’ and ‘GDPPC’ columns respectively.\n\nhunan <- left_join(hunan, hunan2012) %>% \n  select(1:4, 7,15)\nhead(hunan,3)\n\nSimple feature collection with 3 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 111.7027 ymin: 28.61762 xmax: 112.3013 ymax: 29.77344\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n\n\n\n\n9.3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\ntm_fill() ’s n refer to the number of equal intervals\n\n\nequal <- tm_shape(hunan)+\n  tm_fill('GDPPC',\n          n=5,\n          style='equal') +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = 'Equal interval classification',\n            main.title.size=1.5,\n            legend.height = 0.25,\n            legend.width = 0.25)\n\nquantile <- tm_shape(hunan)+\n  tm_fill('GDPPC',\n          n=5,\n          style='quantile') +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = 'Equal quantile classification',\n            main.title.size=1.5,\n            legend.height = 0.25,\n            legend.width = 0.25)\n\ntmap_arrange(equal, quantile, asp =1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#global-spatial-autocorrelation",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#global-spatial-autocorrelation",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "9.4 Global Spatial Autocorrelation",
    "text": "9.4 Global Spatial Autocorrelation\nIn this section, we will\n\ncompute global spatial autocorrelation statistics\nperform spatial complete randomness test for global spatial autocorrelation (test for significance).\n\n\n9.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function will\n\nbuild a neighbours list based on regions with contiguous boundaries.\nif ‘queen’ argument is TRUE: spatial units are considered neighbours if they share a common point. A list of first order neighbours using the Queen criteria will be returned.\nif ‘queen’ argument is FALSE: spatial unit are considered neighbours if they share a least two common points.\n\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan,\n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nSneakpeak at the neighbours’ list of the first three polygons.\n\nwm_q[1:3]\n\n[[1]]\n[1]  2  3  4 57 85\n\n[[2]]\n[1]  1 57 58 78 85\n\n[[3]]\n[1]  1  4  5 85\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n9.4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q <- nb2listw(wm_q,\n                   style='W',\n                   zero.policy=TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nSneak peak at the neighbour weights of the first three polygons\n\nrswm_q$weights[1:3]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n[[2]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n[[3]]\n[1] 0.25 0.25 0.25 0.25\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\nattributes(rswm_q)\n\n$names\n[1] \"style\"      \"neighbours\" \"weights\"   \n\n$class\n[1] \"listw\" \"nb\"   \n\n$region.id\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"27\" \"28\" \"29\" \"30\"\n[31] \"31\" \"32\" \"33\" \"34\" \"35\" \"36\" \"37\" \"38\" \"39\" \"40\" \"41\" \"42\" \"43\" \"44\" \"45\"\n[46] \"46\" \"47\" \"48\" \"49\" \"50\" \"51\" \"52\" \"53\" \"54\" \"55\" \"56\" \"57\" \"58\" \"59\" \"60\"\n[61] \"61\" \"62\" \"63\" \"64\" \"65\" \"66\" \"67\" \"68\" \"69\" \"70\" \"71\" \"72\" \"73\" \"74\" \"75\"\n[76] \"76\" \"77\" \"78\" \"79\" \"80\" \"81\" \"82\" \"83\" \"84\" \"85\" \"86\" \"87\" \"88\"\n\n$call\nnb2listw(neighbours = wm_q, style = \"W\", zero.policy = TRUE)\n\nmethods(class=class(rswm_q))\n\n[1] coerce      initialize  lag         plot        print       show       \n[7] slotsFromS3 subset      summary    \nsee '?methods' for accessing help and source code\n\n\n\n\n9.4.3 Global Spatial Autocorrelation: Moran’s I\nIn this section, we will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\n9.4.4 Moran’s I test\nGlobal spatial association assesses the overall spatial pattern of a variable across the entire study area. It provides a single value or metric that summarizes the extent to which similar values cluster together or are dispersed across the entire geographic space.\nThe Moran’s I assumes data follows a normal distribution and are randomised.\nMoran’s I values range from -1 to 1 while Geary’s C ranges from 0 to 2. The chart below summarises our lecture material.\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep. It takes in the main arguments:\n\nvariable\nlistw , our spatial weights matrix that defines the neighbourhood and relationship between them.\n\n\nmoran.test(hunan$GDPPC,\n           listw = rswm_q,\n           zero.policy=TRUE,\n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nThe null hypothesis: Observed spatial patterns of values is equally likely as any random spatial pattern.\nSince the p-value is less than 0.05 and Moran I statistic is greater than 1, we can reject the null hypothesis and conclude that similar values tend to cluster together in our area of study.\n\n\n9.4.4.1 Computing Monte Carlo Moran’s I\nIn the event we are unsure whether the data follows a normal distribution and are randomised, we can use the Monte Carlo Simulation to simulate Moran’s I n times under the assumption of no spatial pattern (shuffle/permutate the variable across all spatial units). This creates a baseline to compare with the observed Moran’s I value from dataset.\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC,\n                 listw=rswm_q,\n                 nsim=999,\n                 zero.policy=TRUE,\n                 na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nSince the p-value is less than 0.05 and Moran I statistic is greater than 1, we can reject the null hypothesis and conclude that similar values tend to cluster together in our area of study.\n\n\n9.4.4.2 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\nGet the mean of simulated moran’s I values. The ‘res’ column contains the simulated moran’s i values.\n\nmean(bperm$res[1:999]) \n\n[1] -0.01504572\n\n\nGet the variance\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\nSummary statistics\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\nPlotting\n\nggplot2Base Graph\n\n\nWe will use ggplot2 to create the histogram instead of base r.\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nbperm_df <- as.data.frame(bperm$res)\ncolnames(bperm_df) <- c('res')\n\n#q <- quantile(bperm_df$res[1:999], probs = c(0.25, 0.5, 0.75))\nmean <- mean(bperm_df$res[1:999])\nmean\n\n\n[1] -0.01504572\n\n\nShow the code\nggplot(data=bperm_df,\n       aes(x=res)) +\n  geom_histogram(bins=20,\n                 color='grey25',\n                 fill='grey90',size=0.8) +\n  #geom_vline(xintercept = q[2], linetype='dotted', size = 0.8, color='blue') +\n  #geom_vline(xintercept = q[3], linetype='dotted', size = 0.8) +\n  geom_vline(xintercept = mean, linetype='dotted', size = 0.8, color='red') +\n  #annotate('text' , x= -0.055, y=180, label='50th \\npercentile', size = 5, color='blue') +\n  #annotate('text' , x= 0.06, y=180, label='75th \\npercentile', size = 5) +\n  annotate('text' , x= 0.015, y=180, label='mean', size = 5, color='red') +\n  labs(y= 'Frequency', x=\"Moran's I values\") +\n  theme_economist() +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  ggtitle(\"Histogram of Simulated Moran's I\")\n\n\n\n\n\n\n\n\nhist(bperm$res,\n     freq=TRUE,\n     breaks=20,\n     xlab=\"Simulated Moran's I\",\n     main = paste(\"Histogram of Simulated Moran I\"))\nabline(v=0,\n       col='red')\n\n\n\n\n\n\n\n\n\n\n9.4.5 Global Spatial Autocorrelation: Geary’s\nIn this section, we will learn how to perform Geary’s c statistics testing by using appropriate functions of spdep package.\n\n9.4.5.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\nThe null hypothesis: Observed spatial patterns of values is equally likely as any random spatial pattern.\nSince the p-value is less than 0.05 and Geary’s C statistic is greater 0 and less than 1, we can reject the null hypothesis and conclude that similar values tend to cluster together in our area of study.\n\n\n\n9.4.5.2 Computing Monte Carlo Geary’s C\nIn the event we are unsure whether the data follows a normal distribution and are randomised, the code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nSince the p-value is less than 0.05 and Geary’s C statistic is greater 0 and less than 1, we can reject the null hypothesis and conclude that similar values tend to cluster together in our area of study.\n\n\n9.4.5.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\nGet the mean of simulated Geary’s C\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\nGet the variance\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\nSummary statistics\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nPlot the histogram using ggplot2\n\n\nShow the code\nbperm_df <- as.data.frame(bperm$res)\ncolnames(bperm_df) <- c('res')\n\n#q <- quantile(bperm_df$res[1:999], probs = c(0.25, 0.5, 0.75))\nmean <- mean(bperm_df$res[1:999])\nmean\n\n\n[1] 1.004402\n\n\nShow the code\nggplot(data=bperm_df,\n       aes(x=res)) +\n  geom_histogram(bins=20,\n                 color='grey25',\n                 fill='grey90',size=0.8) +\n  geom_vline(xintercept = mean, linetype='dotted', size = 0.8, color='red') +\n  annotate('text' , x= 1.05, y=180, label='mean', size = 5, color='red') +\n  labs(y= 'Frequency', x=\"Geary's C values\") +\n  theme_economist() +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  ggtitle(\"Histogram of Simulated Geary's C\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#spatial-correlogram",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#spatial-correlogram",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "9.5 Spatial Correlogram",
    "text": "9.5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n9.5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nArguments:\n\n‘order’ refers to number of layers away from each polygon using contiguity method. We want to check how the Moran’s I values changes (and its statistical significance changes) as the neighbours get further and further away.\n‘method’: ‘corr’ for correlation, ‘I’ for Moran’s I and ‘C’ for Gerary’s C.\n\nUnderstanding the plot\nY-Axis: The y-axis typically represents the Moran’s I coefficient, which quantifies spatial autocorrelation. Above zero, similar values cluster. Below zero, dissimilar values cluster.\nX-Axis: The x-axis represents spatial distance lags. Each point on the correlogram corresponds to a specific distance lag (e.g., distance between observations). The points are usually organized in bins or distance classes.\nBars or Lines: Bars or lines connect the Moran’s I values at different distance lags, forming a pattern that shows how spatial autocorrelation changes with distance.\nUsefulness of Moran’s I Correlogram\nDetecting Spatial Patterns: A Moran’s I correlogram provides insights into the presence and structure of spatial patterns in our data. It helps identify at what distances spatial autocorrelation is significant.\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see that with the exception of Lag 4, the rest of the results are statistically significant at the 95% level of confidence.\nAs we consider order lag from 1 to 3, there is significant positive autocorrelation (similar values cluster) and we note the Moran’s I values decreases as order increases. For order 5 and 6, there is significant negative autocorrelation (dissimilar values cluster)\nPossible reason for the change of polarity:\nLocal clusters could dominate at smaller distances while at larger distances, can detect dispersion more.\n9.5.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n0< Geary C < 1 : similar values cluster\n1 < Geary C < 2: dissimilar values cluster\nNow examine the full report to check which values are significant.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of Geary’s C statistics test is similar to Moran’s I.\nAll lag orders except Lag order of 3, 4 and 6 are statistically significant at 95% confidence level.\nLag order 1 and 2: clustering of similar values, order 2 is less clustered than order 1. (Moran’s I and Geary’s C are inversely related.)\nLag order 5: clustering of dissimilar values."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.6 Cluster and Outlier Analysis",
    "text": "10.6 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are localised areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, we will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster (HH or LL) and/or outlier (HL, LH) from GDP per capita 2012 of Hunan Province, PRC.\n\n10.6.1 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values (variable) and a listw object (spatial weights matrix) providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(z != E(Ii)): the p-value of local moran statistic\n\nNote there is no County’s name in the output above.\nWe must first create a dataframe that appends the County’s name to its local moran (li) values.\n\nFips is an object of integer class. It contains the row ids if rows are to be arranged in alphabetical order.\nlocalMI[Fips,] would arrange the rows of localMI in alphabetical order of its countys’ name, retaining all the columns.\nThe row names would take the countys’ name.\n\n\ndata.frame(\n  localMI[fips,],\n  row.names=hunan$County[fips]) %>% \n  head()\n\n                     Ii          E.Ii       Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua     -2.249264e-02 -5.004845e-03 5.823550e-02 -0.07246715      0.9422301\nAnren     -3.993226e-01 -7.011066e-03 7.034768e-02 -1.47912938      0.1391057\nAnxiang   -1.468468e-03 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\nBaojing    3.473702e-01 -5.008916e-03 8.363556e-02  1.21846947      0.2230456\nChaling    2.055902e-02 -9.681197e-04 2.771090e-02  0.12931859      0.8971056\nChangning -2.986822e-05 -9.001050e-09 1.510502e-07 -0.07682771      0.9387606\n\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]))\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n10.6.1.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\nThe Pr.z….E.Ii.. containing p-value of the local moran I is renamed to PR.Ii\n\nhunan.localMI <- cbind(hunan,localMI) %>% \n    rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n10.6.1.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chunks below. For paletter colours, refer to colorbrewers link.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col='Ii',\n          style = 'pretty',\n          #palette = 'RdBu', # << refer to colourbrewer\n          title = 'Local Moran statistics') +\n  tm_borders(alpha= 0.5) + \n  tm_layout(main.title = \"Local Moran's I\",\n            legend.width= 0.25,\n            legend.height = 0.25)\n\n\n\n\n\n\n10.6.1.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\nVarious p valuesp values at 95% confidence level\n\n\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran's I p-values\",\n            legend.width= 0.25,\n            legend.height = 0.25)\n\n\n\n\n\n\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks = c(-Inf, 0.05, Inf),\n          palette = c('lightblue', 'grey'), \n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Significant Local Moran's I \\np-values at 95% Confidence level\",\n            main.title.size = 1,\n            legend.width = 0.25,\n            legend.height = 0.25)\n\n\n\n\n\n\n\n\n\n10.6.1.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col='Ii',\n          style = 'pretty',\n          #palette = 'RdBu', # << reger to colourbrewer\n          title = 'Local Moran statistics') +\n  tm_borders(alpha= 0.5) + \n  tm_layout(main.title = \"Local Moran's I\",\n            legend.width= 0.25,\n            legend.height = 0.25)\n\npvalue.map <-tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks = c(-Inf, 0.05, Inf),\n          palette = c('lightblue', 'grey'), \n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Significant Local Moran's I \\np-values at 95% Confidence level\",\n            main.title.size = 1,\n            legend.width = 0.25,\n            legend.height = 0.25)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1,ncol=2)\n\n\n\n\n\nFor the choropleth chart on the left, dark green represents clustering of similar values (HH or LL, to be determine by Moran scatterplot or LISAmap) while orange represents outlier regions (LH or HL). THe chart on the right would show us the significant clusters or outliers."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.7 Creating a LISA Cluster Map",
    "text": "10.7 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n10.7.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC,\n                  listw = rswm_q,\n                  labels = as.character(hunan$County),\n                  xlab = 'GDPPC 2012',\n                  ylab = 'Spatially lag GDPPC 2012')\n\n\n\n\n\nThe x -axis shows the original variable value at a particular spatial unit and the y-axis is the (weighted or without) average of the neighbouring variable values. The neighbour definition and relationship is embedded in the ‘listw’ argument where it could be\n\nspatial lag with row-standardised weights\nspatial lag as sum of neighbours values (binary)\nspatial window average (self-included)\nspatial window sum (self-included and binary)\ninverse-distance\n\nNotice that the plot is split in 4 quadrants.\n\nClusters and + spatial autocorrelation: The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\nCluster and + spatial autocorrelation: Bottom left are the Low-low.\nOutlier and - spatial autocorrelation: Top left contains spatial units with low GDPPC and surrounded by higher values.\nOutlier and - spatial autocorrelation: Bottom right contains spatial units with higher GDPPC and surrounded by relatively lower values.\n\n\n\n\n10.7.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\nhunan$GDPPC will have a mean of 0 and a standard deviation of 1.\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into our hunan dataframe\nWithout setting as vector, scale(hunan$GDPPC) is a matrix array.\nNow, plot the Moran scatterplot again by using the code chunk below.\n\nnci2 <- moran.plot(hunan$Z.GDPPC,\n                  listw = rswm_q,\n                  labels = as.character(hunan$County),\n                  xlab = 'GDPPC 2012',\n                  ylab = 'Spatially lag z-GDPPC 2012')\n\n\n\n\n\nWe notice that both axes are standardised with mean =0 and sd of 1.\n\n10.7.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nThe code initializes a numeric vector named quadrant with a length equal to the number of rows in the localMI data frame. The vector is initially filled with NA.\n\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derive the spatially lagged variable of interest (i.e. GDPPC) using lag.listw() from the spdep package and centers the spatially lagged variable around its mean via subtraction.\n\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nclass(DV)\n\n[1] \"numeric\"\n\n\nRetrieve the first column (li aka Moran’s I values) from the localMI matrix and centered around its mean via subtraction.\n\n#colnames(localMI)\nLM_I <- localMI[,1] - mean(localMI[,1])\nclass(LM_I)\n\n[1] \"numeric\"\n\n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif <- 0.05\n\nThese four command lines define the (1) low-low, (2) low-high , (3) high-low and (4) high-high categories. Comparing the local Moran’s I values with its lag-variable values.\n\nquadrant[DV <0 & LM_I>0] <- 1 # LL\nquadrant[DV >0 & LM_I<0] <- 2 # LH\nquadrant[DV <0 & LM_I<0] <- 3 # HL\nquadrant[DV >0 & LM_I>0] <- 4 # HH\n\n\nLM_I > 0 : cluster of similar values.\n(1) When DV < 0 and LM_I > 0, low values of spatial lag variables cluster together with low non-lag variable. Thus Low-low.\n(4) When DV > 0 and LM_I > 0, high values of spatial lag variables cluster together with high non-lag variable. Thus high-high.\nLM_I <0 : cluster of dissimilar values.\n(2) When DV > 0 and LM < 0, high values of spatial lag variables are among low non-lag variable, thus low-high. (see moran scatterplot’s Y axis)\n(3) When DV < 0 and LM_I < 0 , low values of spatial lag variables are among the high non-lag variable, thus high-low.\n\n\nquadrant\n\n [1] 2 3 3 2 2 3 4 4 4 2 2 3 2 2 2 4 3 3 2 3 3 2 3 3 3 3 3 3 1 3 1 3 3 3 3 3 1 3\n[39] 1 1 3 1 1 1 3 4 2 1 3 3 3 3 1 3 1 2 2 2 3 3 3 3 2 3 2 4 2 4 3 3 4 2 2 4 3 2\n[77] 3 2 1 2 3 3 2 4 3 2 3 3\n\n\nLastly, places non-significant Moran in the category 0. The fifth column of the localMI matrix is the significant value.\n\nquadrant[localMI[,5]>signif] <- 0\nquadrant\n\n [1] 0 0 0 0 0 0 4 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 0 4 0 0 4 0 0\n[77] 0 2 0 0 0 0 0 4 0 2 0 0\n\n\nCombining all the steps above into one code chunk:\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I <- localMI[,1]   \nsignif <- 0.05       \nquadrant[DV <0 & LM_I>0] <- 1 # LL\nquadrant[DV >0 & LM_I<0] <- 2 # LH\nquadrant[DV <0 & LM_I<0] <- 3 # HL\nquadrant[DV >0 & LM_I>0] <- 4 # HH\nquadrant[localMI[,5]>signif] <- 0 # non-sig\n\n\n\n10.7.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], #index starts from 1\n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_text('County',\n          size = 0.5)+\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title='LISA map (significant)',\n            legend.width = 0.25,\n            legend.height = 0.25)\n\nLISAmap\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding LISA map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ntmap_arrange(localMI.map, LISAmap,\n             asp=1,\n             ncol=2 )\n\n\n\n\n\n‘Clustering of dissimilar values’ (Local Moran I < 0) is also known as ‘dispersion’ or ‘outlier’ region.\nThe darker green regions shows signs of clustering of similar values while the orange regions shows signs of dispersion (cluster of dissimilar values). The LISA map on the right will be able to give more detailed insights, for e.g. for cluster regions whether is it HH or LL and for the outliers region whether its LH or HL.\n\nThe GDPPC and LISA Map can also be placed side by side .\n\ngdppc <- qtm(hunan, 'GDPPC') +\n  tm_layout(main.title= 'GDPPC')\n\ntmap_arrange(gdppc, LISAmap,\n             asp=1,\n             ncol=2)\n\n\n\n\n\nLISAmap:\nThe dark blue and red regions are clusters of LL and HH respectively.\nThe light blue regions are outliers (LH), where those regions have relatively lower values than their neighbours."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.8 Hot Spot and Cold Spot Area Analysis",
    "text": "10.8 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot (high values amongst high) and/or cold spot areas (low values amongst low).\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n10.8.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity (use of distance) to identify where either high or low values cluster spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n10.8.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance. The spatial weights matrix used in this section is binary.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\nFrom Hands-on_Ex2.1:\n\n\n10.8.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nThe latitude can be accessed using [[2]]\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nWith both long and lat, we use cbind to put longitude and latitude into the same object. The object coords represents the CG of all 88 spatial units/ features/ polygons.\n\ncoords <- cbind(longitude, latitude)\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n10.8.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep. (k=1)\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length (distance) of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist(). The purpose of doing so it to run the summary() function on it in order for us to decide on a distance threshold. Decide on a distance threshold value such that each spatial unit has at least a neighbour.\n\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour. The threshold chosen is 62 km.\n\n\n10.8.2.3 Computing fixed distance weight matrix\nNow, we will compute the ‘nb’ class by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nsummary(wm_d62)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\n\nSneak peak into neighbours lists of first 3 polygons\n\nwm_d62[1:3]\n\n[[1]]\n[1]  3  4  5 57 64\n\n[[2]]\n[1] 57 58 78 85\n\n[[3]]\n[1]  1  4  5 57\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights matrix object.\n\nwm62_lw <- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nSneak peak of wm62_lw spatial weights matrix (binary). The class is ‘listw’ and ‘nb’.\n\nwm62_lw$weights[1:3]\n\n[[1]]\n[1] 1 1 1 1 1\n\n[[2]]\n[1] 1 1 1 1\n\n[[3]]\n[1] 1 1 1 1\n\n\n\n\n\n10.8.3 Computing adaptive distance weight matrix (Fixed # of nb)\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\nknn is a nb object.\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nSneak peak into the neighbour structure/list of knn.\n\nknn[1:3]\n\n[[1]]\n[1]  2  3  4  5 57 58 64 85\n\n[[2]]\n[1]  1  3  5 57 58 68 78 85\n\n[[3]]\n[1]  1  2  4  5 57 58 64 85\n\n\n\nEach spatial unit has exactly 8 neighbours.\n\nNext, nb2listw() is used to convert the nb object into spatial weights object, knn_lw.\n\nknn_lw <- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014\n\n\nSneak peak into the weights of knn_lw spatial weights matrix.\n\nknn_lw$weights[1:3]\n\n[[1]]\n[1] 1 1 1 1 1 1 1 1\n\n[[2]]\n[1] 1 1 1 1 1 1 1 1\n\n[[3]]\n[1] 1 1 1 1 1 1 1 1\n\n# attributes(knn_lw)"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#computing-gi-statistics",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#computing-gi-statistics",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.9 Computing Gi statistics",
    "text": "10.9 Computing Gi statistics\n\n10.9.1 Gi statistics using fixed distance\n\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n#attributes(gi.fixed)\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to FALSE (as default), “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nThe code chunk above performs three tasks. First, it converts the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n10.9.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc <- qtm(hunan, \"GDPPC\") +\n    tm_layout(main.title= 'GDPPC')\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title= 'local Gi (fixed-dist)')\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\nChart on the right has Red regions (hotspots with high values clustering together) and Blue regions (Coldspots with low values clustering together).\n\nBut we are not sure of the hot/cold spots are statistically significant. We should extract the p-values in order to know.\n\n10.9.2.1 Including significant Gi values\nExamine the structure of gi.fixed\n\nstr(gi.fixed)\n\n 'localG' num [1:88] 0.436 -0.266 -0.073 0.413 0.273 ...\n - attr(*, \"internals\")= num [1:88, 1:5] 0.0642 0.0423 0.045 0.0395 0.0498 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:5] \"Gi\" \"E(Gi)\" \"V(Gi)\" \"Z(Gi)\" ...\n - attr(*, \"cluster\")= Factor w/ 2 levels \"Low\",\"High\": 1 1 2 2 2 2 2 2 2 1 ...\n - attr(*, \"gstari\")= logi FALSE\n - attr(*, \"call\")= language localG(x = hunan$GDPPC, listw = wm62_lw)\n\n\ngi.fixed is a numeric vector (or array) with some additional attributes. To access the Gi p-values, we can access the “Pr(z != E(Gi))” column using the attribute function attr()\n\np_value_fix <- attr(gi.fixed, \"internals\")[, \"Pr(z != E(Gi))\"]\n\nAppend p-values to hunan.gi\n\nhunan.gi <- cbind(hunan.gi, as.vector(p_value_fix)) %>% \n  rename(p_value_fix = as.vector.p_value_fix.)\n\nPlot only significant Gi regions\n\nGimap_sig<-tm_shape(hunan.gi) +\n  tm_fill(col = \"p_value_fix\", \n          breaks = c(-Inf, 0.05, Inf),\n          palette = c('lightgreen', 'grey'),\n          title = \"local Gi p-values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title= 'local Gi signif 95%')\n\ntmap_arrange(gdppc, Gimap, Gimap_sig,\n             asp=1,\n             nrow=2,\n             ncol=2)\n\n\n\n\n\nThe bottom left chart shows significant hot/cold spots at 95% confidence level. We can see that all of the coldspots are not statistically significant here.\n\n\n\n\n10.9.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knn_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n10.9.4 Mapping Gi values with adaptive distance weights\nTo visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using adaptive distance weight matrix.\n\ngdppc<- qtm(hunan, \"GDPPC\") +\n    tm_layout(main.title= 'GDPPC')\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)+\n    tm_layout(main.title= 'local Gi (adaptive)')\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\n\nFrom fixed-dist to adaptive-dist spatial weights matrix, the definition of neighbours and the way they interact with one another have changed, we can observe differences in identified cold/hot spots between the above chart and the one from the previous section."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#references",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#references",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "11 References",
    "text": "11 References\nTin Seong Kam. “9 Global Measures of Spatial Autocorrelation” From R for Geospatial Data Science and Analytics https://r4gdsa.netlify.app/chap09\nTin Seong Kam. “10 Local Measures of Spatial Autocorrelation” From R for Geospatial Data Science and Analytics https://r4gdsa.netlify.app/chap10"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_2.html#summary",
    "href": "Hands-on_Ex2/Hands-on_Ex2_2.html#summary",
    "title": "Hands-on Exercise 2.2 and 2.3: Global and Local Measures of Spatial Autocorrelation",
    "section": "12 Summary",
    "text": "12 Summary\n\nWays to check R object\nclass(), methods(class=class(object_name)), str(), attributes(), spatial_weights_matrix$weights"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2_3.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2_3.html",
    "title": "Hands-on Exercise 2.3:Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "This hands-on exercise is combined with Hands-on_Ex2_2."
  },
  {
    "objectID": "Hands-on_Ex3/data/geospatial/hexagon/hexagon.html",
    "href": "Hands-on_Ex3/data/geospatial/hexagon/hexagon.html",
    "title": "NYX Geospatial App",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  },
  {
    "objectID": "Hands-on_Ex3/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "href": "Hands-on_Ex3/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "title": "NYX Geospatial App",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall.\nBy the end to this hands-on exercise, we will be able to:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html#getting-started",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html#getting-started",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "15.2 Getting Started",
    "text": "15.2 Getting Started\nFor the purpose of this exercise, four R packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps\nstplanr for solving common problems in transport planning and modelling, such as how to best get from point A to point B\nggpubr for some easy-to-use functions for creating and customizing ‘ggplot2’- based publication ready plots.\nperformance for for computing measures to assess model quality, which are not directly provided by R’s ‘base’ or ‘stats’ packages. The primary goal of the performance package is to provide utilities for computing indices of model quality and goodness of fit. These include measures like r-squared (R2), root mean squared error (RMSE)\n\n\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html#preparing-the-flow-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html#preparing-the-flow-data",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "15.3 Preparing the Flow Data",
    "text": "15.3 Preparing the Flow Data\n\n15.3.1 Importing the OD data\nImport the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nodbus <- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\nDisplay the odbus tibble data table by using the code chunk below.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          <chr> \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            <chr> \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       <dbl> 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             <chr> \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      <chr> \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE <chr> \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         <dbl> 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\nA quick check of odbus tibble data frame shows that the values in ORIGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.\n\nodbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n15.3.2 Extracting the study data\nThe data in odbus is generalised into weekend and weekday data. For the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o’clock. After the group-by and sum, the total rows reduced from 5,709,512 ro 241,503.\n\nodbus6_9 <- odbus %>%\n  filter(DAY_TYPE == \"WEEKDAY\") %>%\n  filter(TIME_PER_HOUR >= 6 &\n           TIME_PER_HOUR <= 9) %>%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %>%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nPrint the content of odbus6_9\n\ndatatable(odbus6_9,\n          class = 'cell-border stripe',\n          options = list(pageLength = 5))\n\n\n\n\n\n\nIf we would like to, we could save the output in rds format for future use. We need to ensure that there exists a folder called ‘rds’ in ‘data’ folder before running the code chunk.\n\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\n\nTo read rds files:\n\nodbus6_9 <- read_rds(\"data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html#working-with-geospatial-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html#working-with-geospatial-data",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "15.4 Working with Geospatial Data",
    "text": "15.4 Working with Geospatial Data\nIn this exercise, two geospatial data will be used. They are:\n\nBusStop: This data provides the location of bus stop as at the third quarter of 2023. This data is refreshed quarterly by LTA. The last update was in July 2023.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n15.4.1 Importing geospatial data\nLoad the BusStop geospatial data using the st_read() function of sf package. Using the st_crs(busstop) will show that the coordinate system used is WSG84 (decimal deg). Using st_tranform(), we will convert the geographical coordinates system to SIngapore’s projected coordinate system crs=3414.\nNote that there are repeated bus stop ids , however they have different bus stop roof ids and geometry values.\n\nbusstop <- st_read(dsn = \"data/geospatial/BusStopLocation/BusStopLocation_Jul2023\",\n                   layer = \"BusStop\") %>%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex3\\data\\geospatial\\BusStopLocation\\BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nNext load the mpsz data. There are 332 planning subzones in Singapore.\n\nmpsz <- st_read(dsn = \"data/geospatial/MPSZ-2019\",\n                   layer = \"MPSZ-2019\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex3\\data\\geospatial\\MPSZ-2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "15.5 Geospatial data wrangling",
    "text": "15.5 Geospatial data wrangling\n\n15.5.1 Combining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame. The output of st_intersection() is a point sf object. We do not need and therefore will drop the geometry. The number of observations reduced from 5,161 to 5,156 after operation, suggesting that 5 bus stops have been dropped as their point geometry is not within the polygon boundary of sf df mpsz.\n\nbusstop_mpsz <- st_intersection(busstop, mpsz) %>%\n  select(BUS_STOP_N, SUBZONE_C, LOC_DESC) %>%\n  st_drop_geometry()\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary.\n\n\n\n\ndatatable(busstop_mpsz,\n          class = 'cell-border stripe',\n          options = list(pageLength = 5))\n\n\n\n\n\n\nSave the output into rds format\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame. By doing so, we get the fields ‘ORIGIN_BS’, ‘DESTIN_BS” and ’ORIGIN_SZ’ all in a df .\n\nbusstop_mpsz %>%\n  group_by(BUS_STOP_N, SUBZONE_C) %>%\n  filter(n()>1) %>%\n  ungroup()\n\n# A tibble: 26 × 3\n   BUS_STOP_N SUBZONE_C LOC_DESC       \n   <chr>      <chr>     <chr>          \n 1 11009      QTSZ01    Ghim Moh Ter   \n 2 11009      QTSZ01    GHIM MOH TER   \n 3 82221      GLSZ05    BLK 3A         \n 4 82221      GLSZ05    Blk 3A         \n 5 22501      JWSZ09    Blk 662A       \n 6 22501      JWSZ09    BLK 662A       \n 7 96319      TMSZ05    Yusen Logistics\n 8 96319      TMSZ05    YUSEN LOGISTICS\n 9 43709      BKSZ07    BLK 644        \n10 43709      BKSZ07    BLK 644        \n# ℹ 16 more rows\n\n\nThe join columns will be ‘ORIGIN_PT_CODE’ from odbus6_9 df and ‘BUS_STOP_N’ from busstop_mpsz df. The columns will also be renamed.\nBefore left_join, odbus6_9 has 241,503 rows, after left join od_data has 242,235 rows.\n\nod_data <- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %>%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nCheck for duplicate for proceeding\n\nduplicate <- od_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\nduplicate\n\n# A tibble: 794 × 5\n   ORIGIN_BS DESTIN_BS TRIPS ORIGIN_SZ LOC_DESC\n   <chr>     <fct>     <dbl> <chr>     <chr>   \n 1 43709     43009        15 BKSZ07    BLK 644 \n 2 43709     43009        15 BKSZ07    BLK 644 \n 3 43709     43419        42 BKSZ07    BLK 644 \n 4 43709     43419        42 BKSZ07    BLK 644 \n 5 43709     43469         1 BKSZ07    BLK 644 \n 6 43709     43469         1 BKSZ07    BLK 644 \n 7 43709     43479        62 BKSZ07    BLK 644 \n 8 43709     43479        62 BKSZ07    BLK 644 \n 9 43709     43489        23 BKSZ07    BLK 644 \n10 43709     43489        23 BKSZ07    BLK 644 \n# ℹ 784 more rows\n\n\nRemove the duplicated records. The od_data df reduced from 242,235 rows to 241,838 rows after moving duplicates.\n\nod_data <- unique(od_data)\n\nDouble check again\n\nod_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\n# A tibble: 0 × 5\n# ℹ 5 variables: ORIGIN_BS <chr>, DESTIN_BS <fct>, TRIPS <dbl>,\n#   ORIGIN_SZ <chr>, LOC_DESC <chr>\n\n\nPrint the current od_data df to see what we are still lacking. We are will missing the destination subzone codes.\n\nknitr::kable(head(od_data,3))\n\n\n\n\nORIGIN_BS\nDESTIN_BS\nTRIPS\nORIGIN_SZ\nLOC_DESC\n\n\n\n\n01012\n01112\n276\nRCSZ10\nHOTEL GRAND PACIFIC\n\n\n01012\n01113\n143\nRCSZ10\nHOTEL GRAND PACIFIC\n\n\n01012\n01121\n66\nRCSZ10\nHOTEL GRAND PACIFIC\n\n\n\n\n\nAgain, get the destination subzone code for each destination bus stops by performing a left_join again with busstop_mpsz (contains subzone_c codes for each bus stop id).\nAfter left_join, the number of rows increased from 241,838 rows to 242,831 rows.\n\nod_data <- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"))\n\nCheck for duplicates\n\nduplicate <- od_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\nduplicate\n\n# A tibble: 880 × 7\n   ORIGIN_BS DESTIN_BS TRIPS ORIGIN_SZ LOC_DESC.x           SUBZONE_C LOC_DESC.y\n   <chr>     <chr>     <dbl> <chr>     <chr>                <chr>     <chr>     \n 1 01013     51071         1 RCSZ10    ST JOSEPH'S CH       CCSZ01    MACRITCHI…\n 2 01013     51071         1 RCSZ10    ST JOSEPH'S CH       CCSZ01    MACRITCHI…\n 3 01112     51071        65 RCSZ10    OPP BUGIS STN EXIT C CCSZ01    MACRITCHI…\n 4 01112     51071        65 RCSZ10    OPP BUGIS STN EXIT C CCSZ01    MACRITCHI…\n 5 01112     53041         5 RCSZ10    OPP BUGIS STN EXIT C BSSZ01    Upp Thoms…\n 6 01112     53041         5 RCSZ10    OPP BUGIS STN EXIT C BSSZ01    Upp Thoms…\n 7 01121     51071         3 RCSZ04    STAMFORD PR SCH      CCSZ01    MACRITCHI…\n 8 01121     51071         3 RCSZ04    STAMFORD PR SCH      CCSZ01    MACRITCHI…\n 9 01239     51071        22 KLSZ09    SULTAN PLAZA         CCSZ01    MACRITCHI…\n10 01239     51071        22 KLSZ09    SULTAN PLAZA         CCSZ01    MACRITCHI…\n# ℹ 870 more rows\n\n\nRemove duplicates\n\nod_data <- unique(od_data)\n\nSneak peak of the current od_data\n\nknitr::kable(head(od_data,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\nORIGIN_BS\nDESTIN_BS\nTRIPS\nORIGIN_SZ\nLOC_DESC.x\nSUBZONE_C\nLOC_DESC.y\n\n\n\n\n01012\n01112\n276\nRCSZ10\nHOTEL GRAND PACIFIC\nRCSZ10\nOPP BUGIS STN EXIT C\n\n\n01012\n01113\n143\nRCSZ10\nHOTEL GRAND PACIFIC\nDTSZ01\nBUGIS STN EXIT B\n\n\n01012\n01121\n66\nRCSZ10\nHOTEL GRAND PACIFIC\nRCSZ04\nSTAMFORD PR SCH\n\n\n\n\n\nThe code chunk below will do the following:\n\nRenames the destination ‘SUBZONE_C’ to ‘DESTIN_SZ’.\nThere are missing subzone codes for some of the origin and destination bus stop because the bus stops location in July 2023 could be more outdated than August bus stop 2023. We will drop columns with missing values.\nGroup-by origin subzone and destination subzone to generate a new field ‘MORNING_PEAK’ that contains the summation of all trips from subzone A to subzone B.\n\n\nod_data <- od_data %>%\n  rename(DESTIN_SZ = SUBZONE_C) %>%\n  drop_na() %>%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %>%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\nTake a look at our final od_data df\n\nknitr::kable(head(od_data %>% \n                    arrange(desc(MORNING_PEAK)),\n                  10))\n\n\n\n\nORIGIN_SZ\nDESTIN_SZ\nMORNING_PEAK\n\n\n\n\nTMSZ02\nTMSZ02\n350755\n\n\nWDSZ03\nWDSZ03\n239791\n\n\nJWSZ08\nJWSZ09\n234343\n\n\nBDSZ04\nBDSZ04\n217917\n\n\nJWSZ09\nJWSZ09\n153055\n\n\nYSSZ03\nYSSZ01\n152800\n\n\nJWSZ04\nJWSZ04\n149110\n\n\nTMSZ03\nTMSZ02\n134196\n\n\nPRSZ05\nPRSZ03\n103148\n\n\nJWSZ03\nJWSZ04\n99666\n\n\n\n\n\nSave the output into an rds file format.\n\nwrite_rds(od_data, \"data/rds/od_data.rds\")\n\n\nod_data <- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html#visualising-spatial-interaction",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html#visualising-spatial-interaction",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "15.6 Visualising Spatial Interaction",
    "text": "15.6 Visualising Spatial Interaction\nIn this section, wewill learn how to prepare a desire line by using stplanr package.\n\n15.6.1 Removing intra-zonal flows\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows. It does so by removing the flows that originate and ends in the same subzone.\nRows reduced from 20,987 to 20,697.\n\nod_data1 <- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\n15.6.2 Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\nod_data1 is aspatial while mpsz is geospatial data.\nArguments\n\nflow\n\nA data frame representing origin-destination data. The first two columns of this data frame should correspond to the first column of the data in the zones. Thus in cents_sf(), the first column is geo_code. This corresponds to the first two columns of flow().\n\nzones\n\nA spatial object representing origins (and destinations if no separate destinations object is provided) of travel.\n\ndestinations\n\nA spatial object representing destinations of travel flows.\n\nzone_code\n\nName of the variable in zones containing the ids of the zone. By default this is the first column names in the zones.\n\n\nThe output flowLine is a sf LINESTRING object.\n\nflowLine <- od2line(flow=od_data1,\n                    zones= mpsz,\n                    zone_code= 'SUBZONE_C')\n\nflowLine\n\nSimple feature collection with 20697 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 5105.594 ymin: 25813.33 xmax: 49483.22 ymax: 49552.79\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   ORIGIN_SZ DESTIN_SZ MORNING_PEAK                       geometry\n1     AMSZ01    AMSZ02        11742 LINESTRING (29501.77 39419....\n2     AMSZ01    AMSZ03        14886 LINESTRING (29501.77 39419....\n3     AMSZ01    AMSZ04         3237 LINESTRING (29501.77 39419....\n4     AMSZ01    AMSZ05         9349 LINESTRING (29501.77 39419....\n5     AMSZ01    AMSZ06         2231 LINESTRING (29501.77 39419....\n6     AMSZ01    AMSZ07         1714 LINESTRING (29501.77 39419....\n7     AMSZ01    AMSZ08         2624 LINESTRING (29501.77 39419....\n8     AMSZ01    AMSZ09         2371 LINESTRING (29501.77 39419....\n9     AMSZ01    AMSZ10          183 LINESTRING (29501.77 39419....\n10    AMSZ01    AMSZ11          930 LINESTRING (29501.77 39419....\n\n\n\n\n15.6.3 Visualising the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\nArguments of tm_lines():\nlwd: line width. Either a numeric value or a data variable. In the latter case, the class of the highest values (see style) will get the line width defined by scale. If multiple values are specified, small multiples are drawn (see details).\nstyle: method to process the color scale when col is a numeric variable. Discrete gradient options are \"cat\", \"fixed\", \"sd\", \"equal\", \"pretty\", \"quantile\", \"kmeans\", \"hclust\", \"bclust\", \"fisher\", \"jenks\", \"dpih\", \"headtails\", and \"log10_pretty\". A numeric variable is processed as a categorical variable when using \"cat\", i.e. each unique value will correspond to a distinct category\nscale: line width multiplier number.\nn: preferred number of color scale classes. Only applicable when lwd is the name of a numeric variable.\n\ntm_shape(mpsz) +\n  tm_polygons() +\n  flowLine %>% \n  tm_shape() +\n  tm_lines(lwd = 'MORNING_PEAK',\n           style = 'quantile',\n           scale= c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha= 0.3)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRendering process takes about 1 min because of the transparency argument alpha.\n\n\nWhen the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.\n\ntmap_mode('view')\ntmap_options(check.and.fix = TRUE)\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %>%  \n  filter(MORNING_PEAK >= 5000) %>%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\n\n\n\nttm()\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %>%  \n  filter(MORNING_PEAK >= 5000) %>%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\n\n#| eval: false\n#| echo: false\n#| fig-width: 14\n#| fig-asp: 0.68\n#| code-fold: True"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_1.html#summaries",
    "href": "Hands-on_Ex3/Hands-on_Ex3_1.html#summaries",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "Summaries",
    "text": "Summaries\nOD matrix is often incomplete. Imagine trying to complete the OD matrix, it would involve us doing spatial interaction or OD surveys to find the missing values. There are 332 subzones in Singapore, and each survey is expensive,. In additon, OD matrix is constantly changing as flow patterns changes. We are trying to predict flows between origins and destinations. Flow could be thought of a function of (1) attribute of origin (propulsiveness) (2) attribute of destination (attractiveness) and (3) cost friction (like distance or transport cost or public transport stops). Assumption is that the benefits must outweigh the cost in order for flow to happen.\nGravity model takes into consideration the interaction between all origin and destination locations.\nPotential model takes in consideration the interaction between a location and all other location pairs. (Good for measuring accessibility)\nRetail model is commonly used by franchise like KFC / Pizza Hut to determine their area/region of service (aka delivery zones) for each outlet.\nThere are 4 variations in the Gravity model:\n\nUncontrained: only the overall outflow is fixed and total outflow from origins = total inflow to destinations\nOrigin constrained: outflow by origin is fixed.\nDestination constrained: inflow by destination is fixed.\nDoubly constrained: outflow by origin and inflow by destination is fixed.\n\nTo calculate flow from each origin to each destination, we need parameters like k, alpha, lambda and beta. The beta for distance is usually negative because we assume that there is an inverse relationship between interaction and distance, like Newtonian physics and laws of gravity."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic. Understanding what factors and logic went into the decision-making behind those human-induced movements and interdependencies is important because it enables policy makwers to better understand, predict, manage, and help plan for such circulation. For example, policy makers can make informed decisions about how to better allocate resources to improve traffic in a city or to speed up shipments of perishable foodstuffs. It has to do with having a good understanding of the overall situation.\nSpatial Interaction Models (SIMs) are mathematical models for estimating movement between spatial entities developed by Alan Wilson in the late 1960s and early 1970, with considerable uptake and refinement for transport modelling since then Boyce and Williams (2015). There are four main types of traditional SIMs (Wilson 1971): - Unconstrained - Production-constrained - Attraction-constrained - Doubly-constrained\nBoth ordinary least square (OLS) and negative binomial (NB) regression methods have been used extensively to calibrate OD flow models by processing flow data as different types of dependent variables.\nBy the end to this hands-on exercise, we will be able to:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#getting-started",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#getting-started",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.2 Getting Started",
    "text": "16.2 Getting Started\nFor the purpose of this exercise, four R packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps\nstplanr for solving common problems in transport planning and modelling, such as how to best get from point A to point B\nggpubr for some easy-to-use functions for creating and customizing ‘ggplot2’- based publication ready plots.\nperformance for for computing measures to assess model quality, which are not directly provided by R’s ‘base’ or ‘stats’ packages. The primary goal of the performance package is to provide utilities for computing indices of model quality and goodness of fit. These include measures like r-squared (R2), root mean squared error (RMSE)\n\n\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#preparing-the-flow-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#preparing-the-flow-data",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.3 Preparing the Flow Data",
    "text": "16.3 Preparing the Flow Data\n\n16.3.1 Importing the OD data\nImport the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nodbus <- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\nDisplay the odbus tibble data table by using the code chunk below.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          <chr> \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            <chr> \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       <dbl> 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             <chr> \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      <chr> \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE <chr> \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         <dbl> 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\nA quick check of odbus tibble data frame shows that the values in ORIGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.\n\nodbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n16.3.2 Extracting the study data\nThe data in odbus is generalised into weekend and weekday data. For the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o’clock. After the group-by and sum, the total rows reduced from 5,709,512 ro 241,503.\n\nodbus6_9 <- odbus %>%\n  filter(DAY_TYPE == \"WEEKDAY\") %>%\n  filter(TIME_PER_HOUR >= 6 &\n           TIME_PER_HOUR <= 9) %>%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %>%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nPrint the content of odbus6_9\n\ndatatable(odbus6_9,\n          class = 'cell-border stripe',\n          options = list(pageLength = 5))\n\n\n\n\n\n\nIf we would like to, we could save the output in rds format for future use. We need to ensure that there exists a folder called ‘rds’ in ‘data’ folder before running the code chunk.\n\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\n\nTo read rds files:\n\nodbus6_9 <- read_rds(\"data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#working-with-geospatial-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#working-with-geospatial-data",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.4 Working with Geospatial Data",
    "text": "16.4 Working with Geospatial Data\nIn this exercise, two geospatial data will be used. They are:\n\nBusStop: This data provides the location of bus stop as at the third quarter of 2023. This data is refreshed quarterly by LTA. The last update was in July 2023.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n16.4.1 Importing geospatial data\nLoad the BusStop geospatial data using the st_read() function of sf package. Using the st_crs(busstop) will show that the coordinate system used is WSG84 (decimal deg). Using st_tranform(), we will convert the geographical coordinates system to SIngapore’s projected coordinate system crs=3414.\nNote that there are repeated bus stop ids , however they have different bus stop roof ids and geometry values.\n\nbusstop <- st_read(dsn = \"data/geospatial/BusStopLocation/BusStopLocation_Jul2023\",\n                   layer = \"BusStop\") %>%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex3\\data\\geospatial\\BusStopLocation\\BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nNext load the mpsz data. There are 332 planning subzones in Singapore.\n\nmpsz <- st_read(dsn = \"data/geospatial/MPSZ-2019\",\n                   layer = \"MPSZ-2019\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Hands-on_Ex3\\data\\geospatial\\MPSZ-2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.5 Geospatial data wrangling",
    "text": "16.5 Geospatial data wrangling\n\n16.5.1 Combining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame. The output of st_intersection() is a point sf object. We do not need and therefore will drop the geometry. The number of observations reduced from 5,161 to 5,156 after operation, suggesting that 5 bus stops have been dropped as their point geometry is not within the polygon boundary of sf df mpsz.\n\nbusstop_mpsz <- st_intersection(busstop, mpsz) %>%\n  select(BUS_STOP_N, SUBZONE_C, LOC_DESC) %>%\n  st_drop_geometry()\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary.\n\n\n\n\ndatatable(busstop_mpsz,\n          class = 'cell-border stripe',\n          options = list(pageLength = 5))\n\n\n\n\n\n\nSave the output into rds format\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame. By doing so, we get the fields ‘ORIGIN_BS’, ‘DESTIN_BS” and ’ORIGIN_SZ’ all in a df .\n\nbusstop_mpsz %>%\n  group_by(BUS_STOP_N, SUBZONE_C) %>%\n  filter(n()>1) %>%\n  ungroup()\n\n# A tibble: 26 × 3\n   BUS_STOP_N SUBZONE_C LOC_DESC       \n   <chr>      <chr>     <chr>          \n 1 11009      QTSZ01    Ghim Moh Ter   \n 2 11009      QTSZ01    GHIM MOH TER   \n 3 82221      GLSZ05    BLK 3A         \n 4 82221      GLSZ05    Blk 3A         \n 5 22501      JWSZ09    Blk 662A       \n 6 22501      JWSZ09    BLK 662A       \n 7 96319      TMSZ05    Yusen Logistics\n 8 96319      TMSZ05    YUSEN LOGISTICS\n 9 43709      BKSZ07    BLK 644        \n10 43709      BKSZ07    BLK 644        \n# ℹ 16 more rows\n\n\nThe join columns will be ‘ORIGIN_PT_CODE’ from odbus6_9 df and ‘BUS_STOP_N’ from busstop_mpsz df. The columns will also be renamed.\nBefore left_join, odbus6_9 has 241,503 rows, after left join od_data has 242,235 rows.\n\nod_data <- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %>%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nCheck for duplicate for proceeding\n\nduplicate <- od_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\nduplicate\n\n# A tibble: 794 × 5\n   ORIGIN_BS DESTIN_BS TRIPS ORIGIN_SZ LOC_DESC\n   <chr>     <fct>     <dbl> <chr>     <chr>   \n 1 43709     43009        15 BKSZ07    BLK 644 \n 2 43709     43009        15 BKSZ07    BLK 644 \n 3 43709     43419        42 BKSZ07    BLK 644 \n 4 43709     43419        42 BKSZ07    BLK 644 \n 5 43709     43469         1 BKSZ07    BLK 644 \n 6 43709     43469         1 BKSZ07    BLK 644 \n 7 43709     43479        62 BKSZ07    BLK 644 \n 8 43709     43479        62 BKSZ07    BLK 644 \n 9 43709     43489        23 BKSZ07    BLK 644 \n10 43709     43489        23 BKSZ07    BLK 644 \n# ℹ 784 more rows\n\n\nRemove the duplicated records. The od_data df reduced from 242,235 rows to 241,838 rows after moving duplicates.\n\nod_data <- unique(od_data)\n\nDouble check again\n\nod_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\n# A tibble: 0 × 5\n# ℹ 5 variables: ORIGIN_BS <chr>, DESTIN_BS <fct>, TRIPS <dbl>,\n#   ORIGIN_SZ <chr>, LOC_DESC <chr>\n\n\nPrint the current od_data df to see what we are still lacking. We are will missing the destination subzone codes.\n\nknitr::kable(head(od_data,3))\n\n\n\n\nORIGIN_BS\nDESTIN_BS\nTRIPS\nORIGIN_SZ\nLOC_DESC\n\n\n\n\n01012\n01112\n276\nRCSZ10\nHOTEL GRAND PACIFIC\n\n\n01012\n01113\n143\nRCSZ10\nHOTEL GRAND PACIFIC\n\n\n01012\n01121\n66\nRCSZ10\nHOTEL GRAND PACIFIC\n\n\n\n\n\nAgain, get the destination subzone code for each destination bus stops by performing a left_join again with busstop_mpsz (contains subzone_c codes for each bus stop id).\nAfter left_join, the number of rows increased from 241,838 rows to 242,831 rows.\n\nod_data <- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"))\n\nCheck for duplicates\n\nduplicate <- od_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\nduplicate\n\n# A tibble: 880 × 7\n   ORIGIN_BS DESTIN_BS TRIPS ORIGIN_SZ LOC_DESC.x           SUBZONE_C LOC_DESC.y\n   <chr>     <chr>     <dbl> <chr>     <chr>                <chr>     <chr>     \n 1 01013     51071         1 RCSZ10    ST JOSEPH'S CH       CCSZ01    MACRITCHI…\n 2 01013     51071         1 RCSZ10    ST JOSEPH'S CH       CCSZ01    MACRITCHI…\n 3 01112     51071        65 RCSZ10    OPP BUGIS STN EXIT C CCSZ01    MACRITCHI…\n 4 01112     51071        65 RCSZ10    OPP BUGIS STN EXIT C CCSZ01    MACRITCHI…\n 5 01112     53041         5 RCSZ10    OPP BUGIS STN EXIT C BSSZ01    Upp Thoms…\n 6 01112     53041         5 RCSZ10    OPP BUGIS STN EXIT C BSSZ01    Upp Thoms…\n 7 01121     51071         3 RCSZ04    STAMFORD PR SCH      CCSZ01    MACRITCHI…\n 8 01121     51071         3 RCSZ04    STAMFORD PR SCH      CCSZ01    MACRITCHI…\n 9 01239     51071        22 KLSZ09    SULTAN PLAZA         CCSZ01    MACRITCHI…\n10 01239     51071        22 KLSZ09    SULTAN PLAZA         CCSZ01    MACRITCHI…\n# ℹ 870 more rows\n\n\nRemove duplicates\n\nod_data <- unique(od_data)\n\nSneak peak of the current od_data\n\nknitr::kable(head(od_data,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\nORIGIN_BS\nDESTIN_BS\nTRIPS\nORIGIN_SZ\nLOC_DESC.x\nSUBZONE_C\nLOC_DESC.y\n\n\n\n\n01012\n01112\n276\nRCSZ10\nHOTEL GRAND PACIFIC\nRCSZ10\nOPP BUGIS STN EXIT C\n\n\n01012\n01113\n143\nRCSZ10\nHOTEL GRAND PACIFIC\nDTSZ01\nBUGIS STN EXIT B\n\n\n01012\n01121\n66\nRCSZ10\nHOTEL GRAND PACIFIC\nRCSZ04\nSTAMFORD PR SCH\n\n\n\n\n\nThe code chunk below will do the following:\n\nRenames the destination ‘SUBZONE_C’ to ‘DESTIN_SZ’.\nThere are missing subzone codes for some of the origin and destination bus stop because the bus stops location in July 2023 could be more outdated than August bus stop 2023. We will drop columns with missing values.\nGroup-by origin subzone and destination subzone to generate a new field ‘MORNING_PEAK’ that contains the summation of all trips from subzone A to subzone B.\n\n\nod_data <- od_data %>%\n  rename(DESTIN_SZ = SUBZONE_C) %>%\n  drop_na() %>%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %>%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\nTake a look at our final od_data df\n\nknitr::kable(head(od_data %>% \n                    arrange(desc(MORNING_PEAK)),\n                  10))\n\n\n\n\nORIGIN_SZ\nDESTIN_SZ\nMORNING_PEAK\n\n\n\n\nTMSZ02\nTMSZ02\n350755\n\n\nWDSZ03\nWDSZ03\n239791\n\n\nJWSZ08\nJWSZ09\n234343\n\n\nBDSZ04\nBDSZ04\n217917\n\n\nJWSZ09\nJWSZ09\n153055\n\n\nYSSZ03\nYSSZ01\n152800\n\n\nJWSZ04\nJWSZ04\n149110\n\n\nTMSZ03\nTMSZ02\n134196\n\n\nPRSZ05\nPRSZ03\n103148\n\n\nJWSZ03\nJWSZ04\n99666\n\n\n\n\n\nSave the output into an rds file format.\n\nwrite_rds(od_data, \"data/rds/od_data.rds\")\n\n\nod_data <- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#visualising-spatial-interaction",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#visualising-spatial-interaction",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.6 Visualising Spatial Interaction",
    "text": "16.6 Visualising Spatial Interaction\nIn this section, wewill learn how to prepare a desire line by using stplanr package.\n\n16.6.1 Removing intra-zonal flows\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows. It does so by removing the flows that originate and ends in the same subzone.\nRows reduced from 20,987 to 20,697.\n\nod_data1 <- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\n16.6.2 Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\nod_data1 is aspatial while mpsz is geospatial data.\nArguments\n\nflow\n\nA data frame representing origin-destination data. The first two columns of this data frame should correspond to the first column of the data in the zones. Thus in cents_sf(), the first column is geo_code. This corresponds to the first two columns of flow().\n\nzones\n\nA spatial object representing origins (and destinations if no separate destinations object is provided) of travel.\n\ndestinations\n\nA spatial object representing destinations of travel flows.\n\nzone_code\n\nName of the variable in zones containing the ids of the zone. By default this is the first column names in the zones.\n\n\nThe output flowLine is a sf LINESTRING object.\n\nflowLine <- od2line(flow=od_data1,\n                    zones= mpsz,\n                    zone_code= 'SUBZONE_C')\n\nflowLine\n\nSimple feature collection with 20697 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 5105.594 ymin: 25813.33 xmax: 49483.22 ymax: 49552.79\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   ORIGIN_SZ DESTIN_SZ MORNING_PEAK                       geometry\n1     AMSZ01    AMSZ02        11742 LINESTRING (29501.77 39419....\n2     AMSZ01    AMSZ03        14886 LINESTRING (29501.77 39419....\n3     AMSZ01    AMSZ04         3237 LINESTRING (29501.77 39419....\n4     AMSZ01    AMSZ05         9349 LINESTRING (29501.77 39419....\n5     AMSZ01    AMSZ06         2231 LINESTRING (29501.77 39419....\n6     AMSZ01    AMSZ07         1714 LINESTRING (29501.77 39419....\n7     AMSZ01    AMSZ08         2624 LINESTRING (29501.77 39419....\n8     AMSZ01    AMSZ09         2371 LINESTRING (29501.77 39419....\n9     AMSZ01    AMSZ10          183 LINESTRING (29501.77 39419....\n10    AMSZ01    AMSZ11          930 LINESTRING (29501.77 39419....\n\n\n\n\n16.6.3 Visualising the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\nArguments of tm_lines():\nlwd: line width. Either a numeric value or a data variable. In the latter case, the class of the highest values (see style) will get the line width defined by scale. If multiple values are specified, small multiples are drawn (see details).\nstyle: method to process the color scale when col is a numeric variable. Discrete gradient options are \"cat\", \"fixed\", \"sd\", \"equal\", \"pretty\", \"quantile\", \"kmeans\", \"hclust\", \"bclust\", \"fisher\", \"jenks\", \"dpih\", \"headtails\", and \"log10_pretty\". A numeric variable is processed as a categorical variable when using \"cat\", i.e. each unique value will correspond to a distinct category\nscale: line width multiplier number.\nn: preferred number of color scale classes. Only applicable when lwd is the name of a numeric variable.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRendering process takes about 1 min because of the transparency argument alpha.\n\n\nWhen the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.\n\ntmap_mode('view')\ntmap_options(check.and.fix = TRUE)\n\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %>%  \n  filter(MORNING_PEAK >= 5000) %>%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#viewing-the-subzone-spatial-file",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#viewing-the-subzone-spatial-file",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.8 Viewing the Subzone spatial file",
    "text": "16.8 Viewing the Subzone spatial file\n\nhead(mpsz, 10)\n\nSimple feature collection with 10 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 8012.578 ymin: 15748.72 xmax: 35287.9 ymax: 31092.38\nProjected CRS: SVY21 / Singapore TM\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26..."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#isolating-subzone_c-subzone_code-into-a-new-df",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#isolating-subzone_c-subzone_code-into-a-new-df",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.9 Isolating SUBZONE_C (subzone_code) into a new df",
    "text": "16.9 Isolating SUBZONE_C (subzone_code) into a new df\nSort mpsz based on values of SUBZONE_C column in ascending order.\n\nmpsz <- mpsz[order(mpsz$SUBZONE_C),]\nhead(mpsz, 10)\n\nSimple feature collection with 10 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 26154.57 ymin: 37511.2 xmax: 31072.47 ymax: 41804.65\nProjected CRS: SVY21 / Singapore TM\n                 SUBZONE_N SUBZONE_C PLN_AREA_N PLN_AREA_C          REGION_N\n171 ANG MO KIO TOWN CENTRE    AMSZ01 ANG MO KIO         AM NORTH-EAST REGION\n170              CHENG SAN    AMSZ02 ANG MO KIO         AM NORTH-EAST REGION\n163             CHONG BOON    AMSZ03 ANG MO KIO         AM NORTH-EAST REGION\n330             TOWNSVILLE    AMSZ04 ANG MO KIO         AM NORTH-EAST REGION\n329             SHANGRI-LA    AMSZ05 ANG MO KIO         AM NORTH-EAST REGION\n172            KEBUN BAHRU    AMSZ06 ANG MO KIO         AM NORTH-EAST REGION\n233        SEMBAWANG HILLS    AMSZ07 ANG MO KIO         AM NORTH-EAST REGION\n254                 TAGORE    AMSZ08 ANG MO KIO         AM NORTH-EAST REGION\n242      YIO CHU KANG WEST    AMSZ09 ANG MO KIO         AM NORTH-EAST REGION\n252           YIO CHU KANG    AMSZ10 ANG MO KIO         AM NORTH-EAST REGION\n    REGION_C                       geometry\n171      NER MULTIPOLYGON (((29692.8 389...\n170      NER MULTIPOLYGON (((30384.33 39...\n163      NER MULTIPOLYGON (((30676.17 39...\n330      NER MULTIPOLYGON (((29649.88 38...\n329      NER MULTIPOLYGON (((28228.2 392...\n172      NER MULTIPOLYGON (((28491.21 39...\n233      NER MULTIPOLYGON (((27744.03 38...\n254      NER MULTIPOLYGON (((27193.46 41...\n242      NER MULTIPOLYGON (((29269.91 39...\n252      NER MULTIPOLYGON (((29598.36 39..."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#computing-distance-matrix",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#computing-distance-matrix",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.10 Computing Distance Matrix",
    "text": "16.10 Computing Distance Matrix\nThe are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shows that computing distance matrix by using sf function took relatively longer time that sp method. In view of this, sp method is used in the code chunks below.\n\n16.10.1 Converting from sf data.table to SpatialPolygonDataFrame\nConvert mpsz from simple feature collection to SpatialPolygonDataFrame.\n\nmpsz_sp <- as(mpsz, \"Spatial\")\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\nExploration: How to access a SpatialPolygonDataFrame object of the sp package.\n\nmpsz_sp['SUBZONE_N'][[1]]\nmpsz_sp@data  # class dataframe\nmpsz_sp@polygons # class: list\nmpsz_sp@polygons[[1]]  # access the first polygon / subzone\nmpsz_sp@polygons[[1]]@Polygons # access the slot in the polygon object that contains information about individual Polygons within the overall geometry\nmpsz_sp@polygons[[1]]@Polygons[[1]] # same as above, enter another layer\nmpsz_sp@polygons[[1]]@Polygons[[1]]@coords #get the coordinates of the first polygon / subzone\nmpsz_sp@polygons[[332]]@Polygons[[1]]@coords #total of 333 subzones\n\n\n\n16.10.2 Computing the distance matrix\nspDists(x, y = x, longlat = FALSE, segments = FALSE, diagonal = FALSE)\nspDists returns a full matrix of distances in the metric of the points if longlat=FALSE, or in kilometers if longlat=TRUE; it uses spDistsN1 in case points are two-dimensional. In case of spDists(x,x), it will compute all n x n distances, not the sufficient n x (n-1).\nArguments\nx: A matrix of n-D points with row denoting points, first column x/longitude, second column y/latitude, or a Spatial object that has a coordinates method\ny: A matrix of n-D points with row denoting points, first column x/longitude, second column y/latitude, or a Spatial object that has a coordinates method\nlonglat: logical; if FALSE (default), Euclidean distance, if TRUE Great Circle (WGS84 ellipsoid) distance; if x is a Spatial object, longlat should not be specified but will be derived from is.projected(x)\nsegments: logical; if TRUE, y must be missing; the vector of distances between consecutive points in x is returned.\ndiagonal: logical; if TRUE, y must be given and have the same number of points as x; the vector with distances between points with identical index is returned.\nThe diagonals of the ouput (332 by 332) are all 0. Distance with itself. The unit of distance is if ‘m’ (euclidean?) and km if WSG84?\n\nlibrary(sp)\ndist <- spDists(mpsz_sp)\nclass(dist) \n\n[1] \"matrix\" \"array\" \n\n\n\ndist[1:5,1:5]\n\n          [,1]      [,2]      [,3]      [,4]     [,5]\n[1,]    0.0000  810.4491 1360.9294  840.4432 1076.792\n[2,]  810.4491    0.0000  950.2937 1026.5876 1824.476\n[3,] 1360.9294  950.2937    0.0000  808.0902 1964.059\n[4,]  840.4432 1026.5876  808.0902    0.0000 1158.484\n[5,] 1076.7916 1824.4762 1964.0592 1158.4844    0.000\n\n\nWe can use the code to check for the default arguments of a function quickly.\n\nformals(spDists)\n\n\n\n16.10.3 Get the sorted column and row names of out dist matrix\nmpsz was previoulsy sorted by ‘SUBZONE_C’ in ascending order. The code below will extract only the column of ‘SUBZONE_C’.\n\nsz_names <- mpsz$SUBZONE_C\nsz_names[1:10] \n\n [1] \"AMSZ01\" \"AMSZ02\" \"AMSZ03\" \"AMSZ04\" \"AMSZ05\" \"AMSZ06\" \"AMSZ07\" \"AMSZ08\"\n [9] \"AMSZ09\" \"AMSZ10\"\n\n\n\n\n16.10.4 Attaching SUBZONE_C to row and column for distance matrix matching ahead\nWe would like to set the column names and row names for our distance matrix .\n\ncolnames(dist): This is used to access or set the column names of the object. Note that colnames() is applicable to matrices and arrays.\nrownames(dist): This is used to access or set the row names of the object. Note that rownames() is applicable to matrices and arrays.\npaste0(sz_names): This part creates a character vector by concatenating elements of the sz_names vector without any separator. The resulting vector will be used as column names.\n\n\ncolnames(dist) <- paste0(sz_names)\nrownames(dist) <- paste0(sz_names)\ndist[1:5,1:5]\n\n          AMSZ01    AMSZ02    AMSZ03    AMSZ04   AMSZ05\nAMSZ01    0.0000  810.4491 1360.9294  840.4432 1076.792\nAMSZ02  810.4491    0.0000  950.2937 1026.5876 1824.476\nAMSZ03 1360.9294  950.2937    0.0000  808.0902 1964.059\nAMSZ04  840.4432 1026.5876  808.0902    0.0000 1158.484\nAMSZ05 1076.7916 1824.4762 1964.0592 1158.4844    0.000\n\n\n\n\n16.10.5 Pivoting distance value by SUBZONE_C\nWe will use the melt() function of the reshape2 package to convert wide-format data to long-format data. This function will convert wide-format data to a data frame with columns for each combination of row and column indices and their corresponding values.\nTo do the opposite, used cast().\n\nWideLong\n\n\n\nmatrix(1:6, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\n\nreshape2::melt(matrix(1:6, nrow = 2, ncol = 3)) %>% knitr::kable()\n\n\n\n\nVar1\nVar2\nvalue\n\n\n\n\n1\n1\n1\n\n\n2\n1\n2\n\n\n1\n2\n3\n\n\n2\n2\n4\n\n\n1\n3\n5\n\n\n2\n3\n6\n\n\n\n\n\n\n\n\nThree new columns generated, (1) ‘var1’, (2) ‘var2’ and (3) ‘value’ containing the distance for the corresponding var1-var2 pair; thus rename to ‘dist’.\nThere are 110,224 rows in distPair due to 332P2 + 332 = 332*331 + 332. Number of possible permutations with replacement.\n\ndistPair <- reshape2::melt(dist) %>%\n  rename(dist = value)\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  AMSZ01 AMSZ01    0.0000\n2  AMSZ02 AMSZ01  810.4491\n3  AMSZ03 AMSZ01 1360.9294\n4  AMSZ04 AMSZ01  840.4432\n5  AMSZ05 AMSZ01 1076.7916\n6  AMSZ06 AMSZ01  805.2979\n7  AMSZ07 AMSZ01 1798.7526\n8  AMSZ08 AMSZ01 2576.0199\n9  AMSZ09 AMSZ01 1204.2846\n10 AMSZ10 AMSZ01 1417.8035\n\n\n\n\n16.10.6 Updating intra-zonal distances\nThe row contain subzone A to subzone A (distance = 0) can be removed by filtering.\n\ndistPair %>%\n  filter(dist > 0) %>%\n  summary()\n\n      Var1             Var2             dist        \n AMSZ01 :   331   AMSZ01 :   331   Min.   :  173.8  \n AMSZ02 :   331   AMSZ02 :   331   1st Qu.: 7149.5  \n AMSZ03 :   331   AMSZ03 :   331   Median :11890.0  \n AMSZ04 :   331   AMSZ04 :   331   Mean   :12229.4  \n AMSZ05 :   331   AMSZ05 :   331   3rd Qu.:16401.7  \n AMSZ06 :   331   AMSZ06 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nThe code chunk below adds a constant distance value of 50m into the intra-zones commute. The diagonals of dist matrix (if still a matrix) would have been 50m.\n\ndistPair$dist <- ifelse(distPair$dist==0,\n                        50,\n                        distPair$dist)\n\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  AMSZ01 AMSZ01   50.0000\n2  AMSZ02 AMSZ01  810.4491\n3  AMSZ03 AMSZ01 1360.9294\n4  AMSZ04 AMSZ01  840.4432\n5  AMSZ05 AMSZ01 1076.7916\n6  AMSZ06 AMSZ01  805.2979\n7  AMSZ07 AMSZ01 1798.7526\n8  AMSZ08 AMSZ01 2576.0199\n9  AMSZ09 AMSZ01 1204.2846\n10 AMSZ10 AMSZ01 1417.8035\n\n\nLastly, code chunk is used to save the data frame for future use.\n\nwrite_rds(distPair, \"data/distPair.rds\")"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#preparing-flow-data",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#preparing-flow-data",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.11 Preparing flow data",
    "text": "16.11 Preparing flow data\nThe code chunk below is used to prepare the flow_data. od_data contains intra-zonal trips (unlike od_trip1 ). There are 310 unique origin subzone values and 311 unique destin subzone values.\n\nflow_data <- od_data\n\n\n16.11.1 Separating intra-flow from passenger volume df\nTwo new fields called ‘FlowNoIntra’ and ‘offset’ are created.\ncheck code\n\nflow_data$FlowNoIntra <- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$MORNING_PEAK)\nflow_data$offset <- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\nPrint flow_data\n\nhead(flow_data,3) %>% knitr::kable()\n\n\n\n\nORIGIN_SZ\nDESTIN_SZ\nMORNING_PEAK\nFlowNoIntra\noffset\n\n\n\n\nAMSZ01\nAMSZ01\n2575\n0\n1e-06\n\n\nAMSZ01\nAMSZ02\n11742\n11742\n1e+00\n\n\nAMSZ01\nAMSZ03\n14886\n14886\n1e+00\n\n\n\n\nglimpse(flow_data)\n\nRows: 20,987\nColumns: 5\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ    <chr> \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01…\n$ DESTIN_SZ    <chr> \"AMSZ01\", \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06…\n$ MORNING_PEAK <dbl> 2575, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 1…\n$ FlowNoIntra  <dbl> 0, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 183,…\n$ offset       <dbl> 1e-06, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1…\n\n\nThe ‘ORIGIN_SZ’ and ‘DESTIN_SZ’ fields are in character format. Let us convert to factor format\n\nflow_data$ORIGIN_SZ <- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ <- as.factor(flow_data$DESTIN_SZ)\n\nhead(flow_data,5)\n\n# A tibble: 5 × 5\n# Groups:   ORIGIN_SZ [1]\n  ORIGIN_SZ DESTIN_SZ MORNING_PEAK FlowNoIntra   offset\n  <fct>     <fct>            <dbl>       <dbl>    <dbl>\n1 AMSZ01    AMSZ01            2575           0 0.000001\n2 AMSZ01    AMSZ02           11742       11742 1       \n3 AMSZ01    AMSZ03           14886       14886 1       \n4 AMSZ01    AMSZ04            3237        3237 1       \n5 AMSZ01    AMSZ05            9349        9349 1       \n\n\n\n\n16.11.2 Combining passenger volume data with distance value\ndistPair is a df containing distances for all corresponding subzone pairs (including self, default to 50m). ‘var1’, ‘var2’, ‘dist’\nflow_data is a df containing ‘origin_sz’, ‘destin_sb’ and ‘morning_peak’\nWe will now perform a left join with two sets join keys.\nThe output contains distance and total morning peak trips for each possible pairs of subzones (self included).\nBefore left join:\nflow_data has 20,987 rows.\ndistPair has 110,224 rows (is the all possible pairs out of 332 subzones, order matters and with replacement.)\nAfter join:\nflow_data1 has 20,987 rows.\n\nflow_data1 <- left_join(flow_data, distPair,\n                        by = c('ORIGIN_SZ'= 'Var1',\n                               'DESTIN_SZ'= 'Var2'))\n\nglimpse(flow_data1)\n\nRows: 20,987\nColumns: 6\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ    <fct> AMSZ01, AMSZ01, AMSZ01, AMSZ01, AMSZ01, AMSZ01, AMSZ01, A…\n$ DESTIN_SZ    <fct> AMSZ01, AMSZ02, AMSZ03, AMSZ04, AMSZ05, AMSZ06, AMSZ07, A…\n$ MORNING_PEAK <dbl> 2575, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 1…\n$ FlowNoIntra  <dbl> 0, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 183,…\n$ offset       <dbl> 1e-06, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1…\n$ dist         <dbl> 50.0000, 810.4491, 1360.9294, 840.4432, 1076.7916, 805.29…\n\n\nPrint out\n\nhead(flow_data1) %>% knitr::kable()\n\n\n\n\nORIGIN_SZ\nDESTIN_SZ\nMORNING_PEAK\nFlowNoIntra\noffset\ndist\n\n\n\n\nAMSZ01\nAMSZ01\n2575\n0\n1e-06\n50.0000\n\n\nAMSZ01\nAMSZ02\n11742\n11742\n1e+00\n810.4491\n\n\nAMSZ01\nAMSZ03\n14886\n14886\n1e+00\n1360.9294\n\n\nAMSZ01\nAMSZ04\n3237\n3237\n1e+00\n840.4432\n\n\nAMSZ01\nAMSZ05\n9349\n9349\n1e+00\n1076.7916\n\n\nAMSZ01\nAMSZ06\n2231\n2231\n1e+00\n805.2979"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#preparing-origin-and-destination-attributes",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#preparing-origin-and-destination-attributes",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "16.12 Preparing Origin and Destination Attributes",
    "text": "16.12 Preparing Origin and Destination Attributes\n\n16.12.1 Importing population data\nThe dataset used here is the Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format .(i.e. respopagesextod2011to2020.csv). This is an aspatial data file. It can be downloaded at Department of Statistics, Singapore, the specific link can be found here. Although it does not contain any coordinates values, but it’s ‘PA’ and ‘SZ’ fields can be used as unique identifiers to geocode to ‘PLAN_AREA_N’ and ‘SUBZONE_N’ of the MP14_SUBZONE_WEB_PL shapefile respectively.\n\npop <- read_csv(\"data/aspatial/pop.csv\")\nhead(pop)\n\n# A tibble: 6 × 5\n  PA         SZ                     AGE7_12 AGE13_24 AGE25_64\n  <chr>      <chr>                    <dbl>    <dbl>    <dbl>\n1 ANG MO KIO ANG MO KIO TOWN CENTRE     310      710     2780\n2 ANG MO KIO CHENG SAN                 1140     2770    15700\n3 ANG MO KIO CHONG BOON                1010     2650    14240\n4 ANG MO KIO KEBUN BAHRU               1050     2390    12460\n5 ANG MO KIO SEMBAWANG HILLS            420     1120     3620\n6 ANG MO KIO SHANGRI-LA                 810     1920     9650\n\n\n\n\n16.12.2 Geospatial data wrangling\nLet us append the population data for different age group range to the mpsz df.\npop has 984,656 rows\nmpsz has 332 rows\nAfter join: 984,656 rows\nColumn selected are ‘PA’, ‘SZ’, ‘AGE7-12’, ‘AGE13-24’, ‘AGE25_64’ from pop df and ‘SUBZONE_C’ from mpsz df.\n\npop<- pop %>%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %>%\n  select(1:6) %>%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\n\n\n16.7.3 Preparing origin attribute\n\n\n\n\n#| eval: false\n#| echo: false\n#| fig-width: 14\n#| fig-asp: 0.68\n#| code-fold: True"
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3_2.html#summaries",
    "href": "Hands-on_Ex3/Hands-on_Ex3_2.html#summaries",
    "title": "Hands-on Exercise 3.1: Processing and Visualising Flow Data",
    "section": "Summaries",
    "text": "Summaries\nOD matrix is often incomplete. Imagine trying to complete the OD matrix, it would involve us doing spatial interaction or OD surveys to find the missing values. There are 332 subzones in Singapore, and each survey is expensive,. In addition, OD matrix is constantly changing as flow patterns changes. We are trying to predict flows between origins and destinations. Flow could be thought of a function of (1) attribute of origin (propulsiveness) (2) attribute of destination (attractiveness) and (3) cost friction (like distance or transport cost or public transport stops). Assumption is that the benefits must outweigh the cost in order for flow to happen.\nGravity model takes into consideration the interaction between all origin and destination locations.\nPotential model takes in consideration the interaction between a location and all other location pairs. (Good for measuring accessibility)\nRetail model is commonly used by franchise like KFC / Pizza Hut to determine their area/region of service (aka delivery zones) for each outlet.\nThere are 4 variations in the Gravity model:\n\nUnconstrained: only the overall outflow is fixed and total outflow from origins = total inflow to destinations\nOrigin constrained: outflow by origin is fixed.\nDestination constrained: inflow by destination is fixed.\nDoubly constrained: outflow by origin and inflow by destination is fixed.\n\nTo calculate flow from each origin to each destination, we need parameters like k, alpha, lambda and beta. The beta for distance is usually negative because we assume that there is an inverse relationship between interaction and distance, like Newtonian physics and laws of gravity."
  },
  {
    "objectID": "In-class_Ex1/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "href": "In-class_Ex1/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "title": "NYX Geospatial App",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "The Task\nIn this in-class exercise, we are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops csv aspatial data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1",
    "section": "Getting started",
    "text": "Getting started\nThree R packages will be used in this in-class exercise, they are:\n\ntidyverse for non-spatial data handling,\nsf for geospatial data handling,\ntmap for thematic mapping, and\nknitr for creating html table.\n\nThe code chunk below loads the following packages:\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non spatial data handling\n\n\npacman::p_load(sf, tmap, tidyverse, knitr, h3jsr, DT, skimr)"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "href": "In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "title": "In-class Exercise 1",
    "section": "Importing the OD data",
    "text": "Importing the OD data\nFirstly we will import the Passenger volume by Origin Destination Bus Stops dataset downloaded from the LTA Datamall by using the read_csv() of the readr package (to read text data).\n\nodbus_aug <- read_csv(\"data\\\\aspatial\\\\origin_destination_bus_202308.csv\")\n\nCheck the datafields\n\nglimpse(odbus_aug)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          <chr> \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            <chr> \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       <dbl> 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             <chr> \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      <chr> \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE <chr> \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         <dbl> 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nProcessing the aspatial OD data\nThe ‘ORIGIN_PT_CODE’ and ‘DESTINATION_PT_CODE’ field is in character field. We will convert it to factor data type.\n\nodbus_aug$ORIGIN_PT_CODE <- as.factor(odbus_aug$ORIGIN_PT_CODE)\nodbus_aug$DESTINATION_PT_CODE <- as.factor(odbus_aug$DESTINATION_PT_CODE)\n\nThe code chunk below does the following:\n\nextract commuting flows on weekday and between 7 and 10 o’clock time intervals.\nGroup data by ‘ORIGIN_PT_CODE’ and create a new field call ‘TRIPS’ by summarising the ‘TOTAL_TRIPS’ column.\n\n\n\n\n\norigin7_9 <- odbus_aug %>% \n  filter(DAY_TYPE == 'WEEKDAY') %>% \n  filter(TIME_PER_HOUR >= 7 &\n           TIME_PER_HOUR <= 9) %>% \n  group_by(ORIGIN_PT_CODE) %>% \n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\norigin7_9 %>% \n  arrange(desc(TRIPS)) %>%\n  head(10) %>% \n  kable()\n\n\n\n\nORIGIN_PT_CODE\nTRIPS\n\n\n\n\n22009\n295128\n\n\n46009\n232902\n\n\n75009\n134781\n\n\n52009\n121422\n\n\n55509\n113434\n\n\n24009\n108437\n\n\n65199\n100791\n\n\n59009\n92333\n\n\n84009\n88092\n\n\n54261\n86565\n\n\n\n\n\nSave the output in rds format for future use.\n\nwrite_rds(origin7_9, \"data/rds/origin7_9.rds\")\n\nThe code chunk below will be used to import the save origin7_9.rds into R environment.\n\norigin7_9 <- read_rds(\"data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#importing-the-geospatial-data",
    "href": "In-class_Ex1/In-class_Ex1.html#importing-the-geospatial-data",
    "title": "In-class Exercise 1",
    "section": "Importing the geospatial data",
    "text": "Importing the geospatial data\nTwo geospatial datasets will be used in this exercise.\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\n\nLoad the geospatial bustop shapefiles and immediately transform to Singapore projection system (EPSG 3414)\n\nbusstop <- st_read(dsn=\"data\\\\geospatial\\\\BusStopLocation\\\\BusStopLocation_Jul2023\", layer = \"BusStop\") %>% \n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\In-class_Ex1\\data\\geospatial\\BusStopLocation\\BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\nbusstop\n\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   BUS_STOP_N BUS_ROOF_N             LOC_DESC                  geometry\n1       22069        B06   OPP CEVA LOGISTICS POINT (13576.31 32883.65)\n2       32071        B23         AFT TRACK 13 POINT (13228.59 44206.38)\n3       44331        B01              BLK 239  POINT (21045.1 40242.08)\n4       96081        B05 GRACE INDEPENDENT CH POINT (41603.76 35413.11)\n5       11561        B05              BLK 166 POINT (24568.74 30391.85)\n6       66191        B03         AFT CORFE PL POINT (30951.58 38079.61)\n7       23389       B02A              PEC LTD   POINT (12476.9 32211.6)\n8       54411        B02              BLK 527 POINT (30329.45 39373.92)\n9       28531        B09              BLK 536 POINT (14993.31 36905.61)\n10      96139        B01              BLK 148  POINT (41642.81 36513.9)\n\n\nThere are 5161 spatial units/points in busstop and three attributes, namely “BUS_STOP_N” “BUS_ROOF_N” “LOC_DESC”.\n\nplot(busstop['BUS_STOP_N'])\n\n\n\n\n\n\nLoad the geospatial MPSZ_19 shapefiles (basemap) from gov.sg (usually in WSG84 format) and transform to EPSG 3414\n\nmpsz <- st_read(dsn=\"data\\\\geospatial\\\\MPSZ-2019\", layer='MPSZ-2019')  %>% \n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\In-class_Ex1\\data\\geospatial\\MPSZ-2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n#colnames(mpsz)\n\nThere are 332 spatial units/polygons in mpsz and 6 attributes, namely “SUBZONE_N” “SUBZONE_C” “PLN_AREA_N” “PLN_AREA_C” “REGION_N” “REGION_C”.\n\nplot(mpsz['SUBZONE_C'])"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "href": "In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nCombining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\nbusstop_mpsz <- st_intersection(busstop, mpsz) %>% \n  select(BUS_STOP_N, SUBZONE_C)\n\nbusstop_mpsz\n\nSimple feature collection with 5156 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 49903.39\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     BUS_STOP_N SUBZONE_C                  geometry\n1346      13099    RVSZ05  POINT (28696.2 30905.64)\n5067      13089    RVSZ05 POINT (28303.29 30901.45)\n839       06151    SRSZ01 POINT (28444.68 30126.05)\n965       13211    SRSZ01  POINT (28865.67 30537.6)\n974       13139    SRSZ01 POINT (27859.98 30819.39)\n1197      13109    SRSZ01 POINT (29044.85 30655.31)\n1338      13119    SRSZ01 POINT (27947.47 30664.54)\n2309      06169    SRSZ01 POINT (28751.53 30196.23)\n2566      06159    SRSZ01 POINT (28404.91 30166.55)\n3028      04321    SRSZ01 POINT (29084.63 30510.34)\n\n\n\nst_intersects(): count points in polygon\nst_intersection(): find polygon areas overlap\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overlap and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary. Points reduced from 5161 to 5156 after combine.\n\n\n\nDrop the geometry because busstop_mpsz is a POINT sf object, there is no polygon geometry. Furthermore, we have to process the attribute data. To get back the planning sub-zone POLYGON geometry data, we can always left_join() mpsz df with our processed busstop_mpsz df again later.\n\nbusstop_mpsz <- busstop_mpsz %>% \n  st_drop_geometry()\n\ndatatable(busstop_mpsz, class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\nSave the output into rds format.\n\nwrite_rds(busstop_mpsz, 'data/rds/busstop_mpsz.csv')\n\n\n\nCombining origin7_9 and busstop_mpsz dataframes\nThe ‘BUS_STOP_N’ and ‘ORIGIN_PT_CODE’ columns are the join columns.\n\nCombinebusstop_mpszorigin7_9\n\n\norigin7_9 has 5,019 rows\nbusstop_mpsz has 5,156 rows\nAfter left_join(), origin_data has 5,031 rows. There are 55 missing values in its ’ORIGIN_SZ’ field and this could be due to missing bus stop code in the busstop_mpsz df.\n\norigin_data <- left_join(origin7_9, busstop_mpsz,\n                         by = c('ORIGIN_PT_CODE' = 'BUS_STOP_N')) %>%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C)\n\ndatatable(origin_data)\n\n\n\n\n\n\n\n\nThere are 5,156 rows.\n\n#kable(head(busstop_mpsz,3))\ndatatable(busstop_mpsz, class = 'cell-border stripe', options = list(pageLength = 3))\n\n\n\n\n\n\n\n\nThere are 5,015 rows.\n\n#kable(head(origin7_9,3))\ndatatable(origin7_9, class = 'cell-border stripe', options = list(pageLength = 3))\n\n\n\n\n\n\n\n\n\nBefore proceeding further, check for duplicates in the dataframe.\n\ngroup by all columns and check for duplicate rows using filter(n()>1)\n\nduplicate contains all duplicate rows , including original row. Thus there are 13 duplicated rows. The reason for having 13 duplicated rows in origin_data df is because there are 13 duplicated rows in busstop_mpsz df. We should have done the check prior to left join.\n\nduplicate <- origin_data %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\n\nduplicate\n\n# A tibble: 26 × 3\n   ORIGIN_BS TRIPS ORIGIN_SZ\n   <chr>     <dbl> <chr>    \n 1 11009     13826 QTSZ01   \n 2 11009     13826 QTSZ01   \n 3 22501      9743 JWSZ09   \n 4 22501      9743 JWSZ09   \n 5 43709      1118 BKSZ07   \n 6 43709      1118 BKSZ07   \n 7 47201     23998 WDSZ07   \n 8 47201     23998 WDSZ07   \n 9 51071      6218 CCSZ01   \n10 51071      6218 CCSZ01   \n# ℹ 16 more rows\n\n#duplicated_rows <- origin_data[duplicated(origin_data), ]  #<< alternative code\n\nTo retain only the unique records, use the code chunk below\n\norigin_data <- unique(origin_data)\n\nDouble check there are indeed no more duplicates in origin_data df.\n\norigin_data[duplicated(origin_data), ]\n\n# A tibble: 0 × 3\n# ℹ 3 variables: ORIGIN_BS <chr>, TRIPS <dbl>, ORIGIN_SZ <chr>\n\n\nUse the skim() from the skimr library\n\nskim(origin_data)\n\n\nData summary\n\n\nName\norigin_data\n\n\nNumber of rows\n5018\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nORIGIN_BS\n0\n1.00\n5\n5\n0\n5015\n0\n\n\nORIGIN_SZ\n55\n0.99\n6\n6\n0\n311\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTRIPS\n0\n1\n4209.89\n9285.78\n1\n385\n1765\n4987.25\n295128\n▇▁▁▁▁\n\n\n\n\n\nSum the TRIPS by planning sub-zone level\nCurrently, there are many rows per planning sub-zone. Calculate the aggregate sum of the ‘TRIPS’ field by the ‘ORIGIN_SZ’ in the origin_data df since we are plotting passenger trips originating at sub-zone level.\n\norigin_data_sz <- origin_data %>% \n  group_by(ORIGIN_SZ) %>% \n  summarise(TTRIPS = sum(TRIPS)) %>% \n  ungroup()\n\norigin_data_sz\n\n# A tibble: 312 × 2\n   ORIGIN_SZ TTRIPS\n   <chr>      <dbl>\n 1 AMSZ01     95303\n 2 AMSZ02    184734\n 3 AMSZ03    159714\n 4 AMSZ04     90272\n 5 AMSZ05     55590\n 6 AMSZ06     80269\n 7 AMSZ07     18920\n 8 AMSZ08     23382\n 9 AMSZ09     71036\n10 AMSZ10    136516\n# ℹ 302 more rows"
  },
  {
    "objectID": "In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "href": "In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "title": "In-class Exercise 1",
    "section": "Choropleth Visualisation",
    "text": "Choropleth Visualisation\nTask: Prepare a choropleth map showing the distribution of passenger trips at planning sub-zone level.\nThe origin_data_sz df is an aspatial df without any geometry, we could perform a left join with mpsz and origin_datato retain the sf property.\n\nmpsz_origtrip_sz <- left_join(mpsz, origin_data_sz,\n                           by = c('SUBZONE_C' ='ORIGIN_SZ'))\n\nmpsz_origtrip_sz\n\nSimple feature collection with 332 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C TTRIPS                       geometry\n1        CR     NA MULTIPOLYGON (((33222.98 29...\n2        CR   6976 MULTIPOLYGON (((28481.45 30...\n3        CR  23629 MULTIPOLYGON (((28087.34 30...\n4        WR     NA MULTIPOLYGON (((14557.7 304...\n5        CR   4663 MULTIPOLYGON (((29542.53 31...\n6        CR     NA MULTIPOLYGON (((35279.55 30...\n7        WR     NA MULTIPOLYGON (((15772.59 21...\n8        WR     NA MULTIPOLYGON (((19843.41 21...\n9        CR     NA MULTIPOLYGON (((30870.53 22...\n10       CR     21 MULTIPOLYGON (((26879.04 26...\n\n\nExamine any missing values.\n\nskim(mpsz_origtrip_sz)\n\n\nData summary\n\n\nName\nmpsz_origtrip_sz\n\n\nNumber of rows\n332\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSUBZONE_N\n0\n1\n4\n29\n0\n332\n0\n\n\nSUBZONE_C\n0\n1\n6\n6\n0\n332\n0\n\n\nPLN_AREA_N\n0\n1\n4\n23\n0\n55\n0\n\n\nPLN_AREA_C\n0\n1\n2\n2\n0\n55\n0\n\n\nREGION_N\n0\n1\n11\n17\n0\n5\n0\n\n\nREGION_C\n0\n1\n2\n3\n0\n5\n0\n\n\ngeometry\n0\n1\n658\n77486\n0\n332\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTTRIPS\n21\n0.94\n66888.17\n84492.72\n15\n11471\n36064\n91292.5\n681719\n▇▁▁▁▁\n\n\n\n\n\nPlot using aggregated sum of TRIPS by sub-zones\n\ntmap_mode('plot')\n\ntm_shape(mpsz_origtrip_sz)+\n  tm_fill(\"TTRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          #legend.hist = TRUE, \n          #legend.is.portrait = TRUE,\n          #legend.hist.z = 0.3,\n          title = \"Passengers Trip\") +\n  tm_layout(main.title = \"Passenger trips generated by planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.height = 0.6, \n            legend.width = 0.5,\n            #legend.outside = TRUE,\n            #legend.text.size= 0.6,\n            #inner.margins = c(0.01, 0.01, 0, .15),\n            #legend.position = c(\"right\", \"top\"),\n            #bg.color = \"black\",\n            #main.title.color = 'white',\n            #legend.title.color = 'white',\n            #legend.text.color= 'white',\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nPlot in ‘view’ mode for interactivity. We will be able to zoom and hover and click on each subzone.\n\ntmap_mode('view')\ntmap_options(check.and.fix = TRUE)\n\ntm_shape(mpsz_origtrip_sz)+\n  tm_fill(\"TTRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          #legend.hist = TRUE, \n          #legend.is.portrait = TRUE,\n          #legend.hist.z = 0.3,\n          title = \"Passengers Trip\") +\n  tm_layout(main.title = \"Passenger trips generated by planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.height = 0.6, \n            legend.width = 0.5,\n            #legend.outside = TRUE,\n            #legend.text.size= 0.6,\n            #inner.margins = c(0.01, 0.01, 0, .15),\n            #legend.position = c(\"right\", \"top\"),\n            #bg.color = \"black\",\n            #main.title.color = 'white',\n            #legend.title.color = 'white',\n            #legend.text.color= 'white',\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Ex2/In-class_Ex2.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "Installing and loading the required R packages.\ndplyr: reshape data, joins, pivot for instance..\ntidyr: transform data\nknitr: generate html table\nToday, sfdep library will replace the spdep library because it is more recent (<2 years ago). Allows to mutate using spatial functions. spdep does not allow for mutate for instance.\n\npacman::p_load(sf, tmap, sfdep, tidyverse, knitr, plotly,Kendall, DT)\n\n\n\n\nHunan, geospatial dataset in shapefile format\nHunan_2012, an attribute dataset in csv format\n\n\n\n\n\nIn the code chunk below , import geospatial data using st_read() from sf library. The output has projection of WSG84 and 88 observations. Class of ‘sf’ and ‘tibble df’, and in tibble df contains a geometry list. In ‘sf’, each row/observation represents a geographical region/area/unit.\n\n hunan<- st_read(dsn=\"data/geospatial\", layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\In-class_Ex2\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n class(hunan)\n\n[1] \"sf\"         \"data.frame\"\n\n\nIn the code chunk below , import aspatial data\n\nhunan_2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nCombine both files together using left_join hunan (geo) with hunan_2012 (aspatial) to retain the sf class. If the unique identifier is not specified, R will use identical columns, in this case ‘County’ columns in both objects.\n\nhunan_GDPPC <- left_join(hunan, hunan_2012,\n                   by = c('County' = 'County')) %>% \n  select(1:4, 7,15)\n\nhunan\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3 Shape_Leng Shape_Area    County\n1   Changde 21098   Anxiang      County   1.869074 0.10056190   Anxiang\n2   Changde 21100   Hanshou      County   2.360691 0.19978745   Hanshou\n3   Changde 21101    Jinshi County City   1.425620 0.05302413    Jinshi\n4   Changde 21102        Li      County   3.474325 0.18908121        Li\n5   Changde 21103     Linli      County   2.289506 0.11450357     Linli\n6   Changde 21104    Shimen      County   4.171918 0.37194707    Shimen\n7  Changsha 21109   Liuyang County City   4.060579 0.46016789   Liuyang\n8  Changsha 21110 Ningxiang      County   3.323754 0.26614198 Ningxiang\n9  Changsha 21111 Wangcheng      County   2.292093 0.13049161 Wangcheng\n10 Chenzhou 21112     Anren      County   2.240739 0.13343936     Anren\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\nPlot a choropleth map\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\nWe can see the neighbours list in the first column.\n\nnb_queen <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\nkable(head(nb_queen,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnb\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nCounty\nGDPPC\ngeometry\n\n\n\n\n2, 3, 4, 57, 85\nChangde\n21098\nAnxiang\nCounty\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n1, 57, 58, 78, 85\nChangde\n21100\nHanshou\nCounty\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\nChangde\n21101\nJinshi\nCounty City\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n\n\n\nIdentifying higher order neighbors\nTo identify higher order contiguity nb, we can use st_nb_lag_cumul() should be used as shown in the code chunk below.\n\nnb2_queen <-  hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\nkable(head(nb2_queen,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnb\nnb2\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nCounty\nGDPPC\ngeometry\n\n\n\n\n2, 3, 4, 57, 85\n2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\nChangde\n21098\nAnxiang\nCounty\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n1, 57, 58, 78, 85\n1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\nChangde\n21100\nHanshou\nCounty\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\n1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\nChangde\n21101\nJinshi\nCounty City\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n\n\n\n\n\n\nDeriving Contiguity spatial weights using sfdep (wrapper of spdep: allows for mutate using spatial functions) instead of spdep library.\nIn the code chunk below st_contiguity() is used to derive a contiguity neighbour list by using Queen’s method.\nUse two functions from spdep library, use less objects.\nNote: ‘nb’ and ‘wt’ fields contain lists.\n\nwm_q <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry), # calculate by queen default\n         wt = st_weights(nb,           # calculate row-stand spatial weight matrix\n                         style='W'),\n         .before =1)   # put these two columns at front of wm_q sf df\n\nclass(wm_q)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\n\n\nThere are three popularly used distance-based spatial weights, they are:\n\nfixed distance weights,\nadaptive distance weights, and\ninverse distance weights (IDW).\n\n\n\nTo determine the upper limit for distance band:\n\ngeo <- sf::st_geometry(hunan_GDPPC)\nnb <- st_knn(geo, k=1, longlat = TRUE)\n\ndists <- unlist(st_nb_dists(geo, nb))\n\n\n\nst_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.\nunlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.\n\n\nDerieve summary statistics of nearest nb distances\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nThe maximum nearest neighbour distance is 65.8 km, thus we will use threshold value of 66km to ensure each spatial unit as least one neighbour.\nCompute the fixed distance weights by using the code chunk below.\n\nwm_fd <- hunan_GDPPC %>%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb,\n                               style = 'W'),\n               .before = 1)\n\n\nst_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\n\n\n\nwm_ad <- hunan_GDPPC %>% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),  # refers to `nb` generated above light\n               .before = 1)\n\n\nst_knn() of sfdep is used to identify neighbors based on k (i.e. k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\n\n\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nst_contiguity() of sfdep is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).\nst_inverse_distance() is then used to calculate inverse distance weights of neighbours on the nb list.\n\n\n\n\n\n\nAfter deriving the spatial weights matrix wm_q (class: ‘sf’ and ‘df’ ) (‘nb’ and ‘wt’) using the Queen’s method, we can calculate\n\n\nUsing the sfdep package, it can be calculated using the code chunk below\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\n\nMonte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm(). Do not assume normality.\nIt is always a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nSince p-value is smaller than 0.05 , we can reject the null hypothesis that the spatial patterns spatial independent. Because the Moran’s I statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.\n\n\n\n\nlisa <- wm_q %>% \n  mutate(local_moran = local_moran(\n    GDPPC, nb,wt, nsim=99),   # straightaway use simulation instead of typical p-value\n    .before=1) %>%\n  unnest(local_moran)  # due to local_moran() function it produces output in a group object. to see it, need to unnest()\n\nThe output of local_moran() is a sf df containing the columns\n\ncolnames(lisa)\n\n [1] \"ii\"           \"eii\"          \"var_ii\"       \"z_ii\"         \"p_ii\"        \n [6] \"p_ii_sim\"     \"p_folded_sim\" \"skewness\"     \"kurtosis\"     \"mean\"        \n[11] \"median\"       \"pysal\"        \"nb\"           \"wt\"           \"NAME_2\"      \n[16] \"ID_3\"         \"NAME_3\"       \"ENGTYPE_3\"    \"County\"       \"GDPPC\"       \n[21] \"geometry\"    \n\n\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: p-value\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\nPrint the lisa sf df.\nThe quadrants (HH, LH, HL, LL) is automatically calculated for us.\nUsually we use mean for the Moran-Scatterplot. However, if the data GDPPC is highly skewed, use median instead of mean.\n\nkable(head(lisa,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nii\neii\nvar_ii\nz_ii\np_ii\np_ii_sim\np_folded_sim\nskewness\nkurtosis\nmean\nmedian\npysal\nnb\nwt\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nCounty\nGDPPC\ngeometry\n\n\n\n\n-0.0014685\n0.0017692\n0.0004180\n-0.1583623\n0.8741713\n0.82\n0.41\n-0.8122108\n0.6518754\nLow-High\nHigh-High\nLow-High\n2, 3, 4, 57, 85\n0.2, 0.2, 0.2, 0.2, 0.2\nChangde\n21098\nAnxiang\nCounty\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n0.0258782\n0.0064149\n0.0105104\n0.1898479\n0.8494283\n0.96\n0.48\n-1.0905447\n1.8891775\nLow-Low\nHigh-High\nLow-Low\n1, 57, 58, 78, 85\n0.2, 0.2, 0.2, 0.2, 0.2\nChangde\n21100\nHanshou\nCounty\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n-0.0119876\n-0.0374069\n0.1020555\n0.0795690\n0.9365800\n0.76\n0.38\n0.8239085\n0.0460951\nHigh-Low\nHigh-High\nHigh-Low\n1, 4, 5, 85\n0.25, 0.25, 0.25, 0.25\nChangde\n21101\nJinshi\nCounty City\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n\n\n\n\n\n\nUse tmap core functions to build a choropleth, using the local moran’s I (ii) field\n\ntmap_mode(\"plot\")\n\nmap1 <- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap1\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2<-tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\",\n          title = \"local Moran's I p-values\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\nmap2\n\n\n\n\nPutting both maps side by side\n\ntmap_arrange(map1, map2, asp=1, ncol = 2)\n\n\n\n\n\n\n\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)  # lpot only sig region at 95% Conf.lvl\ntmap_mode(\"plot\")\n\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\nlisa_sig <- lisa  %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\n\n\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.\n\n\n\n\n\nUsing the inverse distance spatial weights matrix wm_idw derived in section 1.3.3, we can use the local_gstar_perm() of sfdep library to compute Gi* values of each region stored in HCSA, class: ‘sf’, tbl_df’, ‘tbl’, ‘df’.\nIt contains not just the Gi* values based on simulated data but also the p-values.\n\nHCSA <- wm_idw %>% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim=99),   # these 3 var are found in wm_idw\n    .before=1) %>% \n  unnest(local_Gi)\n\nHCSA\n\nSimple feature collection with 88 features and 16 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 17\n   gi_star   e_gi    var_gi p_value   p_sim p_folded_sim skewness kurtosis nb   \n     <dbl>  <dbl>     <dbl>   <dbl>   <dbl>        <dbl>    <dbl>    <dbl> <nb> \n 1  0.0416 0.0114   6.41e-6  0.0493 9.61e-1         0.7      0.35    0.875 <int>\n 2 -0.333  0.0106   3.84e-6 -0.0941 9.25e-1         1        0.5     0.661 <int>\n 3  0.281  0.0126   7.51e-6 -0.151  8.80e-1         0.9      0.45    0.640 <int>\n 4  0.411  0.0118   9.22e-6  0.264  7.92e-1         0.6      0.3     0.853 <int>\n 5  0.387  0.0115   9.56e-6  0.339  7.34e-1         0.62     0.31    1.07  <int>\n 6 -0.368  0.0118   5.91e-6 -0.583  5.60e-1         0.72     0.36    0.594 <int>\n 7  3.56   0.0151   7.31e-6  2.61   9.01e-3         0.06     0.03    1.09  <int>\n 8  2.52   0.0136   6.14e-6  1.49   1.35e-1         0.2      0.1     1.12  <int>\n 9  4.56   0.0144   5.84e-6  3.53   4.17e-4         0.04     0.02    1.23  <int>\n10  1.16   0.0104   3.70e-6  1.82   6.86e-2         0.12     0.06    0.416 <int>\n# ℹ 78 more rows\n# ℹ 8 more variables: wts <list>, NAME_2 <chr>, ID_3 <int>, NAME_3 <chr>,\n#   ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>, geometry <POLYGON [°]>\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap1 <- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap1\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2<- tm_shape(HCSA) +\n  tm_fill(\"p_sim\",\n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\",\n          title = \"local Gi* sim p-values\") + \n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\nmap2\n\n\n\n\nPutting both maps side by side\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nTo plot only the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below\n\nHCSA_sig <- HCSA  %>%\n  filter(p_sim < 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\n\n\nIn this section, we will load in additional set of data. Requires time series data to perform emerging hot spots analysis. The Hunan_GDPPC.csv file contains 10 years worth of GDPPC for each region.\nThe GDPPC has to be in NUMERICAL field. The time field year can be in numerical /pct or date format.\n\nGDPPC <- read_csv('data/aspatial/Hunan_GDPPC.csv')\n\n\n\nUsing spacetime() of sfdep. Creating OLAP cube on the fly.\n\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = 'County',  #location column\n                      .time_col = 'Year')    # time column\n\nclass(GDPPC_st)\n\n[1] \"spacetime\"   \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nstr(GDPPC_st)\n\nspacetim [1,496 × 3] (S3: spacetime/spec_tbl_df/tbl_df/tbl/data.frame)\n $ Year  : num [1:1496] 2005 2005 2005 2005 2005 ...\n $ County: chr [1:1496] \"Longshan\" \"Changsha\" \"Wangcheng\" \"Ningxiang\" ...\n $ GDPPC : num [1:1496] 3469 24612 14659 11687 13406 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Year = col_double(),\n  ..   County = col_character(),\n  ..   GDPPC = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n - attr(*, \"active\")= chr \"data\"\n - attr(*, \"data\")= spc_tbl_ [1,496 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ Year  : num [1:1496] 2005 2005 2005 2005 2005 ...\n  ..$ County: chr [1:1496] \"Longshan\" \"Changsha\" \"Wangcheng\" \"Ningxiang\" ...\n  ..$ GDPPC : num [1:1496] 3469 24612 14659 11687 13406 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   Year = col_double(),\n  .. ..   County = col_character(),\n  .. ..   GDPPC = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n - attr(*, \"geometry\")=Classes 'sf' and 'data.frame':   88 obs. of  8 variables:\n  ..$ NAME_2    : chr [1:88] \"Changde\" \"Changde\" \"Changde\" \"Changde\" ...\n  ..$ ID_3      : int [1:88] 21098 21100 21101 21102 21103 21104 21109 21110 21111 21112 ...\n  ..$ NAME_3    : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n  ..$ ENGTYPE_3 : chr [1:88] \"County\" \"County\" \"County City\" \"County\" ...\n  ..$ Shape_Leng: num [1:88] 1.87 2.36 1.43 3.47 2.29 ...\n  ..$ Shape_Area: num [1:88] 0.101 0.2 0.053 0.189 0.115 ...\n  ..$ County    : chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n  ..$ geometry  :sfc_POLYGON of length 88; first list element: List of 1\n  .. ..$ : num [1:427, 1:2] 112 112 112 112 112 ...\n  .. ..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n  ..- attr(*, \"sf_column\")= chr \"geometry\"\n  ..- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA\n  .. ..- attr(*, \"names\")= chr [1:7] \"NAME_2\" \"ID_3\" \"NAME_3\" \"ENGTYPE_3\" ...\n - attr(*, \"loc_col\")= chr \"County\"\n - attr(*, \"locs\")= chr [1:88] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n - attr(*, \"n_locs\")= int 88\n - attr(*, \"time_col\")= chr \"Year\"\n - attr(*, \"times\")= num [1:17] 2005 2006 2007 2008 2009 ...\n - attr(*, \"n_times\")= int 17\n\n\nNote: GDPPC_st looks identical to GDPPC but they are not the same class type. To verify, use the code below\n\nis_spacetime_cube(GDPPC_st)\n\n[1] TRUE\n\n\n\n\n\n\n\nThe code chunk below will identify nb and derive a space-time inverse weights distance weights. The earlier derived were spatial weights matrix, this time round involves time dimension too.\nNote that this dataset now has neighbors and weights for each time-slice.\n\nGDPPC_nb <- GDPPC_st %>% \n  activate('geometry') %>% # space-time cube has 'geometry' and 'attributes' component, we have to activate the geometry component before able to derive neighbours\n  mutate(nb= include_self(st_contiguity(geometry)),  # need to inlude self to get Gi*. otherwise its Gi.\n         wt = st_inverse_distance(nb, geometry,\n                                  scale =1,\n                                  alpha =1),\n         .before =1) %>% \n  set_nbs('nb') %>% \n  set_wts('wt')\n\nclass(GDPPC_nb)\n\n[1] \"spacetime\"  \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\n\n\nExplaining why need to activate geometry, show table later\n\nhead(GDPPC_nb)\n\n# A tibble: 6 × 5\n   Year County  GDPPC nb        wt       \n  <dbl> <chr>   <dbl> <list>    <list>   \n1  2005 Anxiang  8184 <int [6]> <dbl [6]>\n2  2005 Hanshou  6560 <int [6]> <dbl [6]>\n3  2005 Jinshi   9956 <int [5]> <dbl [5]>\n4  2005 Li       8394 <int [5]> <dbl [5]>\n5  2005 Linli    8850 <int [5]> <dbl [5]>\n6  2005 Shimen   9244 <int [6]> <dbl [6]>\n\n\n\n\n\n\n\n\nNote\n\n\n\nGDPPC_st : space-time cube\nGDPPC_nb: space-time weights matrixs\n\n\n\n\n\nWe can use the time-space weights matrix GDPPC_nb to manually calculate the local Gi* for each location.\n\ngroup by Year and using local_gstar_perm() of sfdep package.\nuse unnest() to unnest gi_star column of the newly created gi_stars data.frame. THe output of this function is grouped and thus we cannot see it unless unnest().\ngi_stars df has 1496 rows instead of 88 because we have ten years worth of data\n\n\ngi_stars <- GDPPC_nb %>% \n  group_by(Year) %>%   # look at a time slice, for all locations\n  mutate(gi_star = local_gstar_perm(\n  GDPPC, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n\ngi_stars\n\n# A tibble: 1,496 × 13\n# Groups:   Year [17]\n    Year County    GDPPC nb        wt    gi_star   e_gi  var_gi  p_value   p_sim\n   <dbl> <chr>     <dbl> <list>    <lis>   <dbl>  <dbl>   <dbl>    <dbl>   <dbl>\n 1  2005 Anxiang    8184 <int [6]> <dbl>   0.398 0.0115 2.69e-6  0.382   7.02e-1\n 2  2005 Hanshou    6560 <int [6]> <dbl>  -0.237 0.0109 2.64e-6  0.00199 9.98e-1\n 3  2005 Jinshi     9956 <int [5]> <dbl>   1.05  0.0126 3.33e-6  0.507   6.12e-1\n 4  2005 Li         8394 <int [5]> <dbl>   0.966 0.0117 3.24e-6  0.920   3.57e-1\n 5  2005 Linli      8850 <int [5]> <dbl>   1.05  0.0120 3.23e-6  0.885   3.76e-1\n 6  2005 Shimen     9244 <int [6]> <dbl>   0.210 0.0121 2.99e-6 -0.215   8.30e-1\n 7  2005 Liuyang   13406 <int [5]> <dbl>   3.91  0.0142 2.52e-6  3.36    7.69e-4\n 8  2005 Ningxiang 11687 <int [8]> <dbl>   1.61  0.0127 2.30e-6  0.895   3.71e-1\n 9  2005 Wangcheng 14659 <int [7]> <dbl>   3.88  0.0140 2.49e-6  2.73    6.28e-3\n10  2005 Anren      7423 <int [9]> <dbl>   1.67  0.0113 2.14e-6  1.87    6.21e-2\n# ℹ 1,486 more rows\n# ℹ 3 more variables: p_folded_sim <dbl>, skewness <dbl>, kurtosis <dbl>\n\nstr(gi_stars)\n\ngropd_df [1,496 × 13] (S3: grouped_df/tbl_df/tbl/data.frame)\n $ Year        : num [1:1496] 2005 2005 2005 2005 2005 ...\n $ County      : chr [1:1496] \"Anxiang\" \"Hanshou\" \"Jinshi\" \"Li\" ...\n $ GDPPC       : num [1:1496] 8184 6560 9956 8394 8850 ...\n $ nb          :List of 1496\n  ..$ : int [1:6] 1 2 3 4 57 85\n  ..$ : int [1:6] 1 2 57 58 78 85\n  ..$ : int [1:5] 1 3 4 5 85\n  ..$ : int [1:5] 1 3 4 5 6\n  ..$ : int [1:5] 3 4 5 6 85\n  ..$ : int [1:6] 4 5 6 69 75 85\n  ..$ : int [1:5] 7 67 71 74 84\n  ..$ : int [1:8] 8 9 46 47 56 78 80 86\n  ..$ : int [1:7] 8 9 66 68 78 84 86\n  ..$ : int [1:9] 10 16 17 19 20 22 70 72 73\n  ..$ : int [1:4] 11 14 17 72\n  ..$ : int [1:6] 12 13 60 61 63 83\n  ..$ : int [1:5] 12 13 15 60 83\n  ..$ : int [1:4] 11 14 15 17\n  ..$ : int [1:5] 13 14 15 17 83\n  ..$ : int [1:6] 10 16 17 22 72 83\n  ..$ : int [1:8] 10 11 14 15 16 17 72 83\n  ..$ : int [1:6] 18 20 22 23 77 83\n  ..$ : int [1:7] 10 19 20 21 73 74 86\n  ..$ : int [1:8] 10 18 19 20 21 22 23 82\n  ..$ : int [1:6] 19 20 21 35 82 86\n  ..$ : int [1:6] 10 16 18 20 22 83\n  ..$ : int [1:8] 18 20 23 38 41 77 79 82\n  ..$ : int [1:6] 24 25 28 31 32 54\n  ..$ : int [1:6] 24 25 28 31 33 81\n  ..$ : int [1:5] 26 27 33 42 81\n  ..$ : int [1:4] 26 27 29 42\n  ..$ : int [1:6] 24 25 28 33 49 54\n  ..$ : int [1:4] 27 29 37 42\n  ..$ : int [1:2] 30 33\n  ..$ : int [1:9] 24 25 31 32 36 39 40 56 81\n  ..$ : int [1:9] 24 31 32 50 54 55 56 75 85\n  ..$ : int [1:6] 25 26 28 30 33 81\n  ..$ : int [1:4] 34 36 45 80\n  ..$ : int [1:7] 21 35 41 47 80 82 86\n  ..$ : int [1:7] 31 34 36 40 45 56 80\n  ..$ : int [1:5] 29 37 42 43 44\n  ..$ : int [1:5] 23 38 44 77 79\n  ..$ : int [1:6] 31 39 40 42 43 81\n  ..$ : int [1:7] 31 36 39 40 43 45 79\n  ..$ : int [1:7] 23 35 41 45 79 80 82\n  ..$ : int [1:8] 26 27 29 37 39 42 43 81\n  ..$ : int [1:7] 37 39 40 42 43 44 79\n  ..$ : int [1:5] 37 38 43 44 79\n  ..$ : int [1:7] 34 36 40 41 45 79 80\n  ..$ : int [1:4] 8 46 47 86\n  ..$ : int [1:6] 8 35 46 47 80 86\n  ..$ : int [1:6] 48 50 51 52 53 55\n  ..$ : int [1:5] 28 49 51 52 54\n  ..$ : int [1:6] 32 48 50 52 54 55\n  ..$ : int [1:4] 48 49 51 52\n  ..$ : int [1:6] 48 49 50 51 52 54\n  ..$ : int [1:4] 48 53 55 75\n  ..$ : int [1:7] 24 28 32 49 50 52 54\n  ..$ : int [1:6] 32 48 50 53 55 75\n  ..$ : int [1:8] 8 31 32 36 56 78 80 85\n  ..$ : int [1:7] 1 2 57 58 64 76 85\n  ..$ : int [1:6] 2 57 58 68 76 78\n  ..$ : int [1:5] 59 60 61 87 88\n  ..$ : int [1:5] 12 13 59 60 61\n  ..$ : int [1:8] 12 59 60 61 62 63 77 87\n  ..$ : int [1:4] 61 62 77 87\n  ..$ : int [1:5] 12 61 63 77 83\n  ..$ : int [1:3] 57 64 76\n  ..$ : int [1:2] 65 76\n  ..$ : int [1:6] 9 66 67 68 76 84\n  ..$ : int [1:5] 7 66 67 76 84\n  ..$ : int [1:6] 9 58 66 68 76 78\n  ..$ : int [1:4] 6 69 75 85\n  ..$ : int [1:4] 10 70 72 73\n  ..$ : int [1:4] 7 71 73 74\n  ..$ : int [1:6] 10 11 16 17 70 72\n  ..$ : int [1:6] 10 19 70 71 73 74\n  ..$ : int [1:7] 7 19 71 73 74 84 86\n  ..$ : int [1:7] 6 32 53 55 69 75 85\n  ..$ : int [1:8] 57 58 64 65 66 67 68 76\n  ..$ : int [1:8] 18 23 38 61 62 63 77 83\n  ..$ : int [1:8] 2 8 9 56 58 68 78 85\n  ..$ : int [1:8] 23 38 40 41 43 44 45 79\n  ..$ : int [1:9] 8 34 35 36 41 45 47 56 80\n  ..$ : int [1:7] 25 26 31 33 39 42 81\n  ..$ : int [1:6] 20 21 23 35 41 82\n  ..$ : int [1:10] 12 13 15 16 17 18 22 63 77 83\n  ..$ : int [1:7] 7 9 66 67 74 84 86\n  ..$ : int [1:12] 1 2 3 5 6 32 56 57 69 75 ...\n  ..$ : int [1:10] 8 9 19 21 35 46 47 74 84 86\n  ..$ : int [1:5] 59 61 62 87 88\n  ..$ : int [1:3] 59 87 88\n  ..$ : int [1:6] 1 2 3 4 57 85\n  ..$ : int [1:6] 1 2 57 58 78 85\n  ..$ : int [1:5] 1 3 4 5 85\n  ..$ : int [1:5] 1 3 4 5 6\n  ..$ : int [1:5] 3 4 5 6 85\n  ..$ : int [1:6] 4 5 6 69 75 85\n  ..$ : int [1:5] 7 67 71 74 84\n  ..$ : int [1:8] 8 9 46 47 56 78 80 86\n  ..$ : int [1:7] 8 9 66 68 78 84 86\n  ..$ : int [1:9] 10 16 17 19 20 22 70 72 73\n  ..$ : int [1:4] 11 14 17 72\n  .. [list output truncated]\n  ..- attr(*, \"class\")= chr [1:2] \"list\" \"nb\"\n $ wt          :List of 1496\n  ..$ : num [1:6] 0 0.0153 0.0352 0.0218 0.0284 ...\n  ..$ : num [1:6] 0.0153 0 0.016 0.0191 0.0233 ...\n  ..$ : num [1:5] 0.0352 0 0.0458 0.0412 0.0121\n  ..$ : num [1:5] 0.0218 0.0458 0 0.0464 0.0159\n  ..$ : num [1:5] 0.0412 0.0464 0 0.019 0.0135\n  ..$ : num [1:6] 0.0159 0.019 0 0.0271 0.0114 ...\n  ..$ : num [1:5] 0 0.0162 0.0154 0.0113 0.0184\n  ..$ : num [1:8] 0 0.0193 0.0268 0.0215 0.0108 ...\n  ..$ : num [1:7] 0.0193 0 0.0165 0.018 0.0147 ...\n  ..$ : num [1:9] 0 0.0274 0.0139 0.0146 0.0216 ...\n  ..$ : num [1:4] 0 0.0234 0.0236 0.0239\n  ..$ : num [1:6] 0 0.0288 0.0271 0.0251 0.0293 ...\n  ..$ : num [1:5] 0.0288 0 0.0449 0.0283 0.0175\n  ..$ : num [1:4] 0.02339 0 0.00972 0.0204\n  ..$ : num [1:5] 0.04488 0.00972 0 0.01052 0.01695\n  ..$ : num [1:6] 0.0274 0 0.0273 0.0233 0.019 ...\n  ..$ : num [1:8] 0.0139 0.0236 0.0204 0.0105 0.0273 ...\n  ..$ : num [1:6] 0 0.0148 0.0188 0.018 0.018 ...\n  ..$ : num [1:7] 0.0146 0 0.0253 0.0454 0.0225 ...\n  ..$ : num [1:8] 0.0216 0.0148 0.0253 0 0.0194 ...\n  ..$ : num [1:6] 0.0454 0.0194 0 0.0147 0.0227 ...\n  ..$ : num [1:6] 0.0242 0.0233 0.0188 0.0255 0 ...\n  ..$ : num [1:8] 0.018 0.0121 0 0.0142 0.0188 ...\n  ..$ : num [1:6] 0 0.0244 0.0185 0.0228 0.012 ...\n  ..$ : num [1:6] 0.0244 0 0.0172 0.019 0.0159 ...\n  ..$ : num [1:5] 0 0.022 0.0187 0.0199 0.0213\n  ..$ : num [1:4] 0.022 0 0.0253 0.0158\n  ..$ : num [1:6] 0.0185 0.0172 0 0.0205 0.0343 ...\n  ..$ : num [1:4] 0.0253 0 0.0166 0.0127\n  ..$ : num [1:2] 0 0.0237\n  ..$ : num [1:9] 0.0228 0.019 0 0.0119 0.0157 ...\n  ..$ : num [1:9] 0.012 0.0119 0 0.0164 0.014 ...\n  ..$ : num [1:6] 0.0159 0.0187 0.0205 0.0237 0 ...\n  ..$ : num [1:4] 0 0.0311 0.039 0.03\n  ..$ : num [1:7] 0.0147 0 0.022 0.0255 0.0218 ...\n  ..$ : num [1:7] 0.0157 0.0311 0 0.015 0.0189 ...\n  ..$ : num [1:5] 0.0166 0 0.0247 0.0149 0.014\n  ..$ : num [1:5] 0.0142 0 0.0205 0.0228 0.0177\n  ..$ : num [1:6] 0.0133 0 0.0237 0.0164 0.0266 ...\n  ..$ : num [1:7] 0.0162 0.015 0.0237 0 0.0153 ...\n  ..$ : num [1:7] 0.0188 0.022 0 0.0236 0.0188 ...\n  ..$ : num [1:8] 0.0199 0.0158 0.0127 0.0247 0.0164 ...\n  ..$ : num [1:7] 0.0149 0.0266 0.0153 0.0201 0 ...\n  ..$ : num [1:5] 0.014 0.0205 0.0316 0 0.0166\n  ..$ : num [1:7] 0.039 0.0189 0.0191 0.0236 0 ...\n  ..$ : num [1:4] 0.0268 0 0.039 0.0267\n  ..$ : num [1:6] 0.0215 0.0255 0.039 0 0.0192 ...\n  ..$ : num [1:6] 0 0.0207 0.0344 0.0214 0.0165 ...\n  ..$ : num [1:5] 0.0343 0 0.0231 0.0266 0.0208\n  ..$ : num [1:6] 0.0164 0.0207 0 0.0269 0.0227 ...\n  ..$ : num [1:4] 0.0344 0.0231 0 0.0306\n  ..$ : num [1:6] 0.0214 0.0266 0.0269 0.0306 0 ...\n  ..$ : num [1:4] 0.0165 0 0.0207 0.0122\n  ..$ : num [1:7] 0.0273 0.0177 0.014 0.0208 0.0227 ...\n  ..$ : num [1:6] 0.0114 0.016 0.0181 0.0207 0 ...\n  ..$ : num [1:8] 0.0108 0.011 0.0131 0.0204 0 ...\n  ..$ : num [1:7] 0.0284 0.016 0 0.0258 0.041 ...\n  ..$ : num [1:6] 0.0191 0.0258 0 0.0309 0.0153 ...\n  ..$ : num [1:5] 0 0.0169 0.0127 0.0162 0.0184\n  ..$ : num [1:5] 0.0271 0.0283 0.0169 0 0.0219\n  ..$ : num [1:8] 0.0251 0.0127 0.0219 0 0.024 ...\n  ..$ : num [1:4] 0.024 0 0.0184 0.0225\n  ..$ : num [1:5] 0.0293 0.0292 0 0.0152 0.0172\n  ..$ : num [1:3] 0.041 0 0.0184\n  ..$ : num [1:2] 0 0.0209\n  ..$ : num [1:6] 0.0165 0 0.0183 0.0344 0.016 ...\n  ..$ : num [1:5] 0.0162 0.0183 0 0.0121 0.0135\n  ..$ : num [1:6] 0.018 0.0309 0.0344 0 0.0154 ...\n  ..$ : num [1:4] 0.0271 0 0.0146 0.0156\n  ..$ : num [1:4] 0.0235 0 0.018 0.0274\n  ..$ : num [1:4] 0.0154 0 0.0158 0.0297\n  ..$ : num [1:6] 0.0178 0.0239 0.019 0.0166 0.018 ...\n  ..$ : num [1:6] 0.0162 0.0225 0.0274 0.0158 0 ...\n  ..$ : num [1:7] 0.0113 0.0175 0.0297 0.0151 0 ...\n  ..$ : num [1:7] 0.0114 0.0108 0.0122 0.0223 0.0146 ...\n  ..$ : num [1:8] 0.0165 0.0153 0.0184 0.0209 0.016 ...\n  ..$ : num [1:8] 0.018 0.0194 0.0228 0.0128 0.0184 ...\n  ..$ : num [1:8] 0.0233 0.0261 0.0147 0.012 0.0164 ...\n  ..$ : num [1:8] 0.013 0.0177 0.0197 0.0188 0.0155 ...\n  ..$ : num [1:9] 0.0152 0.03 0.0218 0.0175 0.0163 ...\n  ..$ : num [1:7] 0.029 0.0213 0.0122 0.0195 0.018 ...\n  ..$ : num [1:6] 0.0171 0.0227 0.0215 0.0197 0.0168 ...\n  ..$ : num [1:10] 0.0207 0.0175 0.017 0.015 0.0159 ...\n  ..$ : num [1:7] 0.0184 0.0302 0.0197 0.0135 0.0134 ...\n  ..$ : num [1:12] 0.0103 0.0159 0.0121 0.0135 0.0108 ...\n  ..$ : num [1:10] 0.0134 0.0161 0.0146 0.0184 0.0142 ...\n  ..$ : num [1:5] 0.0162 0.0209 0.0225 0 0.0233\n  ..$ : num [1:3] 0.0184 0.0233 0\n  ..$ : num [1:6] 0 0.0153 0.0352 0.0218 0.0284 ...\n  ..$ : num [1:6] 0.0153 0 0.016 0.0191 0.0233 ...\n  ..$ : num [1:5] 0.0352 0 0.0458 0.0412 0.0121\n  ..$ : num [1:5] 0.0218 0.0458 0 0.0464 0.0159\n  ..$ : num [1:5] 0.0412 0.0464 0 0.019 0.0135\n  ..$ : num [1:6] 0.0159 0.019 0 0.0271 0.0114 ...\n  ..$ : num [1:5] 0 0.0162 0.0154 0.0113 0.0184\n  ..$ : num [1:8] 0 0.0193 0.0268 0.0215 0.0108 ...\n  ..$ : num [1:7] 0.0193 0 0.0165 0.018 0.0147 ...\n  ..$ : num [1:9] 0 0.0274 0.0139 0.0146 0.0216 ...\n  ..$ : num [1:4] 0 0.0234 0.0236 0.0239\n  .. [list output truncated]\n $ gi_star     : num [1:1496] 0.398 -0.237 1.053 0.966 1.048 ...\n $ e_gi        : num [1:1496] 0.0115 0.0109 0.0126 0.0117 0.012 ...\n $ var_gi      : num [1:1496] 2.69e-06 2.64e-06 3.33e-06 3.24e-06 3.23e-06 ...\n $ p_value     : num [1:1496] 0.3821 0.00199 0.50708 0.92031 0.88485 ...\n $ p_sim       : num [1:1496] 0.702 0.998 0.612 0.357 0.376 ...\n $ p_folded_sim: num [1:1496] 0.608 0.892 0.528 0.308 0.352 0.92 0.008 0.396 0.036 0.08 ...\n $ skewness    : num [1:1496] 0.304 0.446 0.264 0.154 0.176 0.46 0.004 0.198 0.018 0.04 ...\n $ kurtosis    : num [1:1496] 0.893 0.82 0.929 1.185 0.874 ...\n - attr(*, \"groups\")= tibble [17 × 2] (S3: tbl_df/tbl/data.frame)\n  ..$ Year : num [1:17] 2005 2006 2007 2008 2009 ...\n  ..$ .rows: list<int> [1:17] \n  .. ..$ : int [1:88] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..$ : int [1:88] 89 90 91 92 93 94 95 96 97 98 ...\n  .. ..$ : int [1:88] 177 178 179 180 181 182 183 184 185 186 ...\n  .. ..$ : int [1:88] 265 266 267 268 269 270 271 272 273 274 ...\n  .. ..$ : int [1:88] 353 354 355 356 357 358 359 360 361 362 ...\n  .. ..$ : int [1:88] 441 442 443 444 445 446 447 448 449 450 ...\n  .. ..$ : int [1:88] 529 530 531 532 533 534 535 536 537 538 ...\n  .. ..$ : int [1:88] 617 618 619 620 621 622 623 624 625 626 ...\n  .. ..$ : int [1:88] 705 706 707 708 709 710 711 712 713 714 ...\n  .. ..$ : int [1:88] 793 794 795 796 797 798 799 800 801 802 ...\n  .. ..$ : int [1:88] 881 882 883 884 885 886 887 888 889 890 ...\n  .. ..$ : int [1:88] 969 970 971 972 973 974 975 976 977 978 ...\n  .. ..$ : int [1:88] 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 ...\n  .. ..$ : int [1:88] 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 ...\n  .. ..$ : int [1:88] 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 ...\n  .. ..$ : int [1:88] 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 ...\n  .. ..$ : int [1:88] 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 ...\n  .. ..@ ptype: int(0) \n  ..- attr(*, \".drop\")= logi TRUE\n\n\nPrint the gi_stars df\n\n#kable(head(gi_stars,5))\ngi_stars %>% \n  arrange(County) %>% \n  head(5) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nCounty\nGDPPC\nnb\nwt\ngi_star\ne_gi\nvar_gi\np_value\np_sim\np_folded_sim\nskewness\nkurtosis\n\n\n\n\n2005\nAnhua\n4411\n8, 31, 32, 36, 56, 78, 80, 85\n0.01076895, 0.01100041, 0.01312949, 0.02041572, 0.00000000, 0.01199886, 0.01269364, 0.01519702\n-1.1495103\n0.0104408\n2.8e-06\n-0.6061786\n0.5443961\n0.632\n0.316\n0.7466849\n\n\n2006\nAnhua\n5048\n8, 31, 32, 36, 56, 78, 80, 85\n0.01076895, 0.01100041, 0.01312949, 0.02041572, 0.00000000, 0.01199886, 0.01269364, 0.01519702\n-1.2062412\n0.0103340\n2.4e-06\n-0.6666373\n0.5050038\n0.532\n0.266\n0.6857186\n\n\n2007\nAnhua\n6130\n8, 31, 32, 36, 56, 78, 80, 85\n0.01076895, 0.01100041, 0.01312949, 0.02041572, 0.00000000, 0.01199886, 0.01269364, 0.01519702\n-1.0623115\n0.0104177\n2.3e-06\n-0.6082854\n0.5429982\n0.580\n0.290\n0.7884648\n\n\n2008\nAnhua\n7230\n8, 31, 32, 36, 56, 78, 80, 85\n0.01076895, 0.01100041, 0.01312949, 0.02041572, 0.00000000, 0.01199886, 0.01269364, 0.01519702\n-0.9631087\n0.0104526\n3.0e-06\n-0.5122872\n0.6084500\n0.696\n0.348\n0.6719202\n\n\n2009\nAnhua\n8528\n8, 31, 32, 36, 56, 78, 80, 85\n0.01076895, 0.01100041, 0.01312949, 0.02041572, 0.00000000, 0.01199886, 0.01269364, 0.01519702\n-0.7063891\n0.0105453\n4.2e-06\n-0.3396736\n0.7341023\n0.848\n0.424\n0.8584200\n\n\n\n\n\n\n\n\n\nWith these yearly Gi* measures for each location, we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county. Test location by location.\n\ncbg <- gi_stars %>% \n  ungroup() %>%   # we have grouped it earlier by Year\n  filter(County == 'Changsha') %>% \n  select(County, Year, gi_star)\n\ncbg\n\n# A tibble: 17 × 3\n   County    Year gi_star\n   <chr>    <dbl>   <dbl>\n 1 Changsha  2005    5.03\n 2 Changsha  2006    5.17\n 3 Changsha  2007    5.30\n 4 Changsha  2008    5.60\n 5 Changsha  2009    6.28\n 6 Changsha  2010    5.94\n 7 Changsha  2011    5.75\n 8 Changsha  2012    5.69\n 9 Changsha  2013    5.71\n10 Changsha  2014    5.76\n11 Changsha  2015    6.10\n12 Changsha  2016    6.00\n13 Changsha  2017    6.20\n14 Changsha  2018    6.04\n15 Changsha  2019    6.58\n16 Changsha  2020    5.77\n17 Changsha  2021    5.75\n\n\nPlotting using ggplot2() functions\n\nggplot(data = cbg,\n       aes(x= Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\nUse ggploty() from plotly library to make the chart interactive.\n\nggplotly(ggplot(data = cbg,\n       aes(x= Year,\n           y = gi_star)) +\n  geom_line() +\n  theme_light())\n\n\n\n\n\nPerforming the Mann-Kendall test.\nThe ‘sl’ column is the p_value. ‘tau’ column is the trend\n\ncbg %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 0.485 0.00742    66  136.  589.\n\n\nThere is a slight upward but insignificant trend (> 0.05)\n\n\nTo replicate this code for each location (to check for statistical sig for trend in all location), use the group_by function. The number of rows is back to 88.\n\nehsa <- gi_stars %>%\n  group_by(County) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\nPrint the output in an interactive table\n\ndatatable(ehsa)\n\n\n\n\n\n\n\n\n\n\n\n\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:5)\n\nkable(emerging)\n\n\n\n\nCounty\ntau\nsl\nS\nD\nvarS\n\n\n\n\nShuangfeng\n0.8676472\n1.4e-06\n118\n136\n589.3333\n\n\nXiangtan\n0.8676472\n1.4e-06\n118\n136\n589.3333\n\n\nXiangxiang\n0.8676472\n1.4e-06\n118\n136\n589.3333\n\n\nChengbu\n-0.8235295\n4.8e-06\n-112\n136\n589.3333\n\n\nDongan\n-0.8235295\n4.8e-06\n-112\n136\n589.3333\n\n\n\n\n\n\n\n\n\n\n\nWe will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It requires\n\na spacetime object, x (i.e. GDPPC_st)\nthe quoted name of the variable of interest, .var (i.e. GDPPC)\nk argument is used to specify the number of time lags which is set to 1 by default.\nnsim map numbers of simulation to be performed.\n\n\nehsa <- emerging_hotspot_analysis(\n  x = GDPPC_st, # timeseries data stored in this spacetime cube\n  .var='GDPPC',  # spatial element found inside this column\n  k=1,   # timelag\n  nsim=99\n)\n\nPrint the output\n\ndatatable(ehsa)\n\n\n\n\n\n\n\n\n\nplot barchart to check distribution of EHSA classes.\n\nggplotly(ggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar())\n\n\n\n\n\nSporadic cold spots has the highest numbers of county\n\n\n\nBecause ehsa df does not have geometry data, we can join both hunan geospatial df with ehsa to make a sf object.\n\nhunan_ehsa <-left_join(hunan, ehsa,\n            by=c('County' = 'location'))\n\nclass(hunan_ehsa)\n\n[1] \"sf\"         \"data.frame\"\n\n\nUse tmap functions to plot choropleth map We can use the p-value inside to plot only the significant emerging hot/cold spots, by filtering rows with p_values < 0.05\nTake note: no pattern (yellow) doesnt mean not-sig (grey)\n\nehsa_sig <-  hunan_ehsa %>% \n  filter(p_value < 0.05)\n\ntmap_mode('plot')\n#base\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  #sif\n  tm_shape(ehsa_sig) +\n  tm_fill('classification') +\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "NYX Geospatial App",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex3/In-class_Ex3.html",
    "title": "In-class Exercise 3:",
    "section": "",
    "text": "#pacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#overview",
    "href": "In-class_Ex3/In-class_Ex3_2.html#overview",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nSpatial Interaction Models (SIMs) are mathematical models for estimating flows between spatial entities developed by Alan Wilson in the late 1960s and early 1970, with considerable uptake and refinement for transport modelling since then Boyce and Williams (2015).\nThere are four main types of traditional SIMs (Wilson 1971):\n\nUnconstrained\nProduction-constrained\nAttraction-constrained\nDoubly-constrained\n\nOrdinary least square (OLS), log-normal, Poisson and negative binomial (NB) regression methods have been used extensively to calibrate OD flow models by processing flow data as different types of dependent variables. In this chapter, you will gain hands-on experiences on using appropriate R packages to calibrate SIM by using there four regression methods."
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#the-case-study-and-data",
    "href": "In-class_Ex3/In-class_Ex3_2.html#the-case-study-and-data",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.2 The Case Study and Data",
    "text": "16.2 The Case Study and Data\nIn this exercise, we are going to calibrate SIM to determine factors affecting the public bus passenger flows during the morning peak in Singapore."
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#getting-started",
    "href": "In-class_Ex3/In-class_Ex3_2.html#getting-started",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.3 Getting Started",
    "text": "16.3 Getting Started\nFor the purpose of this exercise, four R packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\nsp package , although an older package, is more efficient for computation of large data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps\nggpubr for some easy-to-use functions (like ggarrange())for creating and customizing ‘ggplot2’- based publication ready plots.\nperformance is part of the easystats package for computing measures to assess model quality, which are not directly provided by R’s ‘base’ or ‘stats’ packages. The primary goal of the performance package is to provide utilities for computing indices of model quality and goodness of fit. These include measures like r-squared (R2), root mean squared error (RMSE)\nreshape2 is an old tool from base R. It handles matrix well for our distance matrix, like pivoting function like melt(). Tidyverse does not handle matrix very well.\n\n\npacman::p_load(tmap, sf, sp, DT,\n               performance, reshape2,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#the-data",
    "href": "In-class_Ex3/In-class_Ex3_2.html#the-data",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.4 The Data",
    "text": "16.4 The Data\nThis exercise is a continuation of Chapter 15: Processing and Visualising Flow Data and the following data will be used:\n\nod_data.rds, weekday morning peak passenger flows at planning subzone level.\nmpsz.rds, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.\n\nBeside these two data sets, an additional attribute data file called pop.csv will be provided. It"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#computing-distance-matrix",
    "href": "In-class_Ex3/In-class_Ex3_2.html#computing-distance-matrix",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.5 Computing Distance Matrix",
    "text": "16.5 Computing Distance Matrix\nIn spatial interaction, a distance matrix is a table that shows the distance between pairs of locations. For example, in the table below we can see an Euclidean distance of 3926.0025 between MESZ01 and RVSZ05, of 3939.1079 between MESZ01 and SRSZ01, and so on. By definition, an location’s distance from itself, which is shown in the main diagonal of the table, is 0.\n\nIn this section, you will learn how to compute a distance matrix by using URA Master Plan 2019 Planning Subzone boundary in which you saved as an rds file called mpsz.\nFirst, let us import mpsz.rds into R environemnt by using the code chunk below.\n\nmpsz <- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\In-class_Ex3\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\nNotice that it is a sf tibble dataframe object class.\n\n16.5.1 Converting from sf data.table to SpatialPolygonsDataFrame\nThere are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shown that computing distance matrix by using sf function took relatively longer time that sp method especially the data set is large. In view of this, sp method is used in the code chunks below.\nFirst as.Spatial() will be used to convert mpsz from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.\nIt has become a large spatialpolygendataframe (older). It contains a data table inside, but no geometry column (contain in another table). Wheras in new sf, everything is in a single table.\n\nmpsz_sp <- as(mpsz, \"Spatial\")\n#mpsz_sp <- mpsz %>% \n  #as.Spatial()\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\nExploration: How to access a SpatialPolygonDataFrame object of the older sp package.\n\nmpsz_sp['SUBZONE_N'][[1]]\nmpsz_sp@data  # class dataframe\nmpsz_sp@polygons # class: list\nmpsz_sp@polygons[[1]]  # access the first polygon / subzone\nmpsz_sp@polygons[[1]]@Polygons # access the slot in the polygon object that contains information about individual Polygons within the overall geometry\nmpsz_sp@polygons[[1]]@Polygons[[1]] # same as above, enter another layer\nmpsz_sp@polygons[[1]]@Polygons[[1]]@coords #get the coordinates of the first polygon / subzone\nmpsz_sp@polygons[[332]]@Polygons[[1]]@coords #total of 333 subzones\n\n\n\n16.5.2 Computing the distance matrix\nNext, spDists() of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones.\nspDists(x, y = x, longlat = FALSE, segments = FALSE, diagonal = FALSE)\nspDists returns a full matrix of distances in the metric of the points if longlat=FALSE, or in kilometers if longlat=TRUE; it uses spDistsN1 in case points are two-dimensional. In case of spDists(x,x), it will compute all n x n distances, not the sufficient n x (n-1).\nArguments\nx: A matrix of n-D points with row denoting points, first column x/longitude, second column y/latitude, or a Spatial object that has a coordinates method\ny: A matrix of n-D points with row denoting points, first column x/longitude, second column y/latitude, or a Spatial object that has a coordinates method\nlonglat: logical; if FALSE (default), Euclidean distance, if TRUE Great Circle (WGS84 ellipsoid) distance; if x is a Spatial object, longlat should not be specified but will be derived from is.projected(x)\nsegments: logical; if TRUE, y must be missing; the vector of distances between consecutive points in x is returned.\ndiagonal: logical; if TRUE, y must be given and have the same number of points as x; the vector with distances between points with identical index is returned.\nThe diagonals of the ouput (332 by 332) are all 0. Distance with itself. The unit of distance is if ‘m’ (euclidean?) and km if WSG84?\n\ndist <- spDists(mpsz_sp, \n                longlat = FALSE) # already projected in EPSG:3414\nhead(dist, n=c(10, 10))\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\nNotice that the output dist is a matrix object class of R. Also notice that the column heanders and row headers are not labeled with the planning subzone codes.\n\n\n16.5.3 Labelling column and row heanders of a distance matrix\nFirst, we will create a list sorted according to the the distance matrix by planning sub-zone code.\n\nsz_names <- mpsz$SUBZONE_C\n\nNext we will attach SUBZONE_C to row and column for distance matrix matching ahead\n\ncolnames(dist) <- paste0(sz_names)\nrownames(dist) <- paste0(sz_names)\ndist[1:5,1:5]\n\n          MESZ01     RVSZ05    SRSZ01   WISZ01     MUSZ02\nMESZ01     0.000  3926.0025  3939.108 20252.96  2989.9839\nRVSZ05  3926.003     0.0000   305.737 16513.86   951.8314\nSRSZ01  3939.108   305.7370     0.000 16412.06  1045.9088\nWISZ01 20252.964 16513.8648 16412.062     0.00 17450.3044\nMUSZ02  2989.984   951.8314  1045.909 17450.30     0.0000\n\n\n\n\n16.5.4 Pivoting distance value by SUBZONE_C\nNext, we will pivot the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.\nWe will use the melt() function of the reshape2 package to convert wide-format data to long-format data. This function will convert wide-format data to a data frame with columns for each combination of row and column indices and their corresponding values.\nTo do the opposite, used cast().\n\nwidelong\n\n\n\nmatrix(1:6, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\n\nreshape2::melt(matrix(1:6, nrow = 2, ncol = 3)) %>% knitr::kable()\n\n\n\n\nVar1\nVar2\nvalue\n\n\n\n\n1\n1\n1\n\n\n2\n1\n2\n\n\n1\n2\n3\n\n\n2\n2\n4\n\n\n1\n3\n5\n\n\n2\n3\n6\n\n\n\n\n\n\n\n\nThree new columns generated, (1) ‘var1’, (2) ‘var2’ and (3) ‘value’ containing the distance for the corresponding var1-var2 pair; thus rename to ‘dist’.\nThere are 110,224 rows in distPair due to 332P2 + 332 = 332*331 + 332. Number of possible permutations with replacement.\n\ndistPair <- melt(dist) %>%\n  rename(dist = value)\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\nNotice that the within zone distance is 0.\n\n\n16.5.5 Updating intra-zonal distances\nIn this section, we are going to append a constant value to replace the intra-zonal distance of 0.\nFirst, we will select and find out the minimum value of the distance by using summary().\n\ndistPair %>%\n  filter(dist > 0) %>%\n  summary()\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nAfter removing distance = 0 (intra), the minimum inter-zonal distance is 173.8m.\nNext, a constant distance value of 50m (estimate based on 173.8m) is added into intra-zones distance. The diagonals of dist matrix (if still a matrix) would have been 50m.\n\ndistPair$dist <- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\nThe code chunk below will be used to check the result data.frame.\n\ndistPair %>%\n  summary()\n\n      Var1             Var2             dist      \n MESZ01 :   332   MESZ01 :   332   Min.   :   50  \n RVSZ05 :   332   RVSZ05 :   332   1st Qu.: 7097  \n SRSZ01 :   332   SRSZ01 :   332   Median :11864  \n WISZ01 :   332   WISZ01 :   332   Mean   :12193  \n MUSZ02 :   332   MUSZ02 :   332   3rd Qu.:16388  \n MPSZ05 :   332   MPSZ05 :   332   Max.   :49894  \n (Other):108232   (Other):108232                  \n\n\nThe code chunk below is used to rename the origin and destination fields.\n\ndistPair <- distPair %>%\n  rename(orig = Var1,\n         dest = Var2)\n\nLastly, the code chunk below is used to save the dataframe for future use.\n\nwrite_rds(distPair, \"data/rds/distPair.rds\")"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#preparing-flow-data",
    "href": "In-class_Ex3/In-class_Ex3_2.html#preparing-flow-data",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.6 Preparing flow data",
    "text": "16.6 Preparing flow data\nThe code chunk below is used import od_data save in Chapter 15 into R environment.\nThere are 310 unique origin subzone values and 311 unique destin subzone values.\n\nod_data <- read_rds(\"data/rds/od_data.rds\")\n\nNext, we will compute the total passenger trip between and within planning subzones by using the code chunk below. The output is all flow_data.\n\nflow_data <- od_data %>%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %>% \n  summarize(TRIPS = sum(MORNING_PEAK)) \n\nhead(flow_data, 10)\n\n# A tibble: 10 × 3\n# Groups:   ORIGIN_SZ [1]\n   ORIGIN_SZ DESTIN_SZ TRIPS\n   <chr>     <chr>     <dbl>\n 1 AMSZ01    AMSZ01     2575\n 2 AMSZ01    AMSZ02    11742\n 3 AMSZ01    AMSZ03    14886\n 4 AMSZ01    AMSZ04     3237\n 5 AMSZ01    AMSZ05     9349\n 6 AMSZ01    AMSZ06     2231\n 7 AMSZ01    AMSZ07     1714\n 8 AMSZ01    AMSZ08     2624\n 9 AMSZ01    AMSZ09     2371\n10 AMSZ01    AMSZ10      183\n\n\n\n16.6.1 Separating intra-flow from passenger volume df\nCode chunk below is used to add three new fields in flow_data dataframe.\nTwo new fields called ‘FlowNoIntra’ and ‘offset’ are created.\n\nflow_data$FlowNoIntra <- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset <- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\nPrint\n\nhead(flow_data,3) %>% knitr::kable()\n\n\n\n\nORIGIN_SZ\nDESTIN_SZ\nTRIPS\nFlowNoIntra\noffset\n\n\n\n\nAMSZ01\nAMSZ01\n2575\n0\n1e-06\n\n\nAMSZ01\nAMSZ02\n11742\n11742\n1e+00\n\n\nAMSZ01\nAMSZ03\n14886\n14886\n1e+00\n\n\n\n\nglimpse(flow_data)\n\nRows: 20,987\nColumns: 5\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ   <chr> \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\"…\n$ DESTIN_SZ   <chr> \"AMSZ01\", \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06\"…\n$ TRIPS       <dbl> 2575, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 18…\n$ FlowNoIntra <dbl> 0, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 183, …\n$ offset      <dbl> 1e-06, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e…\n\n\n\n\n16.6.2 Combining passenger volume data with distance value\nBefore we can join flow_data and distPair, we need to convert data value type of ORIGIN_SZ and DESTIN_SZ fields of flow_data dataframe into factor data type.\n\nflow_data$ORIGIN_SZ <- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ <- as.factor(flow_data$DESTIN_SZ)\n\nNow, left_join() of dplyr will be used to flow_data dataframe and distPair dataframe. The output is called flow_data1.\nNotes:\ndistPair is a df containing distances for all corresponding subzone pairs (including self, default to 50m). ‘var1’, ‘var2’, ‘dist’\nflow_data is a df containing ‘origin_sz’, ‘destin_sb’ and ‘morning_peak’\nWe will now perform a left join with two sets join keys.\nThe output contains distance and total morning peak trips for each possible pairs of subzones (self included).\nBefore left join:\nflow_data has 20,987 rows.\ndistPair has 110,224 rows (is the all possible pairs out of 332 subzones, order matters and with replacement.)\nAfter join:\nflow_data1 has 20,987 rows.\nflow_data has no distance. flow_data1 has distance data.\n\nflow_data1 <- flow_data %>%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))\n\nglimpse(flow_data1)\n\nRows: 20,987\nColumns: 6\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ   <fct> AMSZ01, AMSZ01, AMSZ01, AMSZ01, AMSZ01, AMSZ01, AMSZ01, AM…\n$ DESTIN_SZ   <fct> AMSZ01, AMSZ02, AMSZ03, AMSZ04, AMSZ05, AMSZ06, AMSZ07, AM…\n$ TRIPS       <dbl> 2575, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 18…\n$ FlowNoIntra <dbl> 0, 11742, 14886, 3237, 9349, 2231, 1714, 2624, 2371, 183, …\n$ offset      <dbl> 1e-06, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e+00, 1e…\n$ dist        <dbl> 50.0000, 810.4491, 1360.9294, 840.4432, 1076.7916, 805.297…\n\n\nPrint out\n\nhead(flow_data1) %>% knitr::kable()\n\n\n\n\nORIGIN_SZ\nDESTIN_SZ\nTRIPS\nFlowNoIntra\noffset\ndist\n\n\n\n\nAMSZ01\nAMSZ01\n2575\n0\n1e-06\n50.0000\n\n\nAMSZ01\nAMSZ02\n11742\n11742\n1e+00\n810.4491\n\n\nAMSZ01\nAMSZ03\n14886\n14886\n1e+00\n1360.9294\n\n\nAMSZ01\nAMSZ04\n3237\n3237\n1e+00\n840.4432\n\n\nAMSZ01\nAMSZ05\n9349\n9349\n1e+00\n1076.7916\n\n\nAMSZ01\nAMSZ06\n2231\n2231\n1e+00\n805.2979"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#preparing-origin-and-destination-attributes",
    "href": "In-class_Ex3/In-class_Ex3_2.html#preparing-origin-and-destination-attributes",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.7 Preparing Origin and Destination Attributes",
    "text": "16.7 Preparing Origin and Destination Attributes\n\n16.7.1 Importing population data\n‘pop.csv’ is a processed version of ‘respopagesextod2011to2020.csv’ .\nThe original dataset used here is the Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2022 in csv format . This is an aspatial data file. It can be downloaded at Department of Statistics, Singapore, the specific link can be found here. Although it does not contain any coordinates values, but it’s ‘PA’ and ‘SZ’ fields can be used as unique identifiers to geocode to ‘PLAN_AREA_N’ and ‘SUBZONE_N’ of the MP14_SUBZONE_WEB_PL shapefile respectively.\n\npop <- read_csv(\"data/aspatial/pop.csv\")\nhead(pop)\n\n# A tibble: 6 × 5\n  PA         SZ                     AGE7_12 AGE13_24 AGE25_64\n  <chr>      <chr>                    <dbl>    <dbl>    <dbl>\n1 ANG MO KIO ANG MO KIO TOWN CENTRE     310      710     2780\n2 ANG MO KIO CHENG SAN                 1140     2770    15700\n3 ANG MO KIO CHONG BOON                1010     2650    14240\n4 ANG MO KIO KEBUN BAHRU               1050     2390    12460\n5 ANG MO KIO SEMBAWANG HILLS            420     1120     3620\n6 ANG MO KIO SHANGRI-LA                 810     1920     9650\n\n\nWhy is the data prepared in this way?\nAge group 7-12: Feeder bus to send kids to school\nAge group 13-24: Feeder bus for secondary / JC / ITE/ poly students to school.\nInteresting observation: When we examine the flow map in Hands-on_Ex3, the top few flow movement by bus have their destinations at Republic Poly in woodlands, AMK central ITE along AMK ave 5 for instance.\n\n\n16.7.2 Geospatial data wrangling\nPOP + MPSZ\nLet us append the zone codes in mpsz df to the pop’s population data by age groups. We do not really need to geometry data.\npop has 332 rows\nmpsz has 332 rows\nAfter join: 984,656 rows\nColumn selected are ‘PA’, ‘SZ’, ‘AGE7-12’, ‘AGE13-24’, ‘AGE25_64’ from pop df and ‘SUBZONE_C’ from mpsz df.\n\npop <- pop %>%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %>%\n  select(1:6) %>%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\nhead(pop)\n\n# A tibble: 6 × 6\n  PA         SZ_NAME                AGE7_12 AGE13_24 AGE25_64 SZ    \n  <chr>      <chr>                    <dbl>    <dbl>    <dbl> <chr> \n1 ANG MO KIO ANG MO KIO TOWN CENTRE     310      710     2780 AMSZ01\n2 ANG MO KIO CHENG SAN                 1140     2770    15700 AMSZ02\n3 ANG MO KIO CHONG BOON                1010     2650    14240 AMSZ03\n4 ANG MO KIO KEBUN BAHRU               1050     2390    12460 AMSZ06\n5 ANG MO KIO SEMBAWANG HILLS            420     1120     3620 AMSZ07\n6 ANG MO KIO SHANGRI-LA                 810     1920     9650 AMSZ05\n\n\n\n\n16.7.3 Preparing origin attribute\nFLOW_DATA1 + POP\nWe would like to append the origin’s population data from pop to flow_data1 that contains(1) origin -destination pair, (2) actual flows and (3) distance information.\n\nflow_data1 <- flow_data1 %>%\n  left_join(pop,\n            by = c(ORIGIN_SZ = \"SZ\")) %>%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %>%\n  select(-c(PA, SZ_NAME))\n\nMorning peak: the push factor should be the population from origin population distribution.\nEvening peak : the pull factor would be the population too.\nLimits of our model: transfer trips not accounted for.\n\n\n16.7.4 Preparing destination attribute\nSimilarly, we want to get the destination’s population data by destination from pop. Once again, perform a left join.\n\nflow_data1 <- flow_data1 %>%\n  left_join(pop,\n            by = c(DESTIN_SZ = \"SZ\")) %>%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %>%\n  select(-c(PA, SZ_NAME))\n\nWe will called the output data file SIM_data. it is in rds data file format.\n\nwrite_rds(flow_data1, \"data/rds/SIM_data\")"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#calibrating-spatial-interaction-models",
    "href": "In-class_Ex3/In-class_Ex3_2.html#calibrating-spatial-interaction-models",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "16.8 Calibrating Spatial Interaction Models",
    "text": "16.8 Calibrating Spatial Interaction Models\nIn this section, you will learn how to calibrate Spatial Interaction Models by using Poisson Regression method.\n\n16.8.1 Importing the modelling data\nFirstly, let us import the modelling data by using the code chunk below.\n\nSIM_data <- read_rds(\"data/rds/SIM_data.rds\")\n\n\n\n16.8.2 Visualising the dependent variable\nFirstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.\n\nggplot(data = SIM_data,\n       aes(x = TRIPS)) +\n  geom_histogram()\n\n\n\n\nNotice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.\nNext, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.\n\nggplot(data = SIM_data,\n       aes(x = dist,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nNotice that their relationship hardly resemble linear relationship.\nOn the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship is more resemble linear relationship.\n\nggplot(data = SIM_data,\n       aes(x = log(dist),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nWe have come to the end of our data preparation stage.\n\n\n16.8.3 Checking for variables with zero values\nFeature engineering state starts here: We need to make our data able to work for our chosen algorithm (Poisson regression).\nSince Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.\nIn the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in SIM_data data frame.\n\n\n16.8.4 Unconstrained Spatial Interaction Model\nIn this section, we will learn how to calibrate an unconstrained spatial interaction model by using glm() of Base Stats. The explanatory variables are origin population by different age cohort, destination population by different age cohort (i.e. ORIGIN_AGE25_64) and distance between origin and destination in km (i.e. dist).\nThe general formula of Unconstrained Spatial Interaction Model\n\nThe code chunk used to calibrate to model is shown below:\n\n\n\nThe parameter estimate for distnace is -1.517. If the sign is positive, double-check our workings.\nAIC: GLM by default do not provide R-square, only provide AIC.\nCompare models:\nDoubly constrained: best model as not dispersed. 60% accuracy because we only have one/two variables if we add more variables like job opportunities.\n\n#| eval: false\n#| echo: false\n#| fig-width: 14\n#| fig-asp: 0.68\n#| code-fold: True"
  },
  {
    "objectID": "In-class_Ex3/In-class_Ex3_2.html#summaries",
    "href": "In-class_Ex3/In-class_Ex3_2.html#summaries",
    "title": "In-class Exercise 3: Calibrating Spatial Interaction Models with R",
    "section": "Summaries",
    "text": "Summaries\nOD matrix is often incomplete. Imagine trying to complete the OD matrix, it would involve us doing spatial interaction or OD surveys to find the missing values. There are 332 subzones in Singapore, and each survey is expensive,. In addition, OD matrix is constantly changing as flow patterns changes. We are trying to predict flows between origins and destinations. Flow could be thought of a function of (1) attribute of origin (propulsiveness) (2) attribute of destination (attractiveness) and (3) cost friction (like distance or transport cost or public transport stops). Assumption is that the benefits must outweigh the cost in order for flow to happen.\nGravity model takes into consideration the interaction between all origin and destination locations.\nPotential model takes in consideration the interaction between a location and all other location pairs. (Good for measuring accessibility)\nRetail model is commonly used by franchise like KFC / Pizza Hut to determine their area/region of service (aka delivery zones) for each outlet.\nThere are 4 variations in the Gravity model:\n\nUnconstrained: only the overall outflow is fixed and total outflow from origins = total inflow to destinations\nOrigin constrained: outflow by origin is fixed.\nDestination constrained: inflow by destination is fixed.\nDoubly constrained: outflow by origin and inflow by destination is fixed.\n\nTo calculate flow from each origin to each destination, we need parameters like k, alpha, lambda and beta. The beta for distance is usually negative because we assume that there is an inverse relationship between interaction and distance, like Newtonian physics and laws of gravity."
  },
  {
    "objectID": "In-class_Ex4/In-class_Ex4.html",
    "href": "In-class_Ex4/In-class_Ex4.html",
    "title": "In-class Exercise 4:",
    "section": "",
    "text": "#pacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex5/In-class_Ex5.html",
    "href": "In-class_Ex5/In-class_Ex5.html",
    "title": "In-class Exercise 5:",
    "section": "",
    "text": "#pacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624_AGA",
    "section": "",
    "text": "Welcome to my Applied Geospatial Analysis Website.\nI will be sharing my learning journey with you."
  },
  {
    "objectID": "Take-home_Ex1/data/geospatial/hexagon/hexagon.html",
    "href": "Take-home_Ex1/data/geospatial/hexagon/hexagon.html",
    "title": "NYX Geospatial App",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  },
  {
    "objectID": "Take-home_Ex1/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "href": "Take-home_Ex1/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "title": "NYX Geospatial App",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "",
    "text": "The increasing digitization of urban infrastructures, such as public transportation and utilities, generates vast datasets using technologies like GPS and RFID. These datasets offer valuable insights into human movement patterns within a city, especially with the widespread deployment of smart cards and GPS devices in vehicles. However, current practices often limit the use of this data to basic tracking and mapping with GIS applications, primarily because conventional GIS lacks advanced functions for analyzing and modeling spatial and spatio-temporal data effectively. Enhanced analysis of these datasets could significantly contribute to better urban management and informed decision-making for both public and private urban transport services providers."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#objectives-of-take-home_ex1",
    "href": "Take-home_Ex1/Take-home_Ex1.html#objectives-of-take-home_ex1",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "Objectives of Take-home_Ex1",
    "text": "Objectives of Take-home_Ex1\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, I will perform\n\nGeovisualisation of passengers trips by four different time intervals\n\nWith reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the geographical distribution of the passenger trips by using appropriate geovisualisation methods,\nDescribe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).\n\nApply appropriate Local Indicators of Spatial Association (GLISA) to undercover the spatial mobility patterns of public bus passengers in Singapore.\n\nCompute LISA of the passengers trips generate by origin at hexagon level.\nDisplay the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value < 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 200 words per visual)."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#the-data",
    "href": "Take-home_Ex1/Take-home_Ex1.html#the-data",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "The Data",
    "text": "The Data\n\nAspatial data\nFor this take-home exercise, Passenger Volume by Origin Destination Bus Stops downloaded from LTA DataMall will be used. It contains the number of trips by weekdays and weekends from origin to destination bus stops.\nFor this exercise, the data is collected in August 2023.\nThe fields are YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR, PT_TYPE , ORIGIN_PT_CODE, DESTINATION_PT_CODE, TOTAL_TRIPS .\nA sample row for bus dataset could be ‘2023-08, WEEKDAY, 16, BUS, 28299, 28009, 63’. TIME_PER_HOUR of 16 represents data is collected between 4 pm to 5pm.\n\n\nGeospatial data\nTwo geospatial data will be used in this study, they are:\n\nBus Stop Location from LTA DataMall’s static dataset. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\nhexagon, a hexagon layer of 250m (perpendicular distance between the centre of the hexagon and its edges.) Each spatial unit is regular in shape and finer than the Master Plan 2019 Planning Sub-zone GIS data set of URA."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "href": "Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "Getting Started",
    "text": "Getting Started\nIn this exercise, the following libraries will be used:\n\nsf packageto perform geoprocessing tasks\nsfdep package which builds upon spdep package (compute spatial weights matrix and spatially lagged variable for instance..)\ntmap to create geovisualisations\ntidyverse that supports data science, analysis and communication task including creating static statistical graphs.\nknitr to create html tables\nDT library to create interactive html tables\nggplot2 to plot graphs\nplotly to plot interactive graphs\\\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr, DT, ggplot2, plotly, h3jsr, skimr)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#importing-data",
    "href": "Take-home_Ex1/Take-home_Ex1.html#importing-data",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "Importing Data",
    "text": "Importing Data\n\nAspatial data\nImport the Passenger volume by Origin Destination Bus Stops dataset downloaded from the LTA Datamall by using the read_csv() of the readr package.\n\nodbus_aug <- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\nCheck the data fields\n\nglimpse(odbus_aug)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          <chr> \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            <chr> \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       <dbl> 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             <chr> \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      <chr> \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE <chr> \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         <dbl> 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nProcessing the aspatial OD data\nThe ‘ORIGIN_PT_CODE’ and ‘DESTINATION_PT_CODE’ field is in character field. We will convert it to factor data type.\n\nodbus_aug$ORIGIN_PT_CODE <- as.factor(odbus_aug$ORIGIN_PT_CODE)\nodbus_aug$DESTINATION_PT_CODE <- as.factor(odbus_aug$DESTINATION_PT_CODE)\n\nThe function below will extract origin data based on the four time intervals required by the task. The expected arguments are\n\ndaytype: ‘WEEKDAY’ or ‘WEEKENDS/HOLIDAY’\ntimeinterval: c(8,10) if we want data from 8am to 11am.\n\nThe function will also compute the sum of all trips by ‘ORIGIN_PT_CODE’ for each time interval and stored under a new field called ‘TRIPS’.\n\nget_origin <- function(daytype, timeinterval) {\n  result <- odbus_aug %>%\n    filter(DAY_TYPE == daytype) %>%\n    filter(TIME_PER_HOUR >= timeinterval[1] & TIME_PER_HOUR <= timeinterval[2]) %>%\n    group_by(ORIGIN_PT_CODE) %>%\n    summarise(TRIPS = sum(TOTAL_TRIPS))\n  \n  return(result)\n}\n\nLet’s get the data using get_origin function\n\norigin_day_am <- get_origin('WEEKDAY', c(6, 8))\norigin_day_pm <- get_origin('WEEKDAY', c(5, 7))\n\n\norigin_end_am <- get_origin('WEEKENDS/HOLIDAY', c(11, 13))\norigin_end_pm <- get_origin('WEEKENDS/HOLIDAY', c(4, 6))\n\nTake a look at overview of all the four dataframes.\n\nWD morning peakWD afternoon peakWE morning peakWE afternoon peak\n\n\n\nglimpse(origin_day_am)\n\nRows: 5,005\nColumns: 2\n$ ORIGIN_PT_CODE <fct> 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ TRIPS          <dbl> 1425, 683, 1146, 1707, 1752, 1106, 115, 5827, 6847, 257…\n\n\n\n\n\nglimpse(origin_day_pm)\n\nRows: 4,965\nColumns: 2\n$ ORIGIN_PT_CODE <fct> 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ TRIPS          <dbl> 752, 350, 436, 832, 823, 637, 305, 2918, 5127, 1091, 28…\n\n\n\n\n\nglimpse(origin_end_am)\n\nRows: 4,994\nColumns: 2\n$ ORIGIN_PT_CODE <fct> 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ TRIPS          <dbl> 1546, 1203, 1156, 2373, 3637, 806, 73, 10074, 5624, 446…\n\n\n\n\n\nglimpse(origin_end_pm)\n\nRows: 4,623\nColumns: 2\n$ ORIGIN_PT_CODE <fct> 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ TRIPS          <dbl> 119, 51, 45, 71, 138, 80, 66, 240, 275, 86, 395, 18, 78…\n\n\n\n\n\n\n\n\nGeospatial data\nFirst, we will import the Bus Stop Location shapefiles into R. The output will be a sf point object with 5161 points and 3 fields. As the raw data is in WSG84 geographical coordinate system, we will convert it to EPSG 3414, the projected coordinate system for Singapore.\n\nbusstop <- st_read(dsn=\"data/geospatial/BusStopLocation/BusStopLocation_Jul2023\", layer = \"BusStop\") %>% \n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\yixin-neo\\ISSS624_AGA\\Take-home_Ex1\\data\\geospatial\\BusStopLocation\\BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\nbusstop\n\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   BUS_STOP_N BUS_ROOF_N             LOC_DESC                  geometry\n1       22069        B06   OPP CEVA LOGISTICS POINT (13576.31 32883.65)\n2       32071        B23         AFT TRACK 13 POINT (13228.59 44206.38)\n3       44331        B01              BLK 239  POINT (21045.1 40242.08)\n4       96081        B05 GRACE INDEPENDENT CH POINT (41603.76 35413.11)\n5       11561        B05              BLK 166 POINT (24568.74 30391.85)\n6       66191        B03         AFT CORFE PL POINT (30951.58 38079.61)\n7       23389       B02A              PEC LTD   POINT (12476.9 32211.6)\n8       54411        B02              BLK 527 POINT (30329.45 39373.92)\n9       28531        B09              BLK 536 POINT (14993.31 36905.61)\n10      96139        B01              BLK 148  POINT (41642.81 36513.9)\n\n\nAre there any duplicates in ‘BUS_STOP_N’ in busstop?\nChecking for duplicates in the ‘BUS_STOP_N’ field reveals that there are about 16 repeated bus stop numbers. However, they have different geometry points in the simple feature busstop object above. These could be due to temprorary bus stops . We should retain all these rows as they might have different hexagon ‘fid’ values later.\n\nbusstop %>% \n  st_drop_geometry() %>% \n  group_by(BUS_STOP_N) %>%\n  filter(n()>1) %>%\n  ungroup() %>% \n  arrange(BUS_STOP_N)\n\n# A tibble: 32 × 3\n   BUS_STOP_N BUS_ROOF_N LOC_DESC            \n   <chr>      <chr>      <chr>               \n 1 11009      B04        Ghim Moh Ter        \n 2 11009      B04-TMNL   GHIM MOH TER        \n 3 22501      B02        Blk 662A            \n 4 22501      B02        BLK 662A            \n 5 43709      B06        BLK 644             \n 6 43709      B06        BLK 644             \n 7 47201      UNK        <NA>                \n 8 47201      NIL        W'LANDS NTH STN     \n 9 51071      B21        MACRITCHIE RESERVOIR\n10 51071      B21        MACRITCHIE RESERVOIR\n# ℹ 22 more rows\n\n\nTake for instance bus stop number 51071 with two different point geometry values.\n\nbusstop[3470,]\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28311.27 ymin: 36036.92 xmax: 28311.27 ymax: 36036.92\nProjected CRS: SVY21 / Singapore TM\n     BUS_STOP_N BUS_ROOF_N             LOC_DESC                  geometry\n3470      51071        B21 MACRITCHIE RESERVOIR POINT (28311.27 36036.92)\n\n\n\nbusstop[3472,]\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28282.54 ymin: 36033.93 xmax: 28282.54 ymax: 36033.93\nProjected CRS: SVY21 / Singapore TM\n     BUS_STOP_N BUS_ROOF_N             LOC_DESC                  geometry\n3472      51071        B21 MACRITCHIE RESERVOIR POINT (28282.54 36033.93)\n\n\n\nCreating hexagon layer\nBefore we can plot the base layer, we have to create hexagonal grids of 250 m using the busstop points data using sf.\nFirst , create a grid which the extent equals to the bounding box of the busstop points using st_make_grid().\n\nTo create hexagons of 250m (centre to edge), we should input 500 for ‘cellsize’ parameter. ‘Cellsize’ is defined as the distance from edge to edge.\nConvert to sf object and add a unique identifier to each hexagon grid. The output has 5580 hexagon units.\nUse st_intersects() to count the number of bus stops in each hexagon.\nFilter to retain only hexagons with at least one bus stop. The output bs_count has 1524 hexagon units.\n\n\narea_hex_grid = st_make_grid(busstop,\n                             cellsize= 500, \n                             what = \"polygons\", \n                             square = FALSE)\n\nhex_grid_sf = st_sf(area_hex_grid) %>%\n  mutate(grid_id = 1:length(lengths(area_hex_grid)))\n\nhex_grid_sf$bs = lengths(st_intersects(hex_grid_sf, busstop))\n\n\nbs_count = filter(hex_grid_sf, bs > 0)\nbs_count\n\nSimple feature collection with 1524 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3720.122 ymin: 26193.43 xmax: 48720.12 ymax: 53184.55\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                    area_hex_grid grid_id bs\n1  POLYGON ((3970.122 27925.48...      34  1\n2  POLYGON ((4220.122 28358.49...      65  1\n3  POLYGON ((4470.122 30523.55...      99  1\n4  POLYGON ((4720.122 28358.49...     127  1\n5  POLYGON ((4720.122 30090.54...     129  2\n6  POLYGON ((4720.122 30956.57...     130  1\n7  POLYGON ((4720.122 31822.59...     131  1\n8  POLYGON ((4970.122 28791.5,...     159  1\n9  POLYGON ((4970.122 29657.53...     160  1\n10 POLYGON ((4970.122 30523.55...     161  2\n\n\nHow does our hexagon layer look like?\n\nplot(bs_count['bs'],\n     main = 'Count of bus stops at hexagon level')\n\n\n\n\nHow does the distribution of bus stops in hexagon look like?\n\nq <- quantile(bs_count$bs, probs = c(0.25, 0.5, 0.75))\n\nggplot(data=bs_count,\n  aes(x=bs)) +\n  geom_histogram(binwidth = 1, color='black',size= 0.3, fill = '#DD8888') +\n  geom_vline(xintercept = q[2], linetype='dotted', size = 0.8, color='blue') +\n  geom_vline(xintercept = q[3], linetype='dotted', size = 0.8) +\n  annotate('text' , x= 2.3, y=450, label='50th \\npercentile', size = 3) +\n  annotate('text' , x= 4.7, y=450, label='75th \\npercentile', size = 3) +\n  labs(y= 'Count', x='Number of bus stops') +\n  theme(axis.title.y=element_text(angle = 0)) +\n  ggtitle('Distribution of Bus stops')\n\n\n\n\nThe plot above shows that distribution of the bus stop is right-skewed. Within a hexagon of 250m from the centre to edge, the median number of bus stops is about 3.\nIf we like to, we could save the above hexagon layer to disk\n\nst_write(bs_count, 'data/bs_count.shp')\n\n\n\n\n\n\nGeospatial data wrangling\n\nCombining busstop (polygon sf) and bs_count (point sf)\nWe need to get the corresponding bus stop numbers in each hexagon in bs_count hexagon layer.\nThe code chunk below performs points and hexagon polygon overlap using st_intersection() and the output will be in sf point object.\nBefore overlapping:\nbusstop : 5161 points\nbs_count hexagon base layer: 1524 hexagons\nAfter overlapping:\nbusstop_hex : 5161 points and this is good results because all the bus stop location data is mapped to our bs_count base map.\n\nbusstop_hex <- st_intersection(busstop, bs_count) %>% \n  select(BUS_STOP_N, BUS_ROOF_N, LOC_DESC,  grid_id, bs)\n\nbusstop_hex\n\nSimple feature collection with 5161 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     BUS_STOP_N BUS_ROOF_N            LOC_DESC grid_id bs\n3269      25059        UNK   AFT TUAS STH BLVD      34  1\n2570      25751       B02D BEF TUAS STH AVE 14      65  1\n254       26379        NIL            YONG NAM      99  1\n2897      25761        B03      OPP REC S'PORE     127  1\n2827      25719       B01C           THE INDEX     129  2\n4203      26389        NIL  BEF TUAS STH AVE 5     129  2\n2403      26369        NIL        SEE HUP SENG     130  1\n1565      26299        B13  BEF TUAS STH AVE 6     131  1\n2829      25741        B03         HALLIBURTON     159  1\n2828      25711       B02C       OPP THE INDEX     160  1\n                      geometry\n3269 POINT (3970.122 28063.28)\n2570 POINT (4427.939 28758.67)\n254  POINT (4473.292 30681.57)\n2897 POINT (4737.082 28621.29)\n2827 POINT (4799.476 30158.46)\n4203 POINT (4776.694 30324.88)\n2403 POINT (4604.363 31212.96)\n1565 POINT (4879.489 31948.93)\n2829  POINT (5060.733 29212.4)\n2828 POINT (4831.438 30132.43)\n\n\nWe will drop the geometry because busstop_hex is a POINT sf object, there is no hex polygon geometry data for us to plot based on hexagon level. Furthermore, we have to process the attribute data. To get back the hexagon POLYGON geometry data, we can always left_join() bs_count df with our attribute table again later.\nWe will also use datatable() function of the DT library to print the data. The table is interactive and can perform basic sorting.\n\nbusstop_hex <- busstop_hex  %>% \n  st_drop_geometry()\n\ndatatable(busstop_hex, class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\nChecking for duplicates in busstop_hex\nLet us check for duplicates in busstop_hex df as it will be used to perform a left join later.\nThe output shows that there are 11 duplicated rows.\n\nbusstop_hex %>%\n  group_by(BUS_STOP_N, grid_id, bs) %>%\n  filter(n()>1) %>%\n  ungroup()\n\n# A tibble: 22 × 5\n   BUS_STOP_N BUS_ROOF_N LOC_DESC        grid_id    bs\n   <chr>      <chr>      <chr>             <int> <int>\n 1 22501      B02        Blk 662A           1251     8\n 2 22501      B02        BLK 662A           1251     8\n 3 43709      B06        BLK 644            1904     7\n 4 43709      B06        BLK 644            1904     7\n 5 47201      UNK        <NA>               2381     2\n 6 47201      NIL        W'LANDS NTH STN    2381     2\n 7 11009      B04        Ghim Moh Ter       2395     7\n 8 11009      B04-TMNL   GHIM MOH TER       2395     7\n 9 58031      UNK        OPP CANBERRA DR    2939     7\n10 58031      UNK        OPP CANBERRA DR    2939     7\n# ℹ 12 more rows\n\n\nRemoval of duplicates\n\nbusstop_hex <- busstop_hex %>%\n  distinct(BUS_STOP_N, grid_id, bs, .keep_all = TRUE)\n#busstop_hex <- unique(busstop_hex)\n\nCheck again to be sure. All’s good.\n\nbusstop_hex[duplicated(busstop_hex), ]\n\n[1] BUS_STOP_N BUS_ROOF_N LOC_DESC   grid_id    bs        \n<0 rows> (or 0-length row.names)\n\n\nNote that there will be repeated bus stop ids with different bus stop roof number / hexagon id.\n\nbusstop_hex['BUS_STOP_N'][duplicated(busstop_hex['BUS_STOP_N']), ]\n\n[1] \"53041\" \"52059\" \"68091\" \"68099\" \"67421\"\n\n\n\n\nCombining each of the four aspatial dataframes with busstop_hex\nThe function below performs left outer join for each of the four aspatial origin dataframes with busstop_hex dataframe.\nThe expected argument:\nasp_df: name of aspatial origin dataframe like origin_day_am or origin_day_pm etc..\nThe function will also rename ‘ORIGIN_PT_CODE’ and ‘fid’ fields to ‘ORIGIN_BS’ and ‘ORIGIN_HEXFID’ respectively.\n\nleftjoin <- function(asp_df) {\n  result <- left_join(asp_df, busstop_hex,\n                         by = c('ORIGIN_PT_CODE' = 'BUS_STOP_N')) %>%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_HEXFID = grid_id) %>% \n  select(ORIGIN_HEXFID, \n         ORIGIN_BS, \n         TRIPS,\n         BUS_ROOF_N,\n         LOC_DESC)\n  \n  return(result)\n}\n\nNow, use the ‘leftjoin’ function to get our dataframes containing grid_id and origin bus stop ids.\n\norigin_data_day_am <- leftjoin(origin_day_am) # 5005 to 5010 rows\norigin_data_day_pm <- leftjoin(origin_day_pm) # 4965 to 4970 rows\norigin_data_end_am <- leftjoin(origin_end_am) # 4994 to 4999 rows\norigin_data_end_pm <- leftjoin(origin_end_pm) # 4623 to 4627 rows\n\nThe number of rows of each df increased by 4-5 after left join , this is due to the duplicated 5 bus stops ids , each with two distinct bus stop roofs and located in different hexagon ids, that originated from the busstop and busstop_hex dataframes earlier.\nAgain, double check for duplicates\n\norigin_data_day_am['ORIGIN_HEXFID', 'ORIGIN_BS'][duplicated(origin_data_day_am['ORIGIN_HEXFID', 'ORIGIN_BS']), ]\n\n# A tibble: 0 × 1\n# ℹ 1 variable: ORIGIN_BS <chr>\n\norigin_data_day_pm['ORIGIN_HEXFID', 'ORIGIN_BS'][duplicated(origin_data_day_pm['ORIGIN_HEXFID', 'ORIGIN_BS']), ]\n\n# A tibble: 0 × 1\n# ℹ 1 variable: ORIGIN_BS <chr>\n\norigin_data_end_am['ORIGIN_HEXFID', 'ORIGIN_BS'][duplicated(origin_data_end_am['ORIGIN_HEXFID', 'ORIGIN_BS']), ]\n\n# A tibble: 0 × 1\n# ℹ 1 variable: ORIGIN_BS <chr>\n\norigin_data_end_pm['ORIGIN_HEXFID', 'ORIGIN_BS'][duplicated(origin_data_end_pm['ORIGIN_HEXFID', 'ORIGIN_BS']), ]\n\n# A tibble: 0 × 1\n# ℹ 1 variable: ORIGIN_BS <chr>\n\n\nBus stops with more than 1 bus roof number and exists in more than 1 hexagon in the tibble below.\n\norigin_data_day_am['ORIGIN_BS'][duplicated(origin_data_day_am['ORIGIN_BS']), ]\n\n# A tibble: 5 × 1\n  ORIGIN_BS\n  <chr>    \n1 52059    \n2 53041    \n3 67421    \n4 68091    \n5 68099    \n\n\nCheck for bus stops without any hexagon id\nUsing the skim() from the skimr package reveals that there are about 44-52 missing ‘ORIGIN_HEXFID’ values in each of our dataframes. (see Summary table below).\nThe code chunk below will\n\nfilter character or numeric field\nfilter for fields with missing values\nselect required columns\nconvert output to tibble df\nrename the column name (static and dynamically)\ncombine all the output in a summary tibble df.\n\nSummary table of number of missing ‘ORIGIN_HEXFID’ in each of the four dataframes.\n\n\nShow the code\n#str(skim(origin_data_day_am))\n\nget_na_hex <- function(df, col2header) {\n  result <- skimr::skim(df) %>%\n  filter(skim_type == \"character\" | skim_type == \"numeric\") %>%\n  filter(n_missing > 0) %>%\n  select(skim_variable, n_missing)%>% \n  as_tibble() %>% \n  rename_with(~col2header, n_missing, .cols = c(n_missing)) %>%\n  rename(Variable = skim_variable)\n  \n  return(result)\n}\n\n\nr1 <- get_na_hex(origin_data_day_am, 'Wkday_morn') \nr2 <- get_na_hex(origin_data_day_pm, 'Wkday_aft') \nr3 <- get_na_hex(origin_data_end_am, 'Wkend_morn') \nr4 <- get_na_hex(origin_data_end_pm, 'Wkend_aft') \n\nbind_cols(r1,r2,r3,r4) %>% \n  select(1,2,4,6,8) %>% \n  slice(3) %>% kable()\n\n\n\n\n\nVariable…1\nWkday_morn\nWkday_aft\nWkend_morn\nWkend_aft\n\n\n\n\nORIGIN_HEXFID\n52\n50\n49\n44\n\n\n\n\n\nThe reason for some bus stop ids without a hexagon id could be that the Bus Stop Location file is outdated (last update in July 2023) while the Passenger Volume by Origin Destination Bus Stops contains data in August 2023.\nBus stop ids without hexagon id\n\nWkday MorningWkday AfternoonWkend MorningWkend Afternoon\n\n\n\n\nShow the code\ndatatable(origin_data_day_am %>%\n  filter(is.na(ORIGIN_HEXFID)), \n  class = 'cell-border stripe', \n  options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndatatable(origin_data_day_pm %>%\n  filter(is.na(ORIGIN_HEXFID)), \n  class = 'cell-border stripe', \n  options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndatatable(origin_data_end_am %>%\n  filter(is.na(ORIGIN_HEXFID)), \n  class = 'cell-border stripe', \n  options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndatatable(origin_data_end_pm %>%\n  filter(is.na(ORIGIN_HEXFID)), \n  class = 'cell-border stripe', \n  options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\nAggregating total trips and average daily trips by hexagon level\nSince we are plotting the number of passenger trips generated by hexagon level, we should aggregate the total trips by ‘ORIGIN_HEXFID’ and store these values in a new field called ‘TTRIPS’.\nIn addition, a new field ‘AVG_TTRIPS’ is calculated where it represents number of average weekday trip (=TTRIPS / 22) and average weekend trips (=TTRIPS/9). There are 22 weekdays , 8 days of weekends and 1 public holiday in August 2023.\nAfter the left join, the total distinct of hexagons for each time intervals are 1491, 1489, 1499 and 1444. The total hexagon in bs_count was 1524. This suggests that were some bus stops with no passengers at different time intervals.\n\nget_ttrips_wkday <- function(df) {\n  result <- df %>% \n    group_by(ORIGIN_HEXFID) %>% \n    summarise(\n      TTRIPS = sum(TRIPS),\n      AVG_TRIPS = ceiling(sum(TRIPS) / 22),\n      DESC = paste(LOC_DESC, collapse = ', ')\n    ) %>% \n    ungroup()\n  \n  return(result)\n}\n\nget_ttrips_wkend <- function(df) {\n  result <- df %>% \n    group_by(ORIGIN_HEXFID) %>% \n    summarise(\n      TTRIPS = sum(TRIPS),\n      AVG_TRIPS = ceiling(sum(TRIPS) / 9),\n      DESC = paste(LOC_DESC, collapse = ', ')\n    ) %>% \n    ungroup()\n  \n  return(result)\n}\n\norigin_data_day_am_hex <- get_ttrips_wkday(origin_data_day_am) # 5010 to 1491 rows\norigin_data_day_pm_hex <- get_ttrips_wkday(origin_data_day_pm) # 4970 to 1489 rows\norigin_data_end_am_hex <- get_ttrips_wkend(origin_data_end_am) # 4999 to 1499 rows\norigin_data_end_pm_hex <- get_ttrips_wkend(origin_data_end_pm) # 4627 to 1444 rows\n\n\n\n\nThe four dataframes above will be printed out in the later sections.\n\n\nRetrieve hexagon geometry coordinates\nIn order to plot choropleth maps, we need the geometry data from bs_countsf polygon object.\nThe function below performs a left join with bs_count and the attributes dataframes and also assign 0 to ‘TTRIPS’ and ‘AVG_TRIPS’ value in a hexagon without any passengers.\n\nget_hexgeo <- function(df) {\n  result <- left_join(bs_count, df,\n                      by = c('grid_id' = 'ORIGIN_HEXFID')) %>%\n    mutate(TTRIPS = if_else(is.na(TTRIPS),0, TTRIPS),\n           AVG_TRIPS = if_else(is.na(AVG_TRIPS), 0, AVG_TRIPS))\n    \n  return(result)\n}\n\nday_am <- get_hexgeo(origin_data_day_am_hex)\nday_pm <- get_hexgeo(origin_data_day_pm_hex)\nend_am <- get_hexgeo(origin_data_end_am_hex)\nend_pm <- get_hexgeo(origin_data_end_pm_hex)"
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#task-1-geovisulisation-and-analysis",
    "href": "Take-home_Ex1/Take-home_Ex1.html#task-1-geovisulisation-and-analysis",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "Task 1: Geovisulisation and Analysis",
    "text": "Task 1: Geovisulisation and Analysis\nIn this section , the choropleth maps will be used to show the geographical distribution of the passenger trips by origin. I will be using ‘AVG_TRIPS’ field to plot instead of ‘’TTRIPS’ so that we can compare fairly on a daily basis.\nPlanning for map classification\nTo plan for the classification for the maps, we could check the distribution of the ‘AVG_TRIPS’ field across ALL four time intervals.\nFIrst, combine all the AVG_‘TRIPS’ and create a new column ‘source’ to retain the name of the time interval.\n\norigin_data_day_am_hex$source <- 'Wkday_am'\norigin_data_day_pm_hex$source <- 'Wkday_pm'\norigin_data_end_am_hex$source <- 'Wkend_am'\norigin_data_end_pm_hex$source <- 'Wkend_pm'\n\ncombine <- rbind(origin_data_day_am_hex,\n                 origin_data_day_pm_hex,\n                 origin_data_end_am_hex,\n                 origin_data_end_pm_hex)\n\n\n\n\n\n\n\nBoxplots\nPlot boxplots to get a general sensing of the distribution of ‘AVG_TRIPS’ across different time intervals. Hover the mouse above each point will show the average daily trips generated in each hexagon.\n\n\nShow the code\nggplotly(ggplot(data=combine, \n       aes(y=AVG_TRIPS,\n           x=source)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             size=2) + \n  labs(x = \"Time Intervals\", y = \"Total Trips\", title='Daily average origin trips by time intervals') +\n  theme_minimal() +\n  theme(legend.key.size = unit(0.5,'cm'),\n        legend.position=\"bottom\",\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        axis.title = element_text(size = 11 , face = \"bold\"),\n        axis.text = element_text(size = 10),\n        axis.text.x = element_text(angle = 0, hjust = 1)))\n\n\n\n\n\n\nFrom the chart above, the weekday average daily ridership severely outweighs the weekend period.\n\n\n\nSummary statistics\nFinal check on the summary statistics of TTRIPS before setting custom break points.\n\nsummary(combine$AVG_TRIPS)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    19.0   122.0   411.1   473.0 15321.0 \n\n\nOutliers\nBecause of the large number of outliers bus stops, let us calculate the upper bound of the outliers.\n\nQ1 <- summary(combine$AVG_TRIPS)[2]\nQ3 <- summary(combine$AVG_TRIPS)[5]\nIQR_value <- Q3 - Q1\n\nlower_bound <- Q1 - 1.5 * IQR_value\nupper_bound <- Q3 + 1.5 * IQR_value\n\n#lower_bound\nupper_bound\n\n3rd Qu. \n   1154 \n\n\nSetting custom breaks\nWith reference to the box plot, summary statistics and upper bound values,\n\nMin : 1 , Max : 13,321\nquantile break points can be 20, 120, 470\nbreak vector is therefore be c(0, 20, 120, 470, 1154, 10000, 13321)\n\nThe first three intervals reflects the 1st -3rd quantiles. The last two intervals would distinguish the outliers and extreme outliers (bus stops with very high daily passengers number) on the map later.\n\nplotmap <- function(df, title) {\n  df2<- df %>% top_n(10, AVG_TRIPS)\n  \n  result <- tm_shape(df)+\n    tm_fill(\"AVG_TRIPS\", \n            breaks =  c(0, 20, 120, 470, 1154, 10000, 13321), #style = \"quantile\", \n            palette = \"Blues\",\n            alpha= 0.5,\n            #legend.hist = TRUE, \n            #legend.is.portrait = TRUE,\n            #legend.hist.z = 0.3,\n\n            title = \"Passengers Trip\") +\n    tm_layout(main.title = title,\n              main.title.position = \"center\",\n              main.title.size = 1.2,\n              legend.height = 0.45, \n              legend.width = 0.35,\n              #legend.outside = TRUE,\n              #legend.text.size= 0.6,\n              #inner.margins = c(0.01, 0.01, 0, .15),\n              #legend.position = c(\"right\", \"top\"),\n              #bg.color = \"black\",\n              #main.title.color = 'white',\n              #legend.title.color = 'white',\n              #legend.text.color= 'white',\n              bg.color = \"mintcream\",\n              frame = TRUE) +\n    tm_borders(alpha = 0.5) +\n    tm_compass(type=\"8star\", size = 2) +\n    tm_scale_bar() +\n    tm_grid(alpha =0.2) +\n    tm_credits(\"Source: Passenger Bus origin and destination data and Bus Stop Location data from LTA datamall\", \n             position = c(\"left\", \"bottom\")) +\n    tm_shape(df2) +\n    tm_bubbles(size='AVG_TRIPS', col='red',alpha=0.5, scale=.6)\n  \n  return(result)\n}\n\n\np1 <- plotmap(day_am, 'Weekday-Morning (Daily Average) \\nPassenger Trips generated at Hex lvl')\np2 <- plotmap(day_pm, 'Weekday-Afternoon (Daily Average) \\nPassenger Trips generated at Hex lvl')\np3 <- plotmap(end_am, 'Weekend-Morning (Daily Average) \\nPassenger Trips generated at Hex lvl')\np4 <- plotmap(end_pm, 'Weekend-Afternoon (Daily Average) \\nPassenger Trips generated at Hex lvl')\n\nInteractive DataTables\nWe will print the datatables here for the ease of counter-checking the interesting hexagon-ids spotted in the maps later.\n\norigin_data_day_amorigin_data_day_pmorigin_data_end_amorigin_data_end_pm\n\n\n\n\nShow the code\ndatatable(origin_data_day_am_hex %>% \n            arrange(desc(TTRIPS)), \n          class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndatatable(origin_data_day_pm_hex %>% \n            arrange(desc(TTRIPS)),\n          class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndatatable(origin_data_end_am_hex %>% \n            arrange(desc(TTRIPS)),\n          class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndatatable(origin_data_end_pm_hex %>% \n            arrange(desc(TTRIPS)),\n          class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nMap of Weekday-Morning (Daily Average) Passenger Trips generated at Hex lvl\nThe tmap_mode is set to ‘view’ mode. Clicking on each hexagon will show the hex-id and the ‘AVG_TRIPS’ field. With the hex-id, we could search for the bus stop description using the interactive data tables above.\n\n\nShow the code\ntmap_mode('view')\n#ttm()\np1\n\n\n\n\n\n\n\n\n\nMap of Weekday-Afternoon (Daily Average) Passenger Trips generated at Hex lvl\n\n\nShow the code\np2\n\n\n\n\n\n\n\n\n\nMap of Weekend-Morning (Daily Average) Passenger Trips generated at Hex lvl\n\n\nShow the code\np3\n\n\n\n\n\n\n\n\n\nMap of Weekend-Afternoon (Daily Average) Passenger Trips generated at Hex lvl\n\n\nShow the code\np4\n\n\n\n\n\n\n\n\n\nFacet View\nLet us plot all four maps side-by-side for easy comparison.\n\nttm()\ntmap_arrange(p1,p2,p3,p4,\n             asp=2, nrow = 2, ncol = 2)\n\n\n\n\n\n\nDiscussion\nThe top row shows the average daily Weekday morning and afternoon passenger trips while the bottom row shows the Weekend morning and afternoon ridership generated at hexagon level. The classification method used is manual; the first three intervals represents the 1st, 2nd and 3rd quantile; and the last two intervals represents all the outliers and extreme outliers. In addition, the red bubbles represent the hexagons with the top 10 average daily passengers trip in each time-interval. We can observe the following:\n\nAverage daily ridership generation is the greatest on weekday morning, followed by weekday afternoon and weekend morning (cannot distinguish between these two at one glance). Lastly, weekend afternoon registers the least ridership volume.\nObserved that the passengers trips generated at the Tuas area is very low as compared to the rest.\nAt first glance, the top two hexagons with the highest origin ridership volume are hex id 1251 and hex id 2411. They are both consistently in the top 2 origin bus stop for all four time intervals. The bus stops (refer to the interactive data table above) that fall within these hexagons are\n\nhex id 1251: BOON LAY INT, BLK 662D, OPP BLK 662C, Blk 662A, BLK 664C\nhex id 2411: WOODLANDS REG INT, W’LANDS CIVIC CTR, OPP W’LANDS CIVIC CTR, BEF W’LANDS STN EXIT 7, W’LANDS STN EXIT 5, W’LANDS STN EXIT 4, BLK 894C, BLK 515\n\nThere are four hexagon ids that are consistently in the top 10 origin bus stops for all four time intervals. They are\n\nHex id 3239: ANG MO KIO INT, BLK 322, AFT ANG MO KIO STN EXIT A, BLK 422, AFT ANG MO KIO INT, ANG MO KIO STN\nHex id 4539: TAMPINES INT, OUR TAMPINES HUB, OPP OUR TAMPINES HUB, UOB TAMPINES CTR, OPP CENTURY SQ, Tampines Stn Exit D, Aft Tampines Stn Exit E\nHex id 4349: BEDOK BUS INT, BEDOK STN EXIT B, BEDOK STN EXIT A, OPP PANASONIC, PANASONIC\n\nThe two hexagons that falls within top 10 in the weekdays but fall out of top 10 during weekends are\n\nhex id 909: BEF JOO KOON INT, OPP JOO KOON INT, MOLEX, OPP MOLEX, OPP FAIRPRICE HUB, JOO KOON INT\nhex id 2054: CLEMENTI STN, CLEMENTI STN, BLK 431\n\nInterestingly, the hexagon within top 10 on weekday and weekend mornings and fall out of this top 10 position on weekday and weekend afternoons is id 3204. Suggesting that this region is more busy in the morning than afternoon.\n\nhex id 3204 : TOA PAYOH BUS INT, OPP TOA PAYOH STN, TOA PAYOH STN, BLK 138B, BLK 84B, BLK 79C\n\nAcross all four time-intervals, there appears to be clusters of High-high and low-low as well as outliers of Low-High and High-Low in our area of study. However , we cannot be sure until we run the local Moran’s I statistical test."
  },
  {
    "objectID": "Take-home_Ex1/Take-home_Ex1.html#task-2-local-indicators-of-spatial-association-lisa-analysis",
    "href": "Take-home_Ex1/Take-home_Ex1.html#task-2-local-indicators-of-spatial-association-lisa-analysis",
    "title": "Take-home_Ex1: Geospatial Analytics for Public Good",
    "section": "Task 2: Local Indicators of Spatial Association (LISA) Analysis",
    "text": "Task 2: Local Indicators of Spatial Association (LISA) Analysis\nIn this section, we will use LISA statistics to identify existence of cluster or outlier for a given variable, in our case, the total number of trips generated at hexagon layer. We will apply the Moran’s I statistic to detect cluster and / or outliers for total trips generated at hexagon layer.\n\nComputing Distance-Based Spatial Weights Matrix\nBefore we can compute the global/local spatial autocorrelation statistics, we need to construct a spatial weights of our study area. The spatial weights is used to define who are the neighbours of the spatial units (hexagons) in our study are. There are a few general rules:\n\nFeatures should have at least one neighbour, each feature should not be a neighbour with all other features.\nIf data is skewed, each feature should have at least 8 neighbours.\n\nContiguity or Distance-based method?\nWe will rule out the contiguity methods because as seen in the geovisualisation above, we can see that the location of bus stops are rather ‘sparse’ in some regions like the central catchment areas, military training areas and airports, resulting in gaps in between groups of hexagons. Therefore, we are more inclined towards distance methods.\nThere are two main types of distance-based methods, namely:\n\nAdaptive distance-based (Fixed number of neighbours, eg, knn=18)\nThis is our choice as our data is highly skewed to the right and also each hexagon will be guaranteed at least n neighbours, which makes it useful when testing for statistical significance later. To illustrate this, we can visualise the neighbours network in a cropped area of our study area.\n\nThe code chunk below first determine the rectangular extent of our study area using st_bbox() of sf package. Next, it adjusts the new bounding parameters. Finally we will use the st_crop of sf package to crop the day_am sf object. The resulting cropped map is a grassland bounded by Gambas Ave, Woodlands Ave 12, Seletar Expressway and Mandai Ave.\n\n\nShow the code\nbbox_new <- st_bbox(day_am)\n#    xmin     ymin     xmax     ymax \n#18720.12 31193.43 33720.12 43184.55 \nbbox_new[1] <- bbox_new[1] + 20000  # xmin\nbbox_new[2] <- bbox_new[2] + 15000  # ymin\nbbox_new[3] <- bbox_new[3] - 20000  # xmax\n#bbox_new[4] <- bbox_new[4] - 5000  # ymax\nbbox_new\n\n\n    xmin     ymin     xmax     ymax \n23720.12 41193.43 28720.12 53184.55 \n\n\nShow the code\ncropped <- st_crop(day_am, bbox_new) %>% \n  rename(geometry=area_hex_grid)\n\n\nNext, we will use the st_centroid(), knearneigh() and knn2nb() of spdep package to calculate the centroids coordinates of cropped map. If we use knn = 8 , the neighbours network will look like diagram below. (In this exercise, we will use knn=18 but we will not plot it as the network diagram will look to dense.)\n\n\nShow the code\nlibrary(spdep)\nlongitude <- map_dbl(cropped$geometry, ~st_centroid(.x)[[1]])\nlatitude <- map_dbl(cropped$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitude, latitude)\nknn8 <- spdep::knn2nb(knearneigh(coords, k=8))\nplot(st_geometry(cropped), border = 'lightgray', main='Grassland bounded by Gambas Ave, \\nWoodlands Ave 12, Seletar Expressway and Mandai Ave')\nplot(knn8, coords, pch=18, cex=0.6, add= TRUE, col='red')\n\n\n\n\n\n\nFixed-distance threshold\nThe reason why this method is less appropriate is because hexagons near to water-bodies/grassland/airports will have less neighbours. This will result in some hexagons having less than 8 neighbours. From the diagram below, we can see that if fixed-distance (green circle) is used, the highlighted hexagon beside the grassland will have only 2 neighbours as compared to the highlighted hexagon above, which will have 6 neighbours.\n\n\n\n\n\n\nIdentify adaptive distance weights\nFor this exercise, we will use knn =18. We will set each hexagon to have 18 neighbours each using the code chunk below.\n\nst_knn() of sfdep will output a list of neighbours ‘nb’ of a hex\nbased on ‘nb’ column, st_weights() of sfdep will generate row-standardised spatial weights\n‘.before = 1’ will put the two columns at the front of our sf dataframe.\n\n\nget_weights <- function(df) {\n  result <- df %>% \n    mutate(nb = st_knn(area_hex_grid,\n                       k=18),\n           wt = st_weights(nb,\n                           style='W'),\n           .before =1)\n  return(result)\n}\n\n\nwm_day_am <- get_weights(day_am)\nwm_day_pm <- get_weights(day_pm)\nwm_end_am <- get_weights(end_am)\nwm_end_pm <- get_weights(end_pm)\n\nTaking a sneak peak at one of the spatial weights matrix above\n\nkable(head(wm_day_am,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnb\nwt\ngrid_id\nbs\nTTRIPS\nAVG_TRIPS\nDESC\narea_hex_grid\n\n\n\n\n2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 16, 17, 22, 23, 24, 30, 38, 39\n0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556\n34\n1\n61\n3\nAFT TUAS STH BLVD\nPOLYGON ((3970.122 27925.48…\n\n\n1, 3, 4, 5, 6, 8, 9, 10, 12, 13, 16, 17, 22, 23, 24, 30, 38, 39\n0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556\n65\n1\n43\n2\nBEF TUAS STH AVE 14\nPOLYGON ((4220.122 28358.49…\n\n\n5, 6, 7, 9, 10, 12, 13, 14, 16, 17, 18, 23, 24, 25, 30, 31, 32, 40\n0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556\n99\n1\n43\n2\nYONG NAM\nPOLYGON ((4470.122 30523.55…\n\n\n\n\n\n\n\nGlobal autocorrelation of spatial association (Global Moran’s I with simulation)\nGlobal spatial association assesses the overall spatial pattern of a variable ‘TTRIPS’ across the entire study area. It provides a single value or metric that summarizes the extent to which similar values cluster together or are dispersed across the entire geographic space.\n\n\n\n\n\nSet seed to ensure that the computation is reproducible.\n\nset.seed(1234)\n\nGlobal Moran’s I for weekday morning passenger trips generated by hexagon level. Simulated data is used as we do not assume normality and randomization. The number of simulations is 99+1 = 100.\nWe can use the global_moran_perm() function of the sfdep package to do it. It all starts with the wm_day_am spatial weights matrix.\n\nglobal_moran_perm(wm_day_am$TTRIPS,\n                       wm_day_am$nb,\n                       wm_day_am$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.17009, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nGlobal Moran’s I for weekday afternoon passenger trips generated by hexagon level.\n\nglobal_moran_perm(wm_day_pm$TTRIPS,\n                       wm_day_pm$nb,\n                       wm_day_pm$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.18789, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nGlobal Moran’s I for weekend morning passenger trips generated by hexagon level.\n\nglobal_moran_perm(wm_end_am$TTRIPS,\n                       wm_end_am$nb,\n                       wm_end_am$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.13575, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nGlobal Moran’s I for weekend afternoon passenger trips generated by hexagon level.\n\nglobal_moran_perm(wm_end_pm$TTRIPS,\n                       wm_end_pm$nb,\n                       wm_end_pm$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.17146, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nFor all the four time intervals, since p-value for global Moran’s I is smaller than 0.05 , we can reject the null hypothesis that the spatial patterns is random. Because the Moran’s I statistics is greater than 0, we can infer the spatial distribution shows sign of clustering for all four time intervals. This result is rather consistent with the choropleth maps plotted earlier.\n\n\nLocal autocorrelation of spatial association\nLocal spatial association examines the spatial patterns at a more detailed, local level. Instead of providing a single summary value for the entire study area, local measures identify specific areas where the spatial association is particularly strong or weak. If the Local Moran’s I is positive, it suggests clusters of high-high or low-low. If negative, it suggests outliers of low-high or high-low.\n\n\n\n\n\nCompute LISA of the passengers trips generated by origin at hex level\nWe can use the local_moran() function of the sfdep package to do so, It all starts with the wm_day_am spatial weights matrix, this function automatically compute the neighbour lists and weights using simulated data. As this function outputs results in a group format, we will need to unnest() in order to access the output.\n\nget_lisa<- function(wm){\n  result<- wm %>% \n  mutate(local_moran = local_moran(\n    TTRIPS, nb,wt, nsim=99),\n    .before=1) %>%\n  unnest(local_moran)\n  \n  return(result)\n}\n\nlisa_day_am <- get_lisa(wm_day_am)\nlisa_day_pm <- get_lisa(wm_day_pm)\nlisa_end_am <- get_lisa(wm_end_am)\nlisa_end_pm <- get_lisa(wm_end_pm)\n\nNew columns are added to the lisa_day_am simple feature dataframe smartly. The new columns are\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations\np_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations\np_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative=\np_folded_sim: the simulation folded [0, 0.5] range ranked p-value\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates\nmean: contains the quadrant ‘type’ of a typical Moran Scatterplot\nmedian: contains the quadrant ‘type’ of a typical Moran Scatterplot (use this if dataset is highly skewed).\n\nResults of LISA computation for all four time intervals\n\nlisa_day_amlisa_day_pmlisa_end_amlisa_end_pm\n\n\n\ndatatable(lisa_day_am, class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\ndatatable(lisa_day_pm, class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\ndatatable(lisa_end_am, class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\ndatatable(lisa_end_pm, class = 'cell-border stripe', options = list(pageLength = 5))\n\n\n\n\n\n\n\n\n\n\n\nVisualising significant Local Moran’s I at 95% confidence level\nWe will use tmap core functions to build choropleth maps, using the Local Moran’s I field and p-value field for all four time intervals.\nOnly the significant values of Local Moran’s I values at 95% confidence level are plotted.\n\nget_sig_lmi_map <- function(lisatable, title) {\n  \n  sig_lisatable <- lisatable  %>%\n  filter(p_ii_sim < 0.05)\n  \n  result <- tm_shape(lisatable) +\n    tm_polygons() +\n    tm_borders(alpha = 0.5) +\n    tm_shape(sig_lisatable) +\n    tm_fill(\"ii\") + \n    tm_borders(alpha = 0.4) +\n    tm_layout(main.title = title,\n              main.title.size = 1.3)\n  \n  return(result)\n  \n}\n\nsig_lmi_1 <- get_sig_lmi_map(lisa_day_am,\"Local Moran's I of Total Trips generated on Weekday Morning at 95% CL\" )\nsig_lmi_2 <- get_sig_lmi_map(lisa_day_pm, \"Local Moran's I of Total Trips generated on Weekday Afternoon at 95% CL\" )\nsig_lmi_3 <- get_sig_lmi_map(lisa_end_am, \"Local Moran's I of Total Trips generated on Weekend Morning at 95% CL\" )\nsig_lmi_4 <- get_sig_lmi_map(lisa_end_pm, \"Local Moran's I of Total Trips generated on Weekend Afternoon at 95% CL\" )\n\nWe will plot them side-by-side for easy comparison.\n\n\nShow the code\ntmap_mode('plot')\n\ntmap_arrange(sig_lmi_1,\n              sig_lmi_2,\n              sig_lmi_3,\n              sig_lmi_4,\n              asp=2,\n              nrow=2,\n              ncol=2)\n\n\n\n\n\nFrom the Local Moran choropleth maps above, orange regions would indicate outliers regions however, we would not know whether they are low-high or high-low regions.\nThe green regions would indicate clusters however we would not know whether they are high-high or low-low regions. One thing for sure, the green Tuas region represents low-low based on the geovisualisation in the previous section.\nTo find out, we have to plot the LISA maps in the next section.\n\n\n\n\n\n\n\n\nVisualizing significant LISA map at 95% confidence level\nThe LISA maps that we are building now are categorical map showing outliers (Low-high or High-low) and clusters (high-high or low-low).\nWe will use median values to generate the quadrants (HH, LH, HL or LL) because our data is highly skewed to the right, otherwise we can use the mean values.\n\nget_sig_lisa_map <- function(lisatable, title) {\n  \n  sig_lisatable <- lisatable  %>%\n  filter(p_ii_sim < 0.05)\n  \n  result <- tm_shape(lisatable) +\n    tm_polygons(alpha = 0.5) +\n    tm_borders(alpha = 0.5) +\n    \n    tm_shape(sig_lisatable) +\n    tm_fill(\"median\",\n            palette = c(\"#2c7bb6\",  \"#fdae61\", \"#abd9e9\", \"#d7191c\"),\n            alpha= 0.7) + \n    tm_dots('grid_id', alpha=0.05) +\n    tm_borders(alpha = 0.4) +\n    tm_layout(main.title = title,\n              main.title.size = 1.5,\n              legend.position = c(\"left\", \"top\"))\n    \n  return(result)\n  \n}\n\nsig_lisa_1 <- get_sig_lisa_map(lisa_day_am,\"LISA categories generated on Weekday Morning at 95% CL\" )\nsig_lisa_2 <- get_sig_lisa_map(lisa_day_pm, \"LISA categories generated on Weekday Afternoon at 95% CL\" )\nsig_lisa_3 <- get_sig_lisa_map(lisa_end_am, \"LISA categories generated on Weekend Morning at 95% CL\" )\nsig_lisa_4 <- get_sig_lisa_map(lisa_end_pm, \"LISA categories generated on Weekend Afternoon at 95% CL\" )\n\n\nLISA categories generated on Weekday Morning at 95% CL\nTo leverage on the interactivity of the map, tm_dots() are used to plot the hexagon’s grid_id on the map for identification of each hexagon.\nClicking on the center of the significant hexgons will reveal their grid_id for easy identification of the bus stops that fall within the hexagon. (Use the interactive datatable above to search for hexagon grid id).\n\n\nShow the code\ntmap_mode('view')\nsig_lisa_1\n\n\n\n\n\n\n\n\n\nLISA categories generated on Weekday Afternoon at 95% CL\n\n\nShow the code\ntmap_mode('view')\nsig_lisa_2\n\n\n\n\n\n\n\n\n\nLISA categories generated on Weekend Morning at 95% CL\n\n\nShow the code\ntmap_mode('view')\nsig_lisa_3\n\n\n\n\n\n\n\n\n\nLISA categories generated on Weekend Afternoon at 95% CL\n\n\nShow the code\ntmap_mode('view')\nsig_lisa_4\n\n\n\n\n\n\n\n\n\nPlotting all four LISA maps side by side for easy comparison\n\n\nShow the code\ntmap_mode('plot')\n\ntmap_arrange(sig_lisa_1,\n              sig_lisa_2,\n              sig_lisa_3,\n              sig_lisa_4,\n              asp=2,\n              nrow=2,\n              ncol=2)\n\n\n\n\n\n\n\n\nDiscussion\nThe four maps (different time-intervals) above shows significant clusters and outliers for passengers trips generate by origin bus stops at each hexagon. The confidence level used is 95%.\nBlue region: Low-Low cluster\nLight-blue region: Low-High outlier\nOrange region: High-Low outlier\nRed region: High-High cluster\nWe observe that passengers trips by origin is not homogeneous throughout Singapore, our study area.\nAnalysis by regions\nAt one glance, the spatial pattern is observed to be higher in the North, East and West residential regions than in the South part of Singapore.\nThe high clusters are Boon Lay interchange, Woodlands interchange, Yishun Interchange, Ang Mo kio Interchange, Seng Kang/ Hougang, Tampines Interchange Bedok Bus Interchange, Toa Payoh Interchange, Clementi Station, Chua Chu kang Interchange, Bukit Batok Interchange and Joo Koon Interchange .\nThe western parts of Singapore is a cluster of low passenger trips. They are the Tuas, Pioneer Sector, Gul Circle, Shipyard, Samulun, Jurong port, Penjuru Crescent and Pandan subzones. These are mainly industrial zones and workers are usually provided with company transport.\nAnalysis of the edges\nThe edges of Singapore tend to be clusters of low passenger trips, with the exception of a few outliers edges, namely:\n\nhex id 194 (Western edge High-low): AFT TUAS STH AVE 4, BEF TUAS STH AVE 4\nhex id 353 (Western edge High-Low): OPPO TUAS LK STN\nhex id 2298 (Southern edge High-Low): OPP HAW PAR VILLA STN, HAW PAR VILLA STN, OPP HAW PAR VILLA STN, HAW PAR VILLA STN\nhex id 5154 (Eastern edge High-Low only on weekends): TANAH MERRY FERRY TER .\nThe Tanah Merah Ferry terminal is one of the gateways to Batam and Bintan, Indonesia, as well as Desaru and Tanjung Pengelih, Malaysia.\nhex id 5133 (Eastern edge High-Low): CHANGI VILLAGE TER, AFT CHANGI GOLF COURSE, CHANGI GOLF COURSE, CHANGI VILLAGE HOTEL, BLK 4, BLK 5\nChangi Village is a place of food and leisure attraction.\nhex id 5349 (Eastern edge High-Low): POLICE PASS OFF\nThis is the SAF Ferry terminal.\n\nAnalysis of weekends vs weekdays\nA few interesting observations\n\nA short stretch of bus stops along Bukit Timah / Dunearn Road near to junction of Clementi Road and PIE exit seems to come alive on weekend afternoons, turning into High-Low outliers.\nBus stops near to the Singapore Botanic Gardens also turns into a high-low outliers during weekend afternoons.\nBus stops along Jalan Bukit Merah, River Valley Road, Havelock Road and the Rochor areas turn into high clusters only on Weekend Mornings.\nThe woodlands checkpoint area is also observed to be a high cluster only on weekend afternoons, suggesting increased human flow via bus on the weekends.\n\nWhile this exercise helps us to locate clusters and outliers and we have tried to suggest potential reasons behind the observed spatial pattern, there could be other social and economic factors like income levels, population density and other demographic factors that can play a role in this pattern.\n\n\nReferences\nTin Seong Kam. “2 Choropleth Mapping with R” From R for Geospatial Data Science and Analytics https://r4gdsa.netlify.app/chap02\nTin Seong Kam. “9 Global Measures of Spatial Autocorrelation” From R for Geospatial Data Science and Analytics https://r4gdsa.netlify.app/chap09\nTin Seong Kam. “10 Local Measures of Spatial Autocorrelation” From R for Geospatial Data Science and Analytics https://r4gdsa.netlify.app/chap10"
  }
]